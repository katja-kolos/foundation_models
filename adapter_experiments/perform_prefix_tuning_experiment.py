import argparse
import gc
import logging
import numpy as np
import pandas as pd
import torch

from datasets import load_dataset
from datetime import datetime
from PIL import Image
from torch.utils.data import DataLoader
from tqdm import tqdm


from qwen_vl_utils import process_vision_info #this is some basic if-else image processing, can be reused as is for paligemma runs
from prefix_tuning import PrefixTuningModelPastKeyValues, PrefixDataset, prefix_collate
from helpers import *


"""
Script to prefix-train a generative MLLM
Was initially written for qwen, current adaptation is for paligemma
The training data is the multimodal ScienceQA dataset. 
The targets are free-text solutions, which are compared to solutions generated by the model. The answer field (int: [1:4]) is not used in this implementation.
Assumed model architecture: autoregressive textual model + visual encoder
Script only runs on cuda and does not support CPU-only runs
"""

# Empty cache just in case
torch.cuda.empty_cache()

# Logging
# Generate timestamped filename
start_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f"prefix-tuning_{start_time}.log"
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_filename),  # Log to file
        logging.StreamHandler()            # Also log to console (optional)
    ]
)

# CONSTANTS
NUM_EPOCHS_FT = 20
#BATCH_SIZE = 4
#logging.info(f'Set constants: NUM_EPOCHS_FT={NUM_EPOCHS_FT}, BATCH_SIZE={BATCH_SIZE}')
logging.info(f'Set constants: NUM_EPOCHS_FT={NUM_EPOCHS_FT}')

device = 'cuda' # torch.cuda.get_device_name(0)
logging.info(torch.cuda.get_device_name(0))
logging.info('Memory Usage:')
logging.info(f'Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB')
logging.info(f'Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB')


def preprocess_input_qwen(tokenizer, processor, prompts, texts, images, y, device):
    messages = [processor.apply_chat_template(
                text, tokenize=False, add_generation_prompt=False
    ) for text in texts]
    image_inputs, video_inputs = process_vision_info(texts)
    inputs = processor(
        text=messages,
        images=image_inputs,
        videos=video_inputs,
        padding="longest",
        return_tensors="pt",
    )
    max_length = inputs["input_ids"].size(1) 
    labels = tokenizer(y, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")["input_ids"]
    return inputs.to(device, dtype=torch.bfloat16), labels.to(device)

def preprocess_input_paligemma(tokenizer, processor, prompts, texts, images, y, device):
    images = [image.resize((224, 224)) for image in images]
    inputs = processor(
        text=prompts,
        images=images,
        return_tensors="pt",
        padding="longest"
    )
    max_length = inputs["input_ids"].size(1)
    labels = tokenizer(y, padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")["input_ids"]
    return inputs.to(device, dtype=torch.bfloat16), labels.to(device)

def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val, preprocess_func):
    train_errors = []
    val_errors = []
    model.train()
    logging.info('Set model into train mode')
    for epoch in tqdm(range(NUM_EPOCHS_FT)):
        error = 0
        num_samples = 0
        for prompts, texts, images, y in dataloader_train:
            inputs, labels = preprocess_func(tokenizer, processor, prompts, texts, images, y, device)
            #logging.info("inputs")
            #logging.info(inputs)
            #logging.info("labels")
            #logging.info(labels)

            optimizer.zero_grad()
            #logging.info("forward pass")
            outputs = model(inputs, labels=labels)
            # detailed look into
            #output_ids = outputs.logits.argmax(-1)
            #output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
            #logging.info(output_text)

            loss = outputs.loss
            #logging.info(f"loss: {loss}")
            loss.backward()
            optimizer.step()
            error += loss.item() * len(texts)
            num_samples += len(texts)
            del labels, inputs
            gc.collect()
            torch.cuda.empty_cache()
            #logging.info("backward pass done")
        error /= num_samples
        logging.info(f'Error after epoch {epoch}: {error}')
        train_errors.append((epoch, error))
        #if epoch % 5:
        if True:
            val_error = 0
            num_samples = 0
            for prompts, texts, images, y in dataloader_val:
                inputs, labels = preprocess_func(tokenizer, processor, prompts, texts, images, y, device)
                outputs = model(
                    inputs=inputs,
                    labels=labels,
                )
                loss = outputs.loss
                val_error += loss.item() * len(texts)
                num_samples += len(texts)
                del labels, inputs
                gc.collect()
                torch.cuda.empty_cache()
            val_error /= num_samples
            logging.info(f'Validation error after epoch {epoch}: {val_error}')
            val_errors.append((epoch, val_error))
    return train_errors, val_error

def prepare_dataloaders(exp_setting: str, batch_size: int):
    ### val data -- same for all settings for fair comparison
    df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))
    # filter validation data; #TODO: would it be wiser to validate on exactly the same subset used for benchmarking? or to have as much data as possible?
    df_val = df_val[df_val['solution'] != ''].reset_index()
    df_val['image'] = df_val.apply(lambda row: row['image'] if row['image'] else Image.new("RGB", (224, 224), (0, 0, 0)), axis=1)
    df_val['input'] = df_val.apply(lambda row: build_prompt(row), axis=1)
    df_val['message'] = df_val.apply(lambda row: build_message(row), axis=1)
    logging.info(f'Loaded validation with {len(df_val)} rows')

    # DataLoader for val data
    dataset_val = PrefixDataset(df_val)
    dataloader_val=DataLoader(dataset_val, collate_fn=prefix_collate, batch_size=batch_size, shuffle=True)
    logging.info('DataLoader for validation ready')

    # Load golden data: needed no matter the setting to bring images 
    # data with label and image data
    df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))
    logging.info(f'Loaded train dataset with {len(df_train_label)} rows')
    df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index()
    df_train_label['image'] = df_train_label.apply(lambda row: row['image'] if row['image'] else Image.new("RGB", (224, 224), (0, 0, 0)), axis=1)
    
    if exp_setting == 'golden':
        # data from dataset
        df_train_label['input'] = df_train_label.apply(lambda row: build_prompt(row), axis=1)
        df_train_label['message'] = df_train_label.apply(lambda row: build_message(row), axis=1)
    
        # DataLoader for train data
        dataset_label_train = PrefixDataset(df_train_label)
        dataloader_label_train=DataLoader(dataset_label_train, collate_fn=prefix_collate, batch_size=batch_size, shuffle=True)
        logging.info('DataLoader for train ready')
        return dataloader_label_train, dataloader_val
    elif exp_setting == 'teacher':
        # data from Gemini for KD
        df_train_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep="\t")[['index', 'input', 'answer', 'explanation']]
        logging.info(f'Loaded train dataset with {len(df_train_gemini)} rows')

        df_train_gemini = pd.merge(df_train_gemini, df_train_label[['index', 'image', 'solution']], on='index')
        df_train_gemini['message'] = df_train_gemini.apply(lambda row: build_message(row), axis=1)
        
        # DataLoader for train data
        dataset_gemini_train = PrefixDataset(df_train_gemini)
        dataloader_gemini_train=DataLoader(dataset_gemini_train, collate_fn=prefix_collate, batch_size=batch_size, shuffle=True)
        return dataloader_gemini_train, dataloader_val

def save_assets(model, model_name, train_errors_ft_qwen, val_errors_ft_qwen, batch_size):
    # Save assets
    logging.info('Saving assets')
    saving_path = f"{model_name.replace('/', '_').lower()}_pref_{NUM_EPOCHS_FT}_{batch_size}"
    torch.save(model.state_dict(), saving_path)
    logging.info(f'Saved model to path: {saving_path}')
    with open(f'{saving_path}_train_error', 'w') as f:
        f.write(train_errors_ft_qwen)
        logging.info('Saved train_errors')
    with open(f'{saving_path}_val_error', 'w') as f:
        f.write(val_errors_ft_qwen)
        logging.info('Saved validation_errors')

def main():
    parser = argparse.ArgumentParser(description="Select training experiment setting: what will be used as training data") 
    parser.add_argument(
        "--exp_setting",
        type=str,
        default="golden",
        help="'golden' for dataset, 'teacher' for gemini outputs. Default: 'golden'"
    )
    parser.add_argument(
        "--ngpus",
        type=int,
        default=1,
        help="n gpus to use at training. Default: 1",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=4,
        help="batch_size for both train and validation dataloaders"
    )

    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2-VL-2B-Instruct",
        help="exact hf path to model; default: 'Qwen/Qwen2-VL-2B-Instruct'"
    )

    args = parser.parse_args()
    
    model_name = args.model_name
    logging.info(f'Model: {model_name}')

    if model_name == "Qwen/Qwen2-VL-2B-Instruct":
        from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor, Qwen2VLForConditionalGeneration
        model = AutoModelForImageTextToText.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16, #"auto"
            device_map="auto",
        )
        logging.info('Loaded model')
    
        processor = AutoProcessor.from_pretrained(model_name)
        logging.info('Loaded processor')
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        logging.info('Loaded tokenizer')
        
        preprocess_input_function = preprocess_input_qwen
        match_n_layer = model.config.num_hidden_layers
        match_n_head = model.config.num_key_value_heads
        n_embd = model.config.hidden_size // 6

    elif model_name == "google/paligemma2-3b-pt-224":
        from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration
        from helpers import login_to_hf
        login_to_hf()
        logging.info("Login with token: done")
        model = PaliGemmaForConditionalGeneration.from_pretrained(
                model_name, 
                torch_dtype=torch.bfloat16, 
                device_map="auto", 
        )
        logging.info('Loaded model')
        
        processor = PaliGemmaProcessor.from_pretrained(model_name)
        logging.info('Loaded processor')
        tokenizer = processor.tokenizer #AutoTokenizer.from_pretrained(model_name)
        logging.info('Loaded tokenizer')
        
        preprocess_input_function = preprocess_input_paligemma

        match_n_layer = model.config.num_hidden_layers
        match_n_head = model.config.text_config.num_key_value_heads
        n_embd = model.config.hidden_size // 2

    model_prefix = PrefixTuningModelPastKeyValues(model, match_n_layer, match_n_head, n_embd, device).to(device)
    optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)
    
    if args.ngpus > 1:
        # Wrapper for multi-GPU processing
        model_prefix = torch.nn.DataParallel(model_prefix)
    
    dataloader_train, dataloader_val = prepare_dataloaders(exp_setting=args.exp_setting, batch_size=args.batch_size)

    logging.info('Starting training')
    train_errors, val_errors = train(model_prefix, tokenizer, processor, optimizer, dataloader_train, dataloader_val, preprocess_input_function)
    
    save_assets(model, model_name, train_errors, val_errors, args.batch_size)
    logging.info('Done')

if __name__ == "__main__":
    main()
