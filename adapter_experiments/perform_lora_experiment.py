import gc
import logging
import os
import pandas as pd
import torch
from datasets import Dataset, DatasetDict, load_dataset
from datetime import datetime
from peft import LoraConfig, get_peft_model
#from qwen_vl_utils import process_vision_info
from transformers import (AutoModelForImageTextToText, 
                          AutoTokenizer, 
                          AutoProcessor, 
                          DataCollatorForSeq2Seq,
                          Seq2SeqTrainer,
                          Seq2SeqTrainingArguments,
                          Qwen2VLForConditionalGeneration)

from dataset_utils import *

"""
Script to train a LoRA adapter for a generative MLLM
Was written for qwen
The training data is the multimodal ScienceQA dataset. 
The targets are free-text solutions, which are compared to solutions generated by the model. The answer field (int: [1:4]) is not used in this implementation.
Assumed model architecture: autoregressive textual model + visual encoder
Script only runs on cuda and does not support CPU-only runs
"""
# Empty cache just in case
torch.cuda.empty_cache()

# Logging
# Generate timestamped filename
start_time = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f"lora-tuning_{start_time}.log"
# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_filename),  # Log to file
        logging.StreamHandler()            # Also log to console (optional)
    ]
)

# CONSTANTS
NUM_EPOCHS_FT = 10
BATCH_SIZE = 4
SAVE_DIR = "./qwen_lora_model"
logging.info(f'Set constants: NUM_EPOCHS_FT={NUM_EPOCHS_FT}, BATCH_SIZE={BATCH_SIZE}')

# Main script
device = 'cuda' # torch.cuda.get_device_name(0)
logging.info(torch.cuda.get_device_name(0))
logging.info('Memory Usage:')
logging.info(f'Allocated: {round(torch.cuda.memory_allocated(0)/1024**3,1)} GB')
logging.info(f'Cached: {round(torch.cuda.memory_reserved(0)/1024**3,1)} GB')

model_name = "Qwen/Qwen2-VL-2B-Instruct"

model = AutoModelForImageTextToText.from_pretrained(
    model_name,
    torch_dtype="bfloat16",
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
processor = AutoProcessor.from_pretrained(model_name)

# Configure LoRA
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none"
)

# Apply LoRA to the model
model = get_peft_model(model, lora_config)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, max_length=512)
def add_image_paths(df, temp_images):
    def construct_path(idx):
        image_path = os.path.join(temp_images, f"image_{idx}.png")
        return image_path if os.path.isfile(image_path) else None
    df['image'] = df.index.to_series().apply(construct_path)
    return df

# Load train dataset TODO: add filtering
df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))
temp_images_path = "./temp_images"
df_train_label = add_image_paths(df_train_label, temp_images_path)
# df_train_label['image'] = df_train_label.apply(lambda row: row['image'] if row['image'] else Image.new("RGB", (224, 224), (0, 0, 0)), axis=1)
df_train_label['input'] = df_train_label.apply(lambda row: build_prompt(row)[0], axis=1)
df_train_label['message'] = df_train_label.apply(lambda row: build_message(row), axis=1)
df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index(drop=True)

# Load validation dataset TODO: add filtering
df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))
temp_images_path = "./temp_images_val/"
df_val = add_image_paths(df_val, temp_images_path)
# df_val['image'] = df_val.apply(lambda row: row['image'] if row['image'] else Image.new("RGB", (224, 224), (0, 0, 0)), axis=1)
df_val['input'] = df_val.apply(lambda row: build_prompt(row)[0], axis=1)
df_val['message'] = df_val.apply(lambda row: build_message(row), axis=1)
df_val = df_val[df_val['solution'] != ''].reset_index(drop=True)

# Convert to Hugging Face Dataset
hf_train_dataset = Dataset.from_pandas(df_train_label)
hf_val_dataset = Dataset.from_pandas(df_val)

# Create DatasetDict
dataset = DatasetDict({"train": hf_train_dataset, "val": hf_val_dataset})

# Preprocessing: tokenization
def preprocess_function(examples):
    # Tokenize input and target text
    model_inputs = tokenizer(examples["question"], padding="max_length", truncation=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["solution"], padding="max_length", truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

processed_dataset_val = hf_val_dataset.map(
    preprocess_function,
    batched=True,  # Process multiple examples at once
    remove_columns=hf_val_dataset.column_names  # Remove original columns
)

processed_dataset_train = hf_train_dataset.map(
    preprocess_function,
    batched=True,  # Process multiple examples at once
    remove_columns=hf_val_dataset.column_names  # Remove original columns
)


# main script
training_args = Seq2SeqTrainingArguments(
    output_dir=SAVE_DIR,                # Directory to save the model
    per_device_train_batch_size=8,        # Batch size for training
    per_device_eval_batch_size=8,         # Batch size for evaluation
    predict_with_generate=True,           # Generate sequences for evaluation
    evaluation_strategy="steps",          # Evaluation frequency
    logging_steps=500,                    # Logging frequency
    save_steps=500,                       # Checkpoint saving frequency
    save_total_limit=2,                   # Number of checkpoints to keep
    num_train_epochs=3,                   # Number of epochs
    fp16=True,                            # Use mixed precision training
    learning_rate=5e-5,                   # Learning rate
    lr_scheduler_type="linear",           # Learning rate scheduler
    warmup_steps=500,                     # Warmup steps
    report_to="none",                     # Disable logging services
    remove_unused_columns=False
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset_train,    # Pass training dataset
    eval_dataset=processed_dataset_val,     # Optionally pass validation dataset
    tokenizer=tokenizer,
    data_collator=data_collator
    
)

trainer.train()

model.save_pretrained(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)
