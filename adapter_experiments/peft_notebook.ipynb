{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import load_dataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset_id = \"derek-thomas/ScienceQA\"\n",
    "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[\"train\", \"validation\", \"test\"])"
   ],
   "id": "4bbb2d8c10e1e09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_question_text(problem):\n",
    "    question = problem['question']\n",
    "    return question\n",
    "\n",
    "\n",
    "def get_choice_text(probelm, options):\n",
    "    choices = probelm['choices']\n",
    "    choice_list = []\n",
    "    for i, c in enumerate(choices):\n",
    "        choice_list.append(\"({}) {}\".format(options[i], c))\n",
    "    choice_txt = \" \".join(choice_list)\n",
    "    return choice_txt\n",
    "\n",
    "\n",
    "def get_context_text(problem, use_caption):\n",
    "    txt_context = problem['hint']\n",
    "    img_context = problem['caption'] if use_caption else \"\"\n",
    "    context = \" \".join([txt_context, img_context]).strip()\n",
    "    if context == \"\":\n",
    "        context = \"N/A\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def build_prompt(question_data, use_lecture=False, use_solution=False):\n",
    "    question = get_question_text(question_data)\n",
    "    choices = get_choice_text(question_data, [choice_num for choice_num in range(5)])\n",
    "    hint = get_context_text(question_data, False)\n",
    "    task = question_data['task']\n",
    "    input_prompt = f'Question: {question}\\n Task: {task}\\n Choices: {choices}\\n Hint: {hint}'\n",
    "    if use_lecture:\n",
    "        lecture = f'\\n Lecture: {question_data[\"lecture\"]}'\n",
    "        input_prompt += lecture\n",
    "    if use_solution and question_data[\"solution\"]:\n",
    "        solution = f'\\n Solution: {question_data[\"solution\"]}'\n",
    "        input_prompt += solution\n",
    "    return input_prompt\n",
    "\n",
    "def build_message(row):\n",
    "    row_input = build_prompt(row)\n",
    "    image = row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": row_input },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ],
   "id": "d46e3372979a16ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = train_dataset.filter(lambda example: example['solution']!=\"\")\n",
    "eval_dataset = eval_dataset.filter(lambda example: example['solution']!=\"\")\n",
    "test_dataset = test_dataset.filter(lambda example: (example['solution']!=\"\") & (example['lecture']!=\"\"))"
   ],
   "id": "de9da10690364acb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "train_dataset_df = pd.DataFrame(train_dataset).reset_index()\n",
    "train_dataset_gemini = pd.merge(train_dataset_gemini, train_dataset_df[['index', 'image', 'solution']], on='index')"
   ],
   "id": "55c6d701bc93e8bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset_qwen_gemini = [(sample[1][\"input\"], sample[1][\"solution\"]) for sample in train_dataset_gemini.iterrows()]\n",
    "train_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in train_dataset]\n",
    "eval_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in eval_dataset]\n",
    "test_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in test_dataset]"
   ],
   "id": "ba9e0a8f9cddee6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset_paligemma_gemini = [(sample[1][\"input\"], sample[1][\"image\"], sample[1][\"solution\"]) for sample in train_dataset_gemini.iterrows()] # sample[\"input\"] is the output of build_prompt\n",
    "train_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in train_dataset]\n",
    "eval_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in eval_dataset]\n",
    "test_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in test_dataset]"
   ],
   "id": "d6d9b1a4d1e520ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn_qwen(examples):\n",
    "\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for (example,_) in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for (example,_) in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    max_length = batch[\"input_ids\"].size(1)\n",
    "    example_labels = [label for (x, label) in examples]\n",
    "    labels = tokenizer(example_labels, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "    return batch  # Return the prepared batch"
   ],
   "id": "fb5e0c92a47ae6c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def collate_fn_paligemma(examples):\n",
    "    texts = [text for (text, image, label) in examples]\n",
    "    image_inputs = [image.resize((224, 224)) for (text, image, label) in examples]\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    max_length = batch[\"input_ids\"].size(1)\n",
    "    example_labels = [label for (text, image, label) in examples]\n",
    "    labels = processor.tokenizer(example_labels, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "    return batch  # Return the prepared batch"
   ],
   "id": "854b8d591cd0ceab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Qwen",
   "id": "bf25da54c6accb71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ],
   "id": "f3bd2e3e25e4b19a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=30,\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ],
   "id": "8f3a77a93ca51f79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"Qwen/Qwen2-VL-2B-Instruct-ScienceQA\",  # Directory to save the model\n",
    "    num_train_epochs=20,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ],
   "id": "49d79deb56c96868",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_qwen,\n",
    "    eval_dataset=eval_dataset_qwen,\n",
    "    data_collator=collate_fn_qwen,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "94549d1db3bd4baf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "35b12846682a449d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}