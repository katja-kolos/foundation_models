{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T20:01:20.284441Z",
     "start_time": "2025-01-04T20:00:56.553089Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
    "import prefix_tuning\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "importlib.reload(prefix_tuning)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prefix_tuning' from '/Users/floriandreyer/Library/Mobile Documents/com~apple~CloudDocs/Python Projekte/foundation_models/prefix_tuning.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:58:49.817200Z",
     "start_time": "2025-01-04T15:58:49.798617Z"
    }
   },
   "cell_type": "code",
   "source": "prefix_tuning_layer = torch.load(\"QWEN_PREFIX_TUNING_10/prefix_tuning.pt\", map_location=torch.device('cpu'))",
   "id": "910f26a8c3caa936",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T20:01:48.080780Z",
     "start_time": "2025-01-04T20:01:20.450910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")"
   ],
   "id": "a2bdfd18273af56b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e61dc3353e44cba997ab13263152e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:59:14.758148Z",
     "start_time": "2025-01-04T15:59:14.742571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_prefix_tuning_layer= prefix_tuning.loaded_prefix_tuning = prefix_tuning.PrefixTuning(model.config, prefix_length=10)\n",
    "model_prefix_tuning_layer.load_state_dict(prefix_tuning_layer)"
   ],
   "id": "38896aea83c321c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:59:15.809090Z",
     "start_time": "2025-01-04T15:59:15.805353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix_tuning_model = prefix_tuning.PrefixTuningModel(model, tokenizer, prefix_length=10)\n",
    "prefix_tuning_model.prefix_tuning = model_prefix_tuning_layer"
   ],
   "id": "d1c8dd544056c05a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:23:12.933524Z",
     "start_time": "2025-01-04T16:23:12.913271Z"
    }
   },
   "cell_type": "code",
   "source": "prefix_tuning_model",
   "id": "9823c6fb56f1b812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixTuningModel(\n",
       "  (model): Qwen2VLForConditionalGeneration(\n",
       "    (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2VLVisionBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): VisionSdpaAttention(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): VisionMlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): QuickGELUActivation()\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): PatchMerger(\n",
       "        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (model): Qwen2VLModel(\n",
       "      (embed_tokens): Embedding(151936, 1536)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "          (self_attn): Qwen2VLSdpaAttention(\n",
       "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "  )\n",
       "  (prefix_tuning): PrefixTuning()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:30:23.627180Z",
     "start_time": "2025-01-04T15:30:14.726552Z"
    }
   },
   "cell_type": "code",
   "source": "data = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='test'))",
   "id": "99c01cee30067835",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:31:31.465157Z",
     "start_time": "2025-01-04T15:31:31.434697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.dropna(inplace=True)\n",
    "data"
   ],
   "id": "85376946f607a99e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  image   \n",
       "2     <PIL.PngImagePlugin.PngImageFile image mode=RG...  \\\n",
       "5     <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "9     <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "10    <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "13    <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "...                                                 ...   \n",
       "4234  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4235  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4237  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4238  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4239  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "\n",
       "                                               question   \n",
       "2                 What is the name of the colony shown?  \\\n",
       "5     Which of these organisms contains matter that ...   \n",
       "9     What is the expected ratio of offspring with a...   \n",
       "10    Which property do these three objects have in ...   \n",
       "13    Think about the magnetic force between the mag...   \n",
       "...                                                 ...   \n",
       "4234                    Which continent is highlighted?   \n",
       "4235                    Which continent is highlighted?   \n",
       "4237            Which of these states is farthest west?   \n",
       "4238                    Which continent is highlighted?   \n",
       "4239                What is the direction of this push?   \n",
       "\n",
       "                                                choices  answer   \n",
       "2      [Maryland, New Hampshire, Rhode Island, Vermont]       1  \\\n",
       "5                                  [bilberry, mushroom]       1   \n",
       "9                             [0:4, 4:0, 2:2, 1:3, 3:1]       1   \n",
       "10                            [shiny, slippery, opaque]       2   \n",
       "13    [The magnitude of the magnetic force is the sa...       2   \n",
       "...                                                 ...     ...   \n",
       "4234  [North America, South America, Antarctica, Aus...       0   \n",
       "4235       [Africa, South America, North America, Asia]       1   \n",
       "4237   [Alabama, Illinois, South Carolina, Connecticut]       1   \n",
       "4238           [Asia, Europe, Australia, North America]       1   \n",
       "4239    [away from the bulldozer, toward the bulldozer]       0   \n",
       "\n",
       "                                                   hint           task   \n",
       "2                                                        closed choice  \\\n",
       "5     Below is a food web from a tundra ecosystem in...  closed choice   \n",
       "9     This passage describes the fleece type trait i...  closed choice   \n",
       "10                              Select the best answer.  closed choice   \n",
       "13    The images below show two pairs of magnets. Th...  closed choice   \n",
       "...                                                 ...            ...   \n",
       "4234                                                     closed choice   \n",
       "4235                                                     closed choice   \n",
       "4237                                                     closed choice   \n",
       "4238                                                     closed choice   \n",
       "4239  A bulldozer clears a path for a new road. A fo...  closed choice   \n",
       "\n",
       "       grade          subject       topic                            category   \n",
       "2     grade5   social science  us-history   English colonies in North America  \\\n",
       "5     grade5  natural science     biology                          Ecosystems   \n",
       "9     grade8  natural science     biology                     Genes to traits   \n",
       "10    grade4  natural science     physics                           Materials   \n",
       "13    grade8  natural science     physics  Velocity, acceleration, and forces   \n",
       "...      ...              ...         ...                                 ...   \n",
       "4234  grade3   social science   geography                           Geography   \n",
       "4235  grade5   social science   geography               Oceans and continents   \n",
       "4237  grade3   social science   geography                           Geography   \n",
       "4238  grade5   social science   geography               Oceans and continents   \n",
       "4239  grade4  natural science     physics                    Force and motion   \n",
       "\n",
       "                                                  skill   \n",
       "2                        Identify the Thirteen Colonies  \\\n",
       "5                                Interpret food webs II   \n",
       "9     Use Punnett squares to calculate ratios of off...   \n",
       "10                        Compare properties of objects   \n",
       "13                Compare magnitudes of magnetic forces   \n",
       "...                                                 ...   \n",
       "4234                     Identify oceans and continents   \n",
       "4235                     Identify oceans and continents   \n",
       "4237                    Read a map: cardinal directions   \n",
       "4238                     Identify oceans and continents   \n",
       "4239                      Identify directions of forces   \n",
       "\n",
       "                                                lecture   \n",
       "2                                                        \\\n",
       "5     A food web is a model.\\nA food web shows where...   \n",
       "9     Offspring phenotypes: dominant or recessive?\\n...   \n",
       "10    An object has different properties. A property...   \n",
       "13    Magnets can pull or push on each other without...   \n",
       "...                                                 ...   \n",
       "4234  A continent is one of the seven largest areas ...   \n",
       "4235  A continent is one of the major land masses on...   \n",
       "4237  Maps have four cardinal directions, or main di...   \n",
       "4238  A continent is one of the major land masses on...   \n",
       "4239  A force is a push or a pull that one object ap...   \n",
       "\n",
       "                                               solution  \n",
       "2     The colony is New Hampshire.\\nDuring the colon...  \n",
       "5     Use the arrows to follow how matter moves thro...  \n",
       "9     To determine how many boxes in the Punnett squ...  \n",
       "10    Look at each object.\\nFor each object, decide ...  \n",
       "13    Magnet sizes affect the magnitude of the magne...  \n",
       "...                                                 ...  \n",
       "4234                   This continent is North America.  \n",
       "4235                   This continent is South America.  \n",
       "4237  To find the answer, look at the compass rose. ...  \n",
       "4238                          This continent is Europe.  \n",
       "4239  The bulldozer pushes the loose dirt. The direc...  \n",
       "\n",
       "[1836 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>hint</th>\n",
       "      <th>task</th>\n",
       "      <th>grade</th>\n",
       "      <th>subject</th>\n",
       "      <th>topic</th>\n",
       "      <th>category</th>\n",
       "      <th>skill</th>\n",
       "      <th>lecture</th>\n",
       "      <th>solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the name of the colony shown?</td>\n",
       "      <td>[Maryland, New Hampshire, Rhode Island, Vermont]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>us-history</td>\n",
       "      <td>English colonies in North America</td>\n",
       "      <td>Identify the Thirteen Colonies</td>\n",
       "      <td></td>\n",
       "      <td>The colony is New Hampshire.\\nDuring the colon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which of these organisms contains matter that ...</td>\n",
       "      <td>[bilberry, mushroom]</td>\n",
       "      <td>1</td>\n",
       "      <td>Below is a food web from a tundra ecosystem in...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>natural science</td>\n",
       "      <td>biology</td>\n",
       "      <td>Ecosystems</td>\n",
       "      <td>Interpret food webs II</td>\n",
       "      <td>A food web is a model.\\nA food web shows where...</td>\n",
       "      <td>Use the arrows to follow how matter moves thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the expected ratio of offspring with a...</td>\n",
       "      <td>[0:4, 4:0, 2:2, 1:3, 3:1]</td>\n",
       "      <td>1</td>\n",
       "      <td>This passage describes the fleece type trait i...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade8</td>\n",
       "      <td>natural science</td>\n",
       "      <td>biology</td>\n",
       "      <td>Genes to traits</td>\n",
       "      <td>Use Punnett squares to calculate ratios of off...</td>\n",
       "      <td>Offspring phenotypes: dominant or recessive?\\n...</td>\n",
       "      <td>To determine how many boxes in the Punnett squ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which property do these three objects have in ...</td>\n",
       "      <td>[shiny, slippery, opaque]</td>\n",
       "      <td>2</td>\n",
       "      <td>Select the best answer.</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade4</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Compare properties of objects</td>\n",
       "      <td>An object has different properties. A property...</td>\n",
       "      <td>Look at each object.\\nFor each object, decide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Think about the magnetic force between the mag...</td>\n",
       "      <td>[The magnitude of the magnetic force is the sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>The images below show two pairs of magnets. Th...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade8</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Velocity, acceleration, and forces</td>\n",
       "      <td>Compare magnitudes of magnetic forces</td>\n",
       "      <td>Magnets can pull or push on each other without...</td>\n",
       "      <td>Magnet sizes affect the magnitude of the magne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[North America, South America, Antarctica, Aus...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade3</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Geography</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the seven largest areas ...</td>\n",
       "      <td>This continent is North America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[Africa, South America, North America, Asia]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Oceans and continents</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the major land masses on...</td>\n",
       "      <td>This continent is South America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which of these states is farthest west?</td>\n",
       "      <td>[Alabama, Illinois, South Carolina, Connecticut]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade3</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Geography</td>\n",
       "      <td>Read a map: cardinal directions</td>\n",
       "      <td>Maps have four cardinal directions, or main di...</td>\n",
       "      <td>To find the answer, look at the compass rose. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[Asia, Europe, Australia, North America]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Oceans and continents</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the major land masses on...</td>\n",
       "      <td>This continent is Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the direction of this push?</td>\n",
       "      <td>[away from the bulldozer, toward the bulldozer]</td>\n",
       "      <td>0</td>\n",
       "      <td>A bulldozer clears a path for a new road. A fo...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade4</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Force and motion</td>\n",
       "      <td>Identify directions of forces</td>\n",
       "      <td>A force is a push or a pull that one object ap...</td>\n",
       "      <td>The bulldozer pushes the loose dirt. The direc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1836 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:32:14.200387Z",
     "start_time": "2025-01-04T15:32:14.146164Z"
    }
   },
   "cell_type": "code",
   "source": "data['input'] = data.apply(lambda row: build_prompt(row)[0], axis=1)",
   "id": "cab1fd41dd94d085",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:32:51.192454Z",
     "start_time": "2025-01-04T15:32:51.156359Z"
    }
   },
   "cell_type": "code",
   "source": "data['message'] = data.apply(lambda row: build_message(row), axis=1)",
   "id": "b67bbb30d44177d2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:39:41.386997Z",
     "start_time": "2025-01-04T16:39:40.493218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = []\n",
    "for i in range(20):\n",
    "    row = data.iloc[i]\n",
    "    message = processor.apply_chat_template(row['message'], tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(row['message'])\n",
    "    inputs = processor(\n",
    "        text=[message],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    test_data.append(inputs)\n",
    "    data[\"inputs\"] = inputs"
   ],
   "id": "ef88d62dd71e2813",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:27:39.784090Z",
     "start_time": "2025-01-04T16:27:06.547955Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = prefix_tuning_model.generate(test_data[0], max_new_tokens=128)",
   "id": "3c2b1cb3c23a2a2b",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:33:16.853595Z",
     "start_time": "2025-01-04T16:33:16.844040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(test_data[0].input_ids, generated_ids)\n",
    "]\n",
    "output_text_prefix = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ],
   "id": "32be5a5dbf6bd30f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:21:09.236781Z",
     "start_time": "2025-01-04T16:21:09.232320Z"
    }
   },
   "cell_type": "code",
   "source": "output_text # output of base model for index 0",
   "id": "68236e9a475c411e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Answer: (1) New Hampshire']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:20:59.476016Z",
     "start_time": "2025-01-04T16:16:00.738916Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = model.generate(**test_data[0], max_new_tokens=128)",
   "id": "bcdb760f7d539f70",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:33:19.513931Z",
     "start_time": "2025-01-04T16:33:19.505400Z"
    }
   },
   "cell_type": "code",
   "source": "output_text_prefix",
   "id": "9f7191e24b1d358d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T20:02:17.030658Z",
     "start_time": "2025-01-04T20:02:17.016825Z"
    }
   },
   "cell_type": "code",
   "source": "help(model.forward)",
   "id": "eabce554786dabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module transformers.models.qwen2_vl.modeling_qwen2_vl:\n",
      "\n",
      "forward(input_ids: torch.LongTensor = None, attention_mask: Optional[torch.Tensor] = None, position_ids: Optional[torch.LongTensor] = None, past_key_values: Optional[List[torch.FloatTensor]] = None, inputs_embeds: Optional[torch.FloatTensor] = None, labels: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = None, output_attentions: Optional[bool] = None, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None, pixel_values: Optional[torch.Tensor] = None, pixel_values_videos: Optional[torch.FloatTensor] = None, image_grid_thw: Optional[torch.LongTensor] = None, video_grid_thw: Optional[torch.LongTensor] = None, rope_deltas: Optional[torch.LongTensor] = None) -> Union[Tuple, transformers.models.qwen2_vl.modeling_qwen2_vl.Qwen2VLCausalLMOutputWithPast] method of transformers.models.qwen2_vl.modeling_qwen2_vl.Qwen2VLForConditionalGeneration instance\n",
      "    The [`Qwen2VLForConditionalGeneration`] forward method, overrides the `__call__` special method.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
      "    instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
      "    the latter silently ignores them.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Args:\n",
      "        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n",
      "            it.\n",
      "    \n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "    \n",
      "            [What are input IDs?](../glossary#input-ids)\n",
      "        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
      "    \n",
      "            - 1 for tokens that are **not masked**,\n",
      "            - 0 for tokens that are **masked**.\n",
      "    \n",
      "            [What are attention masks?](../glossary#attention-mask)\n",
      "    \n",
      "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "            [`PreTrainedTokenizer.__call__`] for details.\n",
      "    \n",
      "            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n",
      "            `past_key_values`).\n",
      "    \n",
      "            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n",
      "            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n",
      "            information on the default strategy.\n",
      "    \n",
      "            - 1 indicates the head is **not masked**,\n",
      "            - 0 indicates the head is **masked**.\n",
      "        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n",
      "            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n",
      "        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n",
      "            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n",
      "    \n",
      "            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n",
      "            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n",
      "    \n",
      "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
      "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
      "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
      "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
      "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
      "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
      "            model's internal embedding lookup matrix.\n",
      "        use_cache (`bool`, *optional*):\n",
      "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
      "            `past_key_values`).\n",
      "        output_attentions (`bool`, *optional*):\n",
      "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "            tensors for more detail.\n",
      "        output_hidden_states (`bool`, *optional*):\n",
      "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "            more detail.\n",
      "        return_dict (`bool`, *optional*):\n",
      "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):\n",
      "            The tensors corresponding to the input images. Pixel values can be obtained using\n",
      "            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses\n",
      "            [`Qwen2VLImageProcessor`] for processing images.\n",
      "        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n",
      "            The tensors corresponding to the input videos. Pixel values can be obtained using\n",
      "            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses\n",
      "            [`Qwen2VLImageProcessor`] for processing videos.\n",
      "        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n",
      "            The temporal, height and width of feature shape of each image in LLM.\n",
      "        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n",
      "            The temporal, height and width of feature shape of each video in LLM.\n",
      "        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n",
      "            The rope index difference between sequence length and multimodal rope.\n",
      "    \n",
      "        Args:\n",
      "            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
      "                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
      "                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
      "    \n",
      "    \n",
      "        Returns:\n",
      "            [`transformers.models.qwen2_vl.modeling_qwen2_vl.Qwen2VLCausalLMOutputWithPast`] or `tuple(torch.FloatTensor)`: A [`transformers.models.qwen2_vl.modeling_qwen2_vl.Qwen2VLCausalLMOutputWithPast`] or a tuple of\n",
      "            `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
      "            elements depending on the configuration ([`Qwen2VLConfig`]) and inputs.\n",
      "    \n",
      "            - **loss** (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) -- Language modeling loss (for next-token prediction).\n",
      "            - **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) -- Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
      "            - **past_key_values** (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`) -- Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "              `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n",
      "    \n",
      "              Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n",
      "              `past_key_values` input) to speed up sequential decoding.\n",
      "            - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) -- Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
      "              one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
      "    \n",
      "              Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
      "            - **attentions** (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) -- Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
      "              sequence_length)`.\n",
      "    \n",
      "              Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
      "              heads.\n",
      "            - **rope_deltas** (`torch.LongTensor` of shape `(batch_size, )`, *optional*) -- The rope index difference between sequence length and multimodal rope.\n",
      "      \n",
      "    \n",
      "        Example:\n",
      "    \n",
      "        ```python\n",
      "        >>> from PIL import Image\n",
      "        >>> import requests\n",
      "        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
      "    \n",
      "        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
      "        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
      "    \n",
      "        >>> messages = [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": [\n",
      "                    {\"type\": \"image\"},\n",
      "                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
      "                ],\n",
      "            },\n",
      "        ]\n",
      "        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
      "        >>> image = Image.open(requests.get(url, stream=True).raw)\n",
      "    \n",
      "        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
      "        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n",
      "    \n",
      "        >>> # Generate\n",
      "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
      "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n",
      "        ```\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:28:37.848377Z",
     "start_time": "2025-01-04T16:28:37.826436Z"
    }
   },
   "cell_type": "code",
   "source": "inputs_embeds = model.get_input_embeddings()(test_data[0][\"input_ids\"])",
   "id": "9a79108f92d7f93a",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:32:48.950425Z",
     "start_time": "2025-01-04T16:29:52.017256Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = model.generate(inputs_embeds=inputs_embeds, attention_mask=test_data[0][\"attention_mask\"], pixel_values=test_data[0][\"pixel_values\"], max_new_tokens=128)",
   "id": "9b01484d9c42e853",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:09:08.805520Z",
     "start_time": "2025-01-04T16:57:51.294728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_texts = []\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(30)):\n",
    "        max_length = test_data[i][\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "        labels = tokenizer(data.iloc[i][\"solution\"], padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        outputs = prefix_tuning_model(test_data[i], labels=labels)\n",
    "        output_ids = outputs.logits.argmax(-1)\n",
    "        output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        output_texts.append(output_text)"
   ],
   "id": "e8d1b474bc1107d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [11:17<05:38, 33.86s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[120], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m30\u001B[39m)):\n\u001B[0;32m----> 4\u001B[0m         max_length \u001B[38;5;241m=\u001B[39m \u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;66;03m# +10 for later prefix\u001B[39;00m\n\u001B[1;32m      5\u001B[0m         labels \u001B[38;5;241m=\u001B[39m tokenizer(data\u001B[38;5;241m.\u001B[39miloc[i][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msolution\u001B[39m\u001B[38;5;124m\"\u001B[39m], padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39mmax_length, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      6\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m prefix_tuning_model(test_data[i], labels\u001B[38;5;241m=\u001B[39mlabels)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:12:54.127986Z",
     "start_time": "2025-01-04T17:12:54.090488Z"
    }
   },
   "cell_type": "code",
   "source": "output_texts",
   "id": "eb9027c0df783601",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[')1))'],\n",
       " [') matter. web the. a is.'],\n",
       " [')4)24)4, cut toeces, for. is are aly.. a of the is. is is is for hairyly fleece isF)'],\n",
       " [')'],\n",
       " [' of the) of the magnetic force is smaller in pair 1 of the., are...'],\n",
       " [')3)'],\n",
       " [') the is'],\n",
       " [')1) matter in matter web model matter matter eaten through the. a matter.'],\n",
       " [' between force is2) magnetic is is force is stronger magnetic of.'],\n",
       " [' forest year. has forestree forest forest'],\n",
       " [')))'],\n",
       " [')1)) ('],\n",
       " [\"ieie) Allie can trade get oranges) All for Allie's can All All the All get. something something something or to directly.ie for the and. something tomatoes...ie. something. tomatoes. the of.\"],\n",
       " ['))3)'],\n",
       " ['))'],\n",
       " [''],\n",
       " [''],\n",
       " [')'],\n",
       " [')))'],\n",
       " ['']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6cf2833919bc0e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
