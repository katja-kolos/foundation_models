{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T16:52:40.886501Z",
     "start_time": "2025-01-04T16:52:40.856341Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from helpers import *\n",
    "import pandas as pd\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText\n",
    "import prefix_tuning\n",
    "import importlib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "importlib.reload(prefix_tuning)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prefix_tuning' from '/Users/floriandreyer/Library/Mobile Documents/com~apple~CloudDocs/Python Projekte/foundation_models/prefix_tuning.py'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:58:49.817200Z",
     "start_time": "2025-01-04T15:58:49.798617Z"
    }
   },
   "cell_type": "code",
   "source": "prefix_tuning_layer = torch.load(\"QWEN_PREFIX_TUNING_10/prefix_tuning.pt\", map_location=torch.device('cpu'))",
   "id": "910f26a8c3caa936",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:02:31.493398Z",
     "start_time": "2025-01-04T16:02:05.725233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32\n",
    ")"
   ],
   "id": "a2bdfd18273af56b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efcc52295f9443bd8a6e354459b0204b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:59:14.758148Z",
     "start_time": "2025-01-04T15:59:14.742571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_prefix_tuning_layer= prefix_tuning.loaded_prefix_tuning = prefix_tuning.PrefixTuning(model.config, prefix_length=10)\n",
    "model_prefix_tuning_layer.load_state_dict(prefix_tuning_layer)"
   ],
   "id": "38896aea83c321c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:59:15.809090Z",
     "start_time": "2025-01-04T15:59:15.805353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix_tuning_model = prefix_tuning.PrefixTuningModel(model, tokenizer, prefix_length=10)\n",
    "prefix_tuning_model.prefix_tuning = model_prefix_tuning_layer"
   ],
   "id": "d1c8dd544056c05a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:23:12.933524Z",
     "start_time": "2025-01-04T16:23:12.913271Z"
    }
   },
   "cell_type": "code",
   "source": "prefix_tuning_model",
   "id": "9823c6fb56f1b812",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixTuningModel(\n",
       "  (model): Qwen2VLForConditionalGeneration(\n",
       "    (visual): Qwen2VisionTransformerPretrainedModel(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2VLVisionBlock(\n",
       "          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): VisionSdpaAttention(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): VisionMlp(\n",
       "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (act): QuickGELUActivation()\n",
       "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): PatchMerger(\n",
       "        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (model): Qwen2VLModel(\n",
       "      (embed_tokens): Embedding(151936, 1536)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2VLDecoderLayer(\n",
       "          (self_attn): Qwen2VLSdpaAttention(\n",
       "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2VLRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       "  )\n",
       "  (prefix_tuning): PrefixTuning()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:30:23.627180Z",
     "start_time": "2025-01-04T15:30:14.726552Z"
    }
   },
   "cell_type": "code",
   "source": "data = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='test'))",
   "id": "99c01cee30067835",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:31:31.465157Z",
     "start_time": "2025-01-04T15:31:31.434697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.dropna(inplace=True)\n",
    "data"
   ],
   "id": "85376946f607a99e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                  image   \n",
       "2     <PIL.PngImagePlugin.PngImageFile image mode=RG...  \\\n",
       "5     <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "9     <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "10    <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "13    <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "...                                                 ...   \n",
       "4234  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4235  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4237  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4238  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "4239  <PIL.PngImagePlugin.PngImageFile image mode=RG...   \n",
       "\n",
       "                                               question   \n",
       "2                 What is the name of the colony shown?  \\\n",
       "5     Which of these organisms contains matter that ...   \n",
       "9     What is the expected ratio of offspring with a...   \n",
       "10    Which property do these three objects have in ...   \n",
       "13    Think about the magnetic force between the mag...   \n",
       "...                                                 ...   \n",
       "4234                    Which continent is highlighted?   \n",
       "4235                    Which continent is highlighted?   \n",
       "4237            Which of these states is farthest west?   \n",
       "4238                    Which continent is highlighted?   \n",
       "4239                What is the direction of this push?   \n",
       "\n",
       "                                                choices  answer   \n",
       "2      [Maryland, New Hampshire, Rhode Island, Vermont]       1  \\\n",
       "5                                  [bilberry, mushroom]       1   \n",
       "9                             [0:4, 4:0, 2:2, 1:3, 3:1]       1   \n",
       "10                            [shiny, slippery, opaque]       2   \n",
       "13    [The magnitude of the magnetic force is the sa...       2   \n",
       "...                                                 ...     ...   \n",
       "4234  [North America, South America, Antarctica, Aus...       0   \n",
       "4235       [Africa, South America, North America, Asia]       1   \n",
       "4237   [Alabama, Illinois, South Carolina, Connecticut]       1   \n",
       "4238           [Asia, Europe, Australia, North America]       1   \n",
       "4239    [away from the bulldozer, toward the bulldozer]       0   \n",
       "\n",
       "                                                   hint           task   \n",
       "2                                                        closed choice  \\\n",
       "5     Below is a food web from a tundra ecosystem in...  closed choice   \n",
       "9     This passage describes the fleece type trait i...  closed choice   \n",
       "10                              Select the best answer.  closed choice   \n",
       "13    The images below show two pairs of magnets. Th...  closed choice   \n",
       "...                                                 ...            ...   \n",
       "4234                                                     closed choice   \n",
       "4235                                                     closed choice   \n",
       "4237                                                     closed choice   \n",
       "4238                                                     closed choice   \n",
       "4239  A bulldozer clears a path for a new road. A fo...  closed choice   \n",
       "\n",
       "       grade          subject       topic                            category   \n",
       "2     grade5   social science  us-history   English colonies in North America  \\\n",
       "5     grade5  natural science     biology                          Ecosystems   \n",
       "9     grade8  natural science     biology                     Genes to traits   \n",
       "10    grade4  natural science     physics                           Materials   \n",
       "13    grade8  natural science     physics  Velocity, acceleration, and forces   \n",
       "...      ...              ...         ...                                 ...   \n",
       "4234  grade3   social science   geography                           Geography   \n",
       "4235  grade5   social science   geography               Oceans and continents   \n",
       "4237  grade3   social science   geography                           Geography   \n",
       "4238  grade5   social science   geography               Oceans and continents   \n",
       "4239  grade4  natural science     physics                    Force and motion   \n",
       "\n",
       "                                                  skill   \n",
       "2                        Identify the Thirteen Colonies  \\\n",
       "5                                Interpret food webs II   \n",
       "9     Use Punnett squares to calculate ratios of off...   \n",
       "10                        Compare properties of objects   \n",
       "13                Compare magnitudes of magnetic forces   \n",
       "...                                                 ...   \n",
       "4234                     Identify oceans and continents   \n",
       "4235                     Identify oceans and continents   \n",
       "4237                    Read a map: cardinal directions   \n",
       "4238                     Identify oceans and continents   \n",
       "4239                      Identify directions of forces   \n",
       "\n",
       "                                                lecture   \n",
       "2                                                        \\\n",
       "5     A food web is a model.\\nA food web shows where...   \n",
       "9     Offspring phenotypes: dominant or recessive?\\n...   \n",
       "10    An object has different properties. A property...   \n",
       "13    Magnets can pull or push on each other without...   \n",
       "...                                                 ...   \n",
       "4234  A continent is one of the seven largest areas ...   \n",
       "4235  A continent is one of the major land masses on...   \n",
       "4237  Maps have four cardinal directions, or main di...   \n",
       "4238  A continent is one of the major land masses on...   \n",
       "4239  A force is a push or a pull that one object ap...   \n",
       "\n",
       "                                               solution  \n",
       "2     The colony is New Hampshire.\\nDuring the colon...  \n",
       "5     Use the arrows to follow how matter moves thro...  \n",
       "9     To determine how many boxes in the Punnett squ...  \n",
       "10    Look at each object.\\nFor each object, decide ...  \n",
       "13    Magnet sizes affect the magnitude of the magne...  \n",
       "...                                                 ...  \n",
       "4234                   This continent is North America.  \n",
       "4235                   This continent is South America.  \n",
       "4237  To find the answer, look at the compass rose. ...  \n",
       "4238                          This continent is Europe.  \n",
       "4239  The bulldozer pushes the loose dirt. The direc...  \n",
       "\n",
       "[1836 rows x 13 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answer</th>\n",
       "      <th>hint</th>\n",
       "      <th>task</th>\n",
       "      <th>grade</th>\n",
       "      <th>subject</th>\n",
       "      <th>topic</th>\n",
       "      <th>category</th>\n",
       "      <th>skill</th>\n",
       "      <th>lecture</th>\n",
       "      <th>solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the name of the colony shown?</td>\n",
       "      <td>[Maryland, New Hampshire, Rhode Island, Vermont]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>us-history</td>\n",
       "      <td>English colonies in North America</td>\n",
       "      <td>Identify the Thirteen Colonies</td>\n",
       "      <td></td>\n",
       "      <td>The colony is New Hampshire.\\nDuring the colon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which of these organisms contains matter that ...</td>\n",
       "      <td>[bilberry, mushroom]</td>\n",
       "      <td>1</td>\n",
       "      <td>Below is a food web from a tundra ecosystem in...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>natural science</td>\n",
       "      <td>biology</td>\n",
       "      <td>Ecosystems</td>\n",
       "      <td>Interpret food webs II</td>\n",
       "      <td>A food web is a model.\\nA food web shows where...</td>\n",
       "      <td>Use the arrows to follow how matter moves thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the expected ratio of offspring with a...</td>\n",
       "      <td>[0:4, 4:0, 2:2, 1:3, 3:1]</td>\n",
       "      <td>1</td>\n",
       "      <td>This passage describes the fleece type trait i...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade8</td>\n",
       "      <td>natural science</td>\n",
       "      <td>biology</td>\n",
       "      <td>Genes to traits</td>\n",
       "      <td>Use Punnett squares to calculate ratios of off...</td>\n",
       "      <td>Offspring phenotypes: dominant or recessive?\\n...</td>\n",
       "      <td>To determine how many boxes in the Punnett squ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which property do these three objects have in ...</td>\n",
       "      <td>[shiny, slippery, opaque]</td>\n",
       "      <td>2</td>\n",
       "      <td>Select the best answer.</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade4</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Compare properties of objects</td>\n",
       "      <td>An object has different properties. A property...</td>\n",
       "      <td>Look at each object.\\nFor each object, decide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Think about the magnetic force between the mag...</td>\n",
       "      <td>[The magnitude of the magnetic force is the sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>The images below show two pairs of magnets. Th...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade8</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Velocity, acceleration, and forces</td>\n",
       "      <td>Compare magnitudes of magnetic forces</td>\n",
       "      <td>Magnets can pull or push on each other without...</td>\n",
       "      <td>Magnet sizes affect the magnitude of the magne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[North America, South America, Antarctica, Aus...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade3</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Geography</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the seven largest areas ...</td>\n",
       "      <td>This continent is North America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[Africa, South America, North America, Asia]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Oceans and continents</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the major land masses on...</td>\n",
       "      <td>This continent is South America.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which of these states is farthest west?</td>\n",
       "      <td>[Alabama, Illinois, South Carolina, Connecticut]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade3</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Geography</td>\n",
       "      <td>Read a map: cardinal directions</td>\n",
       "      <td>Maps have four cardinal directions, or main di...</td>\n",
       "      <td>To find the answer, look at the compass rose. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>Which continent is highlighted?</td>\n",
       "      <td>[Asia, Europe, Australia, North America]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade5</td>\n",
       "      <td>social science</td>\n",
       "      <td>geography</td>\n",
       "      <td>Oceans and continents</td>\n",
       "      <td>Identify oceans and continents</td>\n",
       "      <td>A continent is one of the major land masses on...</td>\n",
       "      <td>This continent is Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>&lt;PIL.PngImagePlugin.PngImageFile image mode=RG...</td>\n",
       "      <td>What is the direction of this push?</td>\n",
       "      <td>[away from the bulldozer, toward the bulldozer]</td>\n",
       "      <td>0</td>\n",
       "      <td>A bulldozer clears a path for a new road. A fo...</td>\n",
       "      <td>closed choice</td>\n",
       "      <td>grade4</td>\n",
       "      <td>natural science</td>\n",
       "      <td>physics</td>\n",
       "      <td>Force and motion</td>\n",
       "      <td>Identify directions of forces</td>\n",
       "      <td>A force is a push or a pull that one object ap...</td>\n",
       "      <td>The bulldozer pushes the loose dirt. The direc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1836 rows × 13 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:32:14.200387Z",
     "start_time": "2025-01-04T15:32:14.146164Z"
    }
   },
   "cell_type": "code",
   "source": "data['input'] = data.apply(lambda row: build_prompt(row)[0], axis=1)",
   "id": "cab1fd41dd94d085",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T15:32:51.192454Z",
     "start_time": "2025-01-04T15:32:51.156359Z"
    }
   },
   "cell_type": "code",
   "source": "data['message'] = data.apply(lambda row: build_message(row), axis=1)",
   "id": "b67bbb30d44177d2",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:39:41.386997Z",
     "start_time": "2025-01-04T16:39:40.493218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = []\n",
    "for i in range(20):\n",
    "    row = data.iloc[i]\n",
    "    message = processor.apply_chat_template(row['message'], tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(row['message'])\n",
    "    inputs = processor(\n",
    "        text=[message],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    test_data.append(inputs)\n",
    "    data[\"inputs\"] = inputs"
   ],
   "id": "ef88d62dd71e2813",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:27:39.784090Z",
     "start_time": "2025-01-04T16:27:06.547955Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = prefix_tuning_model.generate(test_data[0], max_new_tokens=128)",
   "id": "3c2b1cb3c23a2a2b",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:33:16.853595Z",
     "start_time": "2025-01-04T16:33:16.844040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(test_data[0].input_ids, generated_ids)\n",
    "]\n",
    "output_text_prefix = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ],
   "id": "32be5a5dbf6bd30f",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:21:09.236781Z",
     "start_time": "2025-01-04T16:21:09.232320Z"
    }
   },
   "cell_type": "code",
   "source": "output_text # output of base model for index 0",
   "id": "68236e9a475c411e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Answer: (1) New Hampshire']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:20:59.476016Z",
     "start_time": "2025-01-04T16:16:00.738916Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = model.generate(**test_data[0], max_new_tokens=128)",
   "id": "bcdb760f7d539f70",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:33:19.513931Z",
     "start_time": "2025-01-04T16:33:19.505400Z"
    }
   },
   "cell_type": "code",
   "source": "output_text_prefix",
   "id": "9f7191e24b1d358d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:21:21.855143Z",
     "start_time": "2025-01-04T16:21:21.820658Z"
    }
   },
   "cell_type": "code",
   "source": "help(model.generate)",
   "id": "eabce554786dabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor] method of transformers.models.qwen2_vl.modeling_qwen2_vl.Qwen2VLForConditionalGeneration instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config ([`~generation.GenerationConfig`], *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which has the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "            intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n",
      "            to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n",
      "            deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "            size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "        negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Attention_mask for `negative_prompt_ids`.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:28:37.848377Z",
     "start_time": "2025-01-04T16:28:37.826436Z"
    }
   },
   "cell_type": "code",
   "source": "inputs_embeds = model.get_input_embeddings()(test_data[0][\"input_ids\"])",
   "id": "9a79108f92d7f93a",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T16:32:48.950425Z",
     "start_time": "2025-01-04T16:29:52.017256Z"
    }
   },
   "cell_type": "code",
   "source": "generated_ids = model.generate(inputs_embeds=inputs_embeds, attention_mask=test_data[0][\"attention_mask\"], pixel_values=test_data[0][\"pixel_values\"], max_new_tokens=128)",
   "id": "9b01484d9c42e853",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:09:08.805520Z",
     "start_time": "2025-01-04T16:57:51.294728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_texts = []\n",
    "with torch.inference_mode():\n",
    "    for i in tqdm(range(30)):\n",
    "        max_length = test_data[i][\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "        labels = tokenizer(data.iloc[i][\"solution\"], padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        outputs = prefix_tuning_model(test_data[i], labels=labels)\n",
    "        output_ids = outputs.logits.argmax(-1)\n",
    "        output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        output_texts.append(output_text)"
   ],
   "id": "e8d1b474bc1107d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [11:17<05:38, 33.86s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[120], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m30\u001B[39m)):\n\u001B[0;32m----> 4\u001B[0m         max_length \u001B[38;5;241m=\u001B[39m \u001B[43mtest_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;66;03m# +10 for later prefix\u001B[39;00m\n\u001B[1;32m      5\u001B[0m         labels \u001B[38;5;241m=\u001B[39m tokenizer(data\u001B[38;5;241m.\u001B[39miloc[i][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msolution\u001B[39m\u001B[38;5;124m\"\u001B[39m], padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39mmax_length, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      6\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m prefix_tuning_model(test_data[i], labels\u001B[38;5;241m=\u001B[39mlabels)\n",
      "\u001B[0;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T17:12:54.127986Z",
     "start_time": "2025-01-04T17:12:54.090488Z"
    }
   },
   "cell_type": "code",
   "source": "output_texts",
   "id": "eb9027c0df783601",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[')1))'],\n",
       " [') matter. web the. a is.'],\n",
       " [')4)24)4, cut toeces, for. is are aly.. a of the is. is is is for hairyly fleece isF)'],\n",
       " [')'],\n",
       " [' of the) of the magnetic force is smaller in pair 1 of the., are...'],\n",
       " [')3)'],\n",
       " [') the is'],\n",
       " [')1) matter in matter web model matter matter eaten through the. a matter.'],\n",
       " [' between force is2) magnetic is is force is stronger magnetic of.'],\n",
       " [' forest year. has forestree forest forest'],\n",
       " [')))'],\n",
       " [')1)) ('],\n",
       " [\"ieie) Allie can trade get oranges) All for Allie's can All All the All get. something something something or to directly.ie for the and. something tomatoes...ie. something. tomatoes. the of.\"],\n",
       " ['))3)'],\n",
       " ['))'],\n",
       " [''],\n",
       " [''],\n",
       " [')'],\n",
       " [')))'],\n",
       " ['']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e6cf2833919bc0e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
