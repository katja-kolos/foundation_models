{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a91db34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  9 12:56:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:15:00.0 Off |                  Off |\n",
      "| 30%   30C    P8             12W /  300W |      18MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:2D:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              9W /  300W |     121MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5670      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      5670      G   /usr/lib/xorg/Xorg                            107MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c77f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 12:56:12.080154: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-09 12:56:12.093934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736423772.110794  524273 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736423772.115846  524273 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-09 12:56:12.133494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dd28a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"HuggingFaceM4/ChartQA\"\n",
    "dataset_id = \"derek-thomas/ScienceQA\"\n",
    "#TODO: DON'T FORGET TO HAVE THE ENTIRE DATASET\n",
    "train_dataset, eval_dataset, test_dataset = load_dataset(dataset_id, split=[\"train\", \"validation\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6388d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_question_text(problem):\n",
    "    question = problem['question']\n",
    "    return question\n",
    "\n",
    "\n",
    "def get_choice_text(probelm, options):\n",
    "    choices = probelm['choices']\n",
    "    choice_list = []\n",
    "    for i, c in enumerate(choices):\n",
    "        choice_list.append(\"({}) {}\".format(options[i], c))\n",
    "    choice_txt = \" \".join(choice_list)\n",
    "    return choice_txt\n",
    "\n",
    "\n",
    "def get_context_text(problem, use_caption):\n",
    "    txt_context = problem['hint']\n",
    "    img_context = problem['caption'] if use_caption else \"\"\n",
    "    context = \" \".join([txt_context, img_context]).strip()\n",
    "    if context == \"\":\n",
    "        context = \"N/A\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def build_prompt(question_data, use_lecture=False, use_solution=False):\n",
    "    question = get_question_text(question_data)\n",
    "    choices = get_choice_text(question_data, [choice_num for choice_num in range(5)])\n",
    "    hint = get_context_text(question_data, False)\n",
    "    task = question_data['task']\n",
    "    input_prompt = f'Question: {question}\\n Task: {task}\\n Choices: {choices}\\n Hint: {hint}'\n",
    "    if use_lecture:\n",
    "        lecture = f'\\n Lecture: {question_data[\"lecture\"]}'\n",
    "        input_prompt += lecture\n",
    "    if use_solution and question_data[\"solution\"]:\n",
    "        solution = f'\\n Solution: {question_data[\"solution\"]}'\n",
    "        input_prompt += solution\n",
    "    return input_prompt\n",
    "\n",
    "# def build_message(row):\n",
    "#     row_input = build_prompt(row)\n",
    "#     image = row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [\n",
    "#                 {\n",
    "#                     \"type\": \"image\",\n",
    "#                     \"image\": image,\n",
    "#                 },\n",
    "#                 {\"type\": \"text\", \"text\": row_input },\n",
    "#             ],\n",
    "#         }\n",
    "#     ]\n",
    "#     return messages\n",
    "\n",
    "def build_message_gemini(row):\n",
    "    row_input = row\n",
    "    image = row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": row_input },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5fd72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(lambda example: example['solution']!=\"\")\n",
    "eval_dataset = eval_dataset.filter(lambda example: example['solution']!=\"\")\n",
    "test_dataset = test_dataset.filter(lambda example: (example['solution']!=\"\") & (example['lecture']!=\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e8a4e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
       "    num_rows: 11515\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "698450fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
       "    num_rows: 3848\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9dc690c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'question', 'choices', 'answer', 'hint', 'task', 'grade', 'subject', 'topic', 'category', 'skill', 'lecture', 'solution'],\n",
       "    num_rows: 3172\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb33d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "train_dataset_gemini['solution'] = train_dataset_gemini['explanation']\n",
    "del train_dataset_gemini['explanation']\n",
    "train_dataset_df = pd.DataFrame(train_dataset).reset_index()\n",
    "train_dataset_gemini = pd.merge(train_dataset_gemini, train_dataset_df[['index', 'image']], on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214fa0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_qwen_gemini = [(build_message_gemini(sample[1]), sample[1][\"solution\"]) for sample in train_dataset_gemini.iterrows()]\n",
    "train_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in train_dataset]\n",
    "eval_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in eval_dataset]\n",
    "test_dataset_qwen = [(build_message(sample), sample[\"solution\"]) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8260ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_paligemma_gemini = [(sample[1][\"input\"], sample[1][\"image\"], sample[1][\"solution\"]) for sample in train_dataset_gemini.iterrows()] # sample[\"input\"] is the output of build_prompt\n",
    "# train_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in train_dataset]\n",
    "# eval_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in eval_dataset]\n",
    "# test_dataset_paligemma = [(build_prompt(sample), sample[\"image\"], sample[\"solution\"]) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06532ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=750x429>,\n",
       " 'question': 'Which of these states is farthest north?',\n",
       " 'choices': ['West Virginia', 'Louisiana', 'Arizona', 'Oklahoma'],\n",
       " 'answer': 0,\n",
       " 'hint': '',\n",
       " 'task': 'closed choice',\n",
       " 'grade': 'grade2',\n",
       " 'subject': 'social science',\n",
       " 'topic': 'geography',\n",
       " 'category': 'Geography',\n",
       " 'skill': 'Read a map: cardinal directions',\n",
       " 'lecture': 'Maps have four cardinal directions, or main directions. Those directions are north, south, east, and west.\\nA compass rose is a set of arrows that point to the cardinal directions. A compass rose usually shows only the first letter of each cardinal direction.\\nThe north arrow points to the North Pole. On most maps, north is at the top of the map.',\n",
       " 'solution': 'To find the answer, look at the compass rose. Look at which way the north arrow is pointing. West Virginia is farthest north.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "641e10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57a9ba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2a8f0b0cae434a9e606e6f6c891f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37694302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9f83ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf28bc665dd46a2b1c02c2611c8e91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f601c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,089,536 || all params: 2,210,075,136 || trainable%: 0.0493\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eba9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: SET EXPERIMENTS IN A LOOP AND MAKE IT RUN BEFORE THE FLIGHT\n",
    "\n",
    "### -> both qwen and paligemma for the normal \"label\" data and the gemini data please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36d05a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA\",  # Directory to save the model\n",
    "    num_train_epochs=10,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=False,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset\n",
    "training_args.eval_strategy = \"epoch\"\n",
    "training_args.save_strategy = \"epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd73ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatyashpr\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8c523abf524a94b87f1f195cc57444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111344342223472, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/lhome/matiada/foundation_models/wandb/run-20250109_130002-hg6puag9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA/runs/hg6puag9' target=\"_blank\">LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA</a></strong> to <a href='https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA' target=\"_blank\">https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA/runs/hg6puag9' target=\"_blank\">https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA/runs/hg6puag9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/matyashpr/LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA/runs/hg6puag9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71c7fe2e75b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA\",  # change this\n",
    "    name=\"LORA-KD-Qwen2-VL-2B-Instruct-ScienceQA\",  # change this\n",
    "    config=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d875ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn_qwen(examples):\n",
    "    \n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for (example,_) in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for (example,_) in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, padding=\"longest\", return_tensors=\"pt\"\n",
    "    ) \n",
    "    max_length = batch[\"input_ids\"].size(1)\n",
    "    example_labels = [label for (x, label) in examples]\n",
    "    labels = tokenizer(example_labels, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e26e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn_paligemma(examples):\n",
    "    texts = [text for (text, image, label) in examples]\n",
    "    image_inputs = [image.resize((224, 224)) if image else Image.new(\"RGB\", (224, 224), (0, 0, 0)) for (text, image, label) in examples]\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )\n",
    "    max_length = batch[\"input_ids\"].size(1)\n",
    "    example_labels = [label for (text, image, label) in examples]\n",
    "    labels = processor.tokenizer(example_labels, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb7b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "504ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524273/3786572145.py:3: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_qwen_gemini, #KD dataset\n",
    "    eval_dataset=eval_dataset_qwen,\n",
    "    data_collator=collate_fn_qwen,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b55ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2873' max='3590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2873/3590 5:28:38 < 1:22:04, 0.15 it/s, Epoch 8.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.369800</td>\n",
       "      <td>2.348135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.421500</td>\n",
       "      <td>2.913847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.249100</td>\n",
       "      <td>2.876669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.438000</td>\n",
       "      <td>3.000561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.342100</td>\n",
       "      <td>3.210442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.452600</td>\n",
       "      <td>3.554393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>9.702800</td>\n",
       "      <td>3.153070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='962' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/962 02:33 < 04:57, 2.13 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: 1cc0938b-01b8-468a-907d-a112395b2de2)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: 6e492ea5-2b35-484b-b101-3c08729fa2fb)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: 22e6e66c-0c78-48d1-8ee1-035f553e1c78)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: 7716a6a1-5a24-4b48-8daa-bab4b65313a5)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: e0602486-8a59-4dbf-88f8-d901f549dc14)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2-VL-2B-Instruct/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 503 Service Unavailable')))\"), '(Request ID: 6cba0050-5668-42fe-b65b-fde9086de6b2)') - silently ignoring the lookup for the file config.json in Qwen/Qwen2-VL-2B-Instruct.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in Qwen/Qwen2-VL-2B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6000eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
