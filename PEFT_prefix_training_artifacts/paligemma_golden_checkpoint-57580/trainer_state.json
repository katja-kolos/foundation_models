{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 57580,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1736714136853074,
      "grad_norm": 1.6954861879348755,
      "learning_rate": 1.9826328586314695e-05,
      "loss": 22.1697,
      "step": 500
    },
    {
      "epoch": 0.3473428273706148,
      "grad_norm": 1.1744965314865112,
      "learning_rate": 1.9652657172629385e-05,
      "loss": 17.6634,
      "step": 1000
    },
    {
      "epoch": 0.5210142410559222,
      "grad_norm": 1.2396296262741089,
      "learning_rate": 1.947898575894408e-05,
      "loss": 13.6602,
      "step": 1500
    },
    {
      "epoch": 0.6946856547412296,
      "grad_norm": 0.6464902758598328,
      "learning_rate": 1.9305314345258772e-05,
      "loss": 10.1343,
      "step": 2000
    },
    {
      "epoch": 0.868357068426537,
      "grad_norm": 1.0856627225875854,
      "learning_rate": 1.9131642931573466e-05,
      "loss": 8.2448,
      "step": 2500
    },
    {
      "epoch": 1.0420284821118444,
      "grad_norm": 0.42181888222694397,
      "learning_rate": 1.8957971517888156e-05,
      "loss": 7.0148,
      "step": 3000
    },
    {
      "epoch": 1.2156998957971519,
      "grad_norm": 0.3299279510974884,
      "learning_rate": 1.878430010420285e-05,
      "loss": 6.56,
      "step": 3500
    },
    {
      "epoch": 1.3893713094824591,
      "grad_norm": 0.5173802971839905,
      "learning_rate": 1.8610628690517543e-05,
      "loss": 6.1592,
      "step": 4000
    },
    {
      "epoch": 1.5630427231677666,
      "grad_norm": 2.009744644165039,
      "learning_rate": 1.8436957276832233e-05,
      "loss": 5.9189,
      "step": 4500
    },
    {
      "epoch": 1.7367141368530739,
      "grad_norm": 0.18562360107898712,
      "learning_rate": 1.8263285863146927e-05,
      "loss": 5.6864,
      "step": 5000
    },
    {
      "epoch": 1.9103855505383813,
      "grad_norm": 0.11717687547206879,
      "learning_rate": 1.808961444946162e-05,
      "loss": 5.4993,
      "step": 5500
    },
    {
      "epoch": 2.084056964223689,
      "grad_norm": 0.2060893177986145,
      "learning_rate": 1.7915943035776314e-05,
      "loss": 5.3125,
      "step": 6000
    },
    {
      "epoch": 2.2577283779089963,
      "grad_norm": 0.3501972556114197,
      "learning_rate": 1.7742271622091004e-05,
      "loss": 5.2023,
      "step": 6500
    },
    {
      "epoch": 2.4313997915943037,
      "grad_norm": 0.9874171614646912,
      "learning_rate": 1.7568600208405697e-05,
      "loss": 5.1099,
      "step": 7000
    },
    {
      "epoch": 2.605071205279611,
      "grad_norm": 0.2251194417476654,
      "learning_rate": 1.739492879472039e-05,
      "loss": 4.9945,
      "step": 7500
    },
    {
      "epoch": 2.7787426189649183,
      "grad_norm": 0.47995859384536743,
      "learning_rate": 1.7221257381035084e-05,
      "loss": 4.9037,
      "step": 8000
    },
    {
      "epoch": 2.9524140326502257,
      "grad_norm": 0.134929358959198,
      "learning_rate": 1.7047585967349774e-05,
      "loss": 4.7985,
      "step": 8500
    },
    {
      "epoch": 3.126085446335533,
      "grad_norm": 0.12435980141162872,
      "learning_rate": 1.6873914553664468e-05,
      "loss": 4.7138,
      "step": 9000
    },
    {
      "epoch": 3.2997568600208407,
      "grad_norm": 0.2262873351573944,
      "learning_rate": 1.670024313997916e-05,
      "loss": 4.6389,
      "step": 9500
    },
    {
      "epoch": 3.473428273706148,
      "grad_norm": 0.20184430480003357,
      "learning_rate": 1.6526571726293855e-05,
      "loss": 4.6035,
      "step": 10000
    },
    {
      "epoch": 3.6470996873914556,
      "grad_norm": 0.2584100365638733,
      "learning_rate": 1.6352900312608545e-05,
      "loss": 4.5187,
      "step": 10500
    },
    {
      "epoch": 3.8207711010767627,
      "grad_norm": 0.5082416534423828,
      "learning_rate": 1.617922889892324e-05,
      "loss": 4.4232,
      "step": 11000
    },
    {
      "epoch": 3.99444251476207,
      "grad_norm": 0.4489784836769104,
      "learning_rate": 1.6005557485237932e-05,
      "loss": 4.3887,
      "step": 11500
    },
    {
      "epoch": 4.168113928447378,
      "grad_norm": 0.12308662384748459,
      "learning_rate": 1.5831886071552625e-05,
      "loss": 4.2835,
      "step": 12000
    },
    {
      "epoch": 4.341785342132685,
      "grad_norm": 0.15385796129703522,
      "learning_rate": 1.5658214657867316e-05,
      "loss": 4.2509,
      "step": 12500
    },
    {
      "epoch": 4.5154567558179926,
      "grad_norm": 0.20671623945236206,
      "learning_rate": 1.548454324418201e-05,
      "loss": 4.169,
      "step": 13000
    },
    {
      "epoch": 4.6891281695033,
      "grad_norm": 0.44969621300697327,
      "learning_rate": 1.53108718304967e-05,
      "loss": 4.124,
      "step": 13500
    },
    {
      "epoch": 4.8627995831886075,
      "grad_norm": 0.20118555426597595,
      "learning_rate": 1.5137200416811394e-05,
      "loss": 4.1254,
      "step": 14000
    },
    {
      "epoch": 5.0364709968739145,
      "grad_norm": 0.17236927151679993,
      "learning_rate": 1.4963529003126088e-05,
      "loss": 4.0448,
      "step": 14500
    },
    {
      "epoch": 5.210142410559222,
      "grad_norm": 0.3333738148212433,
      "learning_rate": 1.478985758944078e-05,
      "loss": 3.9696,
      "step": 15000
    },
    {
      "epoch": 5.3838138242445295,
      "grad_norm": 0.21265961229801178,
      "learning_rate": 1.4616186175755472e-05,
      "loss": 3.9169,
      "step": 15500
    },
    {
      "epoch": 5.5574852379298365,
      "grad_norm": 0.12730197608470917,
      "learning_rate": 1.4442514762070163e-05,
      "loss": 3.8724,
      "step": 16000
    },
    {
      "epoch": 5.731156651615144,
      "grad_norm": 0.6566082239151001,
      "learning_rate": 1.4268843348384857e-05,
      "loss": 3.805,
      "step": 16500
    },
    {
      "epoch": 5.9048280653004515,
      "grad_norm": 0.1601380705833435,
      "learning_rate": 1.409517193469955e-05,
      "loss": 3.7527,
      "step": 17000
    },
    {
      "epoch": 6.078499478985759,
      "grad_norm": 0.2072352021932602,
      "learning_rate": 1.3921500521014242e-05,
      "loss": 3.7954,
      "step": 17500
    },
    {
      "epoch": 6.252170892671066,
      "grad_norm": 0.1589425951242447,
      "learning_rate": 1.3747829107328934e-05,
      "loss": 3.7234,
      "step": 18000
    },
    {
      "epoch": 6.425842306356373,
      "grad_norm": 0.2895115315914154,
      "learning_rate": 1.3574157693643627e-05,
      "loss": 3.6796,
      "step": 18500
    },
    {
      "epoch": 6.599513720041681,
      "grad_norm": 0.24821721017360687,
      "learning_rate": 1.3400486279958321e-05,
      "loss": 3.5624,
      "step": 19000
    },
    {
      "epoch": 6.773185133726988,
      "grad_norm": 0.17382106184959412,
      "learning_rate": 1.3226814866273013e-05,
      "loss": 3.5523,
      "step": 19500
    },
    {
      "epoch": 6.946856547412296,
      "grad_norm": 0.10956759005784988,
      "learning_rate": 1.3053143452587705e-05,
      "loss": 3.5602,
      "step": 20000
    },
    {
      "epoch": 7.120527961097603,
      "grad_norm": 0.14478224515914917,
      "learning_rate": 1.2879472038902398e-05,
      "loss": 3.4688,
      "step": 20500
    },
    {
      "epoch": 7.29419937478291,
      "grad_norm": 0.2935333549976349,
      "learning_rate": 1.270580062521709e-05,
      "loss": 3.4867,
      "step": 21000
    },
    {
      "epoch": 7.467870788468218,
      "grad_norm": 0.14898937940597534,
      "learning_rate": 1.2532129211531783e-05,
      "loss": 3.4761,
      "step": 21500
    },
    {
      "epoch": 7.641542202153525,
      "grad_norm": 0.1304609775543213,
      "learning_rate": 1.2358457797846475e-05,
      "loss": 3.3936,
      "step": 22000
    },
    {
      "epoch": 7.815213615838833,
      "grad_norm": 0.3969370126724243,
      "learning_rate": 1.2184786384161169e-05,
      "loss": 3.3954,
      "step": 22500
    },
    {
      "epoch": 7.98888502952414,
      "grad_norm": 0.46492472290992737,
      "learning_rate": 1.201111497047586e-05,
      "loss": 3.3539,
      "step": 23000
    },
    {
      "epoch": 8.162556443209448,
      "grad_norm": 0.3122325837612152,
      "learning_rate": 1.1837443556790554e-05,
      "loss": 3.3221,
      "step": 23500
    },
    {
      "epoch": 8.336227856894755,
      "grad_norm": 0.3525956869125366,
      "learning_rate": 1.1663772143105246e-05,
      "loss": 3.3148,
      "step": 24000
    },
    {
      "epoch": 8.509899270580062,
      "grad_norm": 0.45107167959213257,
      "learning_rate": 1.1490100729419938e-05,
      "loss": 3.2561,
      "step": 24500
    },
    {
      "epoch": 8.68357068426537,
      "grad_norm": 0.14188796281814575,
      "learning_rate": 1.1316429315734631e-05,
      "loss": 3.2618,
      "step": 25000
    },
    {
      "epoch": 8.857242097950678,
      "grad_norm": 0.1207670271396637,
      "learning_rate": 1.1142757902049323e-05,
      "loss": 3.2639,
      "step": 25500
    },
    {
      "epoch": 9.030913511635985,
      "grad_norm": 0.15962213277816772,
      "learning_rate": 1.0969086488364016e-05,
      "loss": 3.246,
      "step": 26000
    },
    {
      "epoch": 9.204584925321292,
      "grad_norm": 0.2535298466682434,
      "learning_rate": 1.0795415074678708e-05,
      "loss": 3.1714,
      "step": 26500
    },
    {
      "epoch": 9.3782563390066,
      "grad_norm": 0.11571846157312393,
      "learning_rate": 1.0621743660993402e-05,
      "loss": 3.1793,
      "step": 27000
    },
    {
      "epoch": 9.551927752691906,
      "grad_norm": 0.23047985136508942,
      "learning_rate": 1.0448072247308094e-05,
      "loss": 3.1749,
      "step": 27500
    },
    {
      "epoch": 9.725599166377215,
      "grad_norm": 0.15681064128875732,
      "learning_rate": 1.0274400833622787e-05,
      "loss": 3.1589,
      "step": 28000
    },
    {
      "epoch": 9.899270580062522,
      "grad_norm": 0.12754084169864655,
      "learning_rate": 1.0100729419937479e-05,
      "loss": 3.1043,
      "step": 28500
    },
    {
      "epoch": 10.072941993747829,
      "grad_norm": 0.11983465403318405,
      "learning_rate": 9.92705800625217e-06,
      "loss": 3.1284,
      "step": 29000
    },
    {
      "epoch": 10.246613407433136,
      "grad_norm": 0.18057690560817719,
      "learning_rate": 9.753386592566864e-06,
      "loss": 3.097,
      "step": 29500
    },
    {
      "epoch": 10.420284821118443,
      "grad_norm": 0.1054803654551506,
      "learning_rate": 9.579715178881556e-06,
      "loss": 3.0651,
      "step": 30000
    },
    {
      "epoch": 10.593956234803752,
      "grad_norm": 0.13559085130691528,
      "learning_rate": 9.40604376519625e-06,
      "loss": 3.0678,
      "step": 30500
    },
    {
      "epoch": 10.767627648489059,
      "grad_norm": 0.18136288225650787,
      "learning_rate": 9.232372351510941e-06,
      "loss": 3.0433,
      "step": 31000
    },
    {
      "epoch": 10.941299062174366,
      "grad_norm": 0.15580448508262634,
      "learning_rate": 9.058700937825635e-06,
      "loss": 3.0414,
      "step": 31500
    },
    {
      "epoch": 11.114970475859673,
      "grad_norm": 0.09256000071763992,
      "learning_rate": 8.885029524140327e-06,
      "loss": 3.0375,
      "step": 32000
    },
    {
      "epoch": 11.28864188954498,
      "grad_norm": 0.17331558465957642,
      "learning_rate": 8.711358110455018e-06,
      "loss": 2.9846,
      "step": 32500
    },
    {
      "epoch": 11.462313303230289,
      "grad_norm": 0.12860412895679474,
      "learning_rate": 8.537686696769712e-06,
      "loss": 3.0082,
      "step": 33000
    },
    {
      "epoch": 11.635984716915596,
      "grad_norm": 0.17822818458080292,
      "learning_rate": 8.364015283084404e-06,
      "loss": 2.961,
      "step": 33500
    },
    {
      "epoch": 11.809656130600903,
      "grad_norm": 0.21735864877700806,
      "learning_rate": 8.190343869399097e-06,
      "loss": 2.9849,
      "step": 34000
    },
    {
      "epoch": 11.98332754428621,
      "grad_norm": 0.10284194350242615,
      "learning_rate": 8.016672455713789e-06,
      "loss": 2.9781,
      "step": 34500
    },
    {
      "epoch": 12.156998957971519,
      "grad_norm": 0.09014807641506195,
      "learning_rate": 7.843001042028483e-06,
      "loss": 2.917,
      "step": 35000
    },
    {
      "epoch": 12.330670371656826,
      "grad_norm": 0.14570371806621552,
      "learning_rate": 7.669329628343174e-06,
      "loss": 2.9564,
      "step": 35500
    },
    {
      "epoch": 12.504341785342133,
      "grad_norm": 0.348067045211792,
      "learning_rate": 7.495658214657868e-06,
      "loss": 2.9252,
      "step": 36000
    },
    {
      "epoch": 12.67801319902744,
      "grad_norm": 0.13286860287189484,
      "learning_rate": 7.321986800972561e-06,
      "loss": 2.9257,
      "step": 36500
    },
    {
      "epoch": 12.851684612712747,
      "grad_norm": 0.3223777115345001,
      "learning_rate": 7.148315387287253e-06,
      "loss": 2.9538,
      "step": 37000
    },
    {
      "epoch": 13.025356026398056,
      "grad_norm": 0.15177524089813232,
      "learning_rate": 6.974643973601946e-06,
      "loss": 2.9063,
      "step": 37500
    },
    {
      "epoch": 13.199027440083363,
      "grad_norm": 0.236360564827919,
      "learning_rate": 6.800972559916639e-06,
      "loss": 2.8605,
      "step": 38000
    },
    {
      "epoch": 13.37269885376867,
      "grad_norm": 0.3663535416126251,
      "learning_rate": 6.62730114623133e-06,
      "loss": 2.8807,
      "step": 38500
    },
    {
      "epoch": 13.546370267453977,
      "grad_norm": 0.17130623757839203,
      "learning_rate": 6.453629732546024e-06,
      "loss": 2.869,
      "step": 39000
    },
    {
      "epoch": 13.720041681139284,
      "grad_norm": 0.0988507941365242,
      "learning_rate": 6.279958318860716e-06,
      "loss": 2.846,
      "step": 39500
    },
    {
      "epoch": 13.893713094824593,
      "grad_norm": 0.2081855982542038,
      "learning_rate": 6.106286905175408e-06,
      "loss": 2.9519,
      "step": 40000
    },
    {
      "epoch": 14.0673845085099,
      "grad_norm": 0.09979645907878876,
      "learning_rate": 5.932615491490101e-06,
      "loss": 2.8685,
      "step": 40500
    },
    {
      "epoch": 14.241055922195207,
      "grad_norm": 0.10744704306125641,
      "learning_rate": 5.758944077804794e-06,
      "loss": 2.8419,
      "step": 41000
    },
    {
      "epoch": 14.414727335880514,
      "grad_norm": 0.2084282636642456,
      "learning_rate": 5.585272664119486e-06,
      "loss": 2.8227,
      "step": 41500
    },
    {
      "epoch": 14.58839874956582,
      "grad_norm": 1.0816972255706787,
      "learning_rate": 5.411601250434179e-06,
      "loss": 2.8526,
      "step": 42000
    },
    {
      "epoch": 14.76207016325113,
      "grad_norm": 0.3313199281692505,
      "learning_rate": 5.237929836748872e-06,
      "loss": 2.7863,
      "step": 42500
    },
    {
      "epoch": 14.935741576936437,
      "grad_norm": 0.12272140383720398,
      "learning_rate": 5.0642584230635635e-06,
      "loss": 2.8296,
      "step": 43000
    },
    {
      "epoch": 15.109412990621744,
      "grad_norm": 0.17528317868709564,
      "learning_rate": 4.890587009378256e-06,
      "loss": 2.8959,
      "step": 43500
    },
    {
      "epoch": 15.28308440430705,
      "grad_norm": 0.13831637799739838,
      "learning_rate": 4.716915595692949e-06,
      "loss": 2.7717,
      "step": 44000
    },
    {
      "epoch": 15.456755817992358,
      "grad_norm": 0.08928411453962326,
      "learning_rate": 4.5432441820076414e-06,
      "loss": 2.8016,
      "step": 44500
    },
    {
      "epoch": 15.630427231677666,
      "grad_norm": 0.13519451022148132,
      "learning_rate": 4.369572768322334e-06,
      "loss": 2.8331,
      "step": 45000
    },
    {
      "epoch": 15.804098645362973,
      "grad_norm": 0.22545385360717773,
      "learning_rate": 4.195901354637027e-06,
      "loss": 2.7792,
      "step": 45500
    },
    {
      "epoch": 15.97777005904828,
      "grad_norm": 0.12803047895431519,
      "learning_rate": 4.022229940951719e-06,
      "loss": 2.8099,
      "step": 46000
    },
    {
      "epoch": 16.151441472733588,
      "grad_norm": 0.560874342918396,
      "learning_rate": 3.848558527266412e-06,
      "loss": 2.7971,
      "step": 46500
    },
    {
      "epoch": 16.325112886418896,
      "grad_norm": 0.1527058631181717,
      "learning_rate": 3.6748871135811047e-06,
      "loss": 2.7703,
      "step": 47000
    },
    {
      "epoch": 16.4987843001042,
      "grad_norm": 0.1304258406162262,
      "learning_rate": 3.5012156998957974e-06,
      "loss": 2.8214,
      "step": 47500
    },
    {
      "epoch": 16.67245571378951,
      "grad_norm": 0.11212485283613205,
      "learning_rate": 3.32754428621049e-06,
      "loss": 2.7426,
      "step": 48000
    },
    {
      "epoch": 16.84612712747482,
      "grad_norm": 0.18382690846920013,
      "learning_rate": 3.1538728725251827e-06,
      "loss": 2.781,
      "step": 48500
    },
    {
      "epoch": 17.019798541160124,
      "grad_norm": 0.2760486900806427,
      "learning_rate": 2.9802014588398754e-06,
      "loss": 2.7837,
      "step": 49000
    },
    {
      "epoch": 17.193469954845433,
      "grad_norm": 0.106055848300457,
      "learning_rate": 2.806530045154568e-06,
      "loss": 2.7725,
      "step": 49500
    },
    {
      "epoch": 17.36714136853074,
      "grad_norm": 0.27102839946746826,
      "learning_rate": 2.6328586314692607e-06,
      "loss": 2.7686,
      "step": 50000
    },
    {
      "epoch": 17.540812782216047,
      "grad_norm": 0.1439114511013031,
      "learning_rate": 2.459187217783953e-06,
      "loss": 2.7526,
      "step": 50500
    },
    {
      "epoch": 17.714484195901356,
      "grad_norm": 0.16471731662750244,
      "learning_rate": 2.2855158040986456e-06,
      "loss": 2.7616,
      "step": 51000
    },
    {
      "epoch": 17.88815560958666,
      "grad_norm": 0.31201353669166565,
      "learning_rate": 2.1118443904133382e-06,
      "loss": 2.7539,
      "step": 51500
    },
    {
      "epoch": 18.06182702327197,
      "grad_norm": 0.2825196385383606,
      "learning_rate": 1.938172976728031e-06,
      "loss": 2.8064,
      "step": 52000
    },
    {
      "epoch": 18.235498436957275,
      "grad_norm": 0.49336957931518555,
      "learning_rate": 1.7645015630427233e-06,
      "loss": 2.7342,
      "step": 52500
    },
    {
      "epoch": 18.409169850642584,
      "grad_norm": 0.16204868257045746,
      "learning_rate": 1.590830149357416e-06,
      "loss": 2.7718,
      "step": 53000
    },
    {
      "epoch": 18.582841264327893,
      "grad_norm": 0.1484365165233612,
      "learning_rate": 1.4171587356721085e-06,
      "loss": 2.7707,
      "step": 53500
    },
    {
      "epoch": 18.7565126780132,
      "grad_norm": 0.11038882285356522,
      "learning_rate": 1.2434873219868011e-06,
      "loss": 2.7536,
      "step": 54000
    },
    {
      "epoch": 18.930184091698507,
      "grad_norm": 0.11377234011888504,
      "learning_rate": 1.0698159083014936e-06,
      "loss": 2.7756,
      "step": 54500
    },
    {
      "epoch": 19.103855505383812,
      "grad_norm": 0.07613416016101837,
      "learning_rate": 8.961444946161862e-07,
      "loss": 2.6778,
      "step": 55000
    },
    {
      "epoch": 19.27752691906912,
      "grad_norm": 0.15839649736881256,
      "learning_rate": 7.224730809308789e-07,
      "loss": 2.7555,
      "step": 55500
    },
    {
      "epoch": 19.45119833275443,
      "grad_norm": 0.11732719838619232,
      "learning_rate": 5.488016672455714e-07,
      "loss": 2.748,
      "step": 56000
    },
    {
      "epoch": 19.624869746439735,
      "grad_norm": 0.1343051940202713,
      "learning_rate": 3.7513025356026403e-07,
      "loss": 2.7834,
      "step": 56500
    },
    {
      "epoch": 19.798541160125044,
      "grad_norm": 0.19677628576755524,
      "learning_rate": 2.014588398749566e-07,
      "loss": 2.7476,
      "step": 57000
    },
    {
      "epoch": 19.97221257381035,
      "grad_norm": 0.10887011885643005,
      "learning_rate": 2.7787426189649184e-08,
      "loss": 2.7484,
      "step": 57500
    }
  ],
  "logging_steps": 500,
  "max_steps": 57580,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2167453937503954e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
