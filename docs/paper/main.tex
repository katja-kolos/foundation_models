%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

%6 pages + references

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2021}
\begin{document}

\twocolumn[
\icmltitle{Scientific Reasoning: \\
           Assessment of Multimodal Generative LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Florian Dreyer (3667877)}{equal,us}
\icmlauthor{Ekaterina Kolos (3688474)}{equal,us}
\icmlauthor{Daria Matiash (3668740)}{equal,us}
\end{icmlauthorlist}

\icmlaffiliation{us}{University of Stuttgart}
%\icmlaffiliation{ims}{Institut fuer Maschinelle Sprachverarbeitung}

\icmlcorrespondingauthor{Florian Dreyer}{st182762@stud.uni-stuttgart.de}
\icmlcorrespondingauthor{Ekaterina Kolos}{st186032@stud.uni-stuttgart.de}
\icmlcorrespondingauthor{Daria Matiash}{st185745@stud.uni-stuttgart.de}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Scientific Reasoning, Prefix Tuning, Knowledge Distillation}

\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%Note: we are doing scientific QA and not really scientific reasoning in the end. How do we frame it? -> Added QA to multimodal data, what do you think? -- Good, thanks!
This project assesses the capabilities of pre-trained multimodal LLMs to perform scientific reasoning tasks on multimodal Question Answering (QA) data. We further explore how Prefix Tuning and Low-Rank Adaptation (LoRA) can improve performance of smaller LLMs. We then attempt to distill knowledge to a smaller LLM using Prefix Tuning and LoRA.
\end{abstract}
           


\section{Introduction}
\label{introduction}

Scientific machine reasoning is the application of AI techniques to simulate scientific reasoning processes, such as data interpretation, hypothesis generation, and causal inference. 
%It facilitates scientific inquiry and discovery by enabling machines to independently analyze scientific data, generate hypotheses, and make predictions.

Recent developments in foundation models, such as o1/o3 from OpenAI \cite{OpenAI2024o1} or R1 from DeepSeek \cite{deepseek-r1}, have shown remarkable performance in challenges that require contextual awareness and reasoning.
However, these models are often resource consuming, which limits their scalability and accessibility for broader applications.

This leads us to our research questions:

\paragraph{RQ1}
\label{def:rq1}
How can large multimodal LLMs deal with multimodal scientific reasoning?
%We in fact answer three questions: 
% 1) how well they perform in scientific question answering (QTCH), 
% 2) whether they are capable of reasoning (QTCHL) 
% 3) how well they use in-context information (=solution) to produce expected useful generation

\paragraph{RQ2}
\label{def:rq2}
How do Prefix Tuning and LoRA affect the reasoning capabilities of smaller pre-trained models?
% This is only scientific QA, without reasoning!

\paragraph{RQ3}
\label{def:rq3}
How good is knowledge distillation with adapter methods compared to training on manually annotated data?

We first want to evaluate how six large front-tier LLMs perform on the \textsc{ScienceQA} dataset \cite{lu2022learn} using several metrics. Following that we evaluate how well Prefix Tuning \cite{li2021prefix} and LoRA \cite{hu2021lora} perform for fine-tuning two smaller LLMs on this dataset. Last, we perform knowledge distillation using Prefix Tuning and LoRA and compare how the distilled models compare to the previously fine-tuned models.

\section{Background and Related Work}
\label{related-work}

\paragraph{Prefix Tuning}
Prefix Tuning, as introduced in \cite{li2021prefix}, is a parameter-efficient fine-tuning method that freezes the pre-trained model's parameters and trains a small, learnable "prefix" as illustrated in figure \ref{fig:prefix_tuning} that guides the model during task-specific inference. This allows the model to adapt to new tasks while preserving its general pre-trained knowledge.
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{image.png}
    \caption{Intuition behind Prefix Tuning. Source: \cite{li2021prefix}}
    \label{fig:prefix_tuning}
\end{figure}
By significantly reducing the number of trainable parameters, Prefix Tuning is especially advantageous in resource-constrained scenarios or when deploying models for multiple tasks. Studies have shown that it achieves performance comparable to full fine-tuning in many applications, such as natural language generation and classification.
%TODO: references

\paragraph{LoRA}
Low-Rank Adaptation (LoRA) \cite{hu2021lora} is a parameter-efficient fine-tuning technique designed to adapt pre-trained models to downstream tasks with minimal computational overhead. LoRA introduces trainable low-rank matrices into the model's attention layers while keeping the original weights frozen. These matrices encode task-specific knowledge, allowing the model to be fine-tuned with a significantly smaller number of parameters compared to traditional fine-tuning.

LoRA is particularly effective in scenarios where memory efficiency is critical, as it avoids modifying or storing the full set of model weights. Studies have shown that LoRA achieves competitive performance on various NLP tasks, such as machine translation and text classification \cite{mao2025survey}.
%TODO: references

\paragraph{Knowledge distillation (KD)} allows to obtain smaller models capable of successfully following the behavior of larger teacher models. This is particularly useful for privacy reasons (mobile AI apps) and when access to large models in the cloud is not guaranteed (e.g. in cars). The teacher model can be used to intelligently select examples on which the student model is trained (dataset distillation) \cite{yu2023dataset}, or provide negative samples to show the student what incorrect answers or reasoning paths it should avoid to improve task accuracy \cite{li2024turning}.
%(c.f. contrastive CoT). 
Training small models on a chain-of-thought reasoning path of a larger model was also shown to be a way to obtain a small student model replicating reasoning capabilities of teacher on downstream tasks \cite{magister2022teaching}. 
% which is close to \textit{response-based KD} where the student model mimics the output of the teacher. 
In our project, we follow a similar approach and perform \textit{response-based knowledge distillation} \cite{gou2021knowledge}, learning to mimic the output of the teacher.
%Alternatively, with \textit{feature-based KD} the knowledge from specific layers of the teacher model is distilled into the student model \cite{sepahvand2022teacher}, while the student model's structure may be a quantized, simplified, or condensed version of the teacher's architecture \cite{gou2021knowledge}. 

\paragraph{Models} We experiment with the following foundation models: 
\texttt{Pixtral-12b-2409}, \texttt{LLaVA-1.5-7b-hf}, \texttt{Gemini 1.5 Flash}, \texttt{Gemini 1.5 Flash 8B}, \texttt{GPT 4}, \texttt{GPT 4o mini}, %and two small MLLMs: 
\texttt{Qwen2-VL-2B-Instruct}, \texttt{paligemma-3b-pt-224}.

\section{Dataset}
\label{dataset}
This study is based on the \textsc{ScienceQA} dataset \cite{lu2022learn}. The \textsc{ScienceQA} dataset is a benchmark for multimodal reasoning in science, consisting of more than 21,000 questions across topics like Natural science or Social science. 
Each question includes textual prompts, optionally visual aids (e.g., diagrams, charts), answer options, detailed explanations, and lecture context. The dataset spans various difficulty levels, enabling evaluation of models on both basic and advanced scientific reasoning, reaching from elementary to high school level questions.
%%% This line is not really relevant to our purposes. Let us omit it because we are running out of space
%\textsc{ScienceQA} supports tasks such as multimodal comprehension, answer explanation generation, and educational AI development. 
%%% This line is subjective assessment of usefullness of the dataset 
%Its high-quality annotations and diverse content make it an ideal benchmark for assessing models in science education and reasoning. 
%%% This line goes into content of other sections and feels a bit out of place
%In our experiments, we leveraged the dataset to assess knowledge distillation under diverse scientific scenarios, focusing on the ability to distill the reasoning capabilities.
The dataset contains some notable irregularities. For some datapoints, the image data is missing (sometimes when it is required to solve the task, sometimes when the answer can be deduced from text alone).
Apart from the image field, missing values can occur in lecture and solution fields (about 9\% and 15\% respectively). As about 50\% of the datapoints are missing an image, we decide to process both text-only and text+image datapoints similarly with multimodal models, attaching the image to the prompt if it was present in the data. When the image was not available, we generated a blank empty image as a placeholder. 

\section{Methodology}
\label{section:methodology}
% TODO: add references to models
In the following we present the methodology we used to examine the three research questions (RQ).
%(\ref{def:rq1}-\ref{def:rq3})).
\paragraph{RQ1} We benchmark six front-tier LLMs using accuracy and the average of five text similarity metrics introduced in the metrics section \ref{section:metrics}. 
%The six LLMs we use are Google Gemini 1.5 Flash and Flash 8B, OpenAI GPT 4 and 4o mini, LLaVA 1.5 7B and MistralAI Pixtral 12B 2409. 
To investigate how additional information and the correct solution in the prompt influence the performance, we use four different prompt settings: 
\begin{enumerate}
	\item \textbf{question - choices - hint - image - task}
	\item \textbf{question - choices - hint - image - task + lecture}
	\item \textbf{question - choices - hint - image - task + lecture + solution}
	\item \textbf{question - choices - hint - image - task + solution}
\end{enumerate}
Each model is benchmarked on all four settings using the \textsc{ScienceQA} validation data. We use the validation split instead of the test split since we later use the results to choose a champion teacher model. This decision can't be made using the test data since it's utilized to evaluate knowledge distillation performance.

\paragraph{RQ2} For the comparison of Prefix Tuning and LoRA we fine-tune two smaller multimodal LLMs
%both Qwen2 VL 2B Instruct and PaliGemma2 3B 
using the two techniques separately. As the label to train on we choose the solution to enable the adapters to learn more of the reasoning. We then compare the performance of the four fine-tuned models to the zero-shot performance of the two base models on \textsc{ScienceQA} test data.

\paragraph{RQ3} 
%To investigate how Knowledge Distillation performs using these two adapter methods compared to fine-tuning the models using the adapters and the original dataset we use the fine-tuned models from RQ2 and further fine-tune the two models from scratch using the adapter methods and the output from our teacher model. 

We further investigate the impact of knowledge distillation on the performance of adapter tuning. Would learning from the outputs of a champion teacher LLM drop the performance significantly and consistently, compared to learning from the original data? For this, we obtain outputs of the champion teacher model on the train partition of \textsc{ScienceQA}. We train 2 different adapters on 2 different students on this data and on the original train partition of \textsc{ScienceQA}, and compare the text similarity results in section \ref{section:experiments:KD}. 

%As teacher model, we chose Google Gemini 1.5 Flash since together with Gemini 1.5 Flash 8B it had the best performance but has more parameters which can be beneficial for Knowledge Distillation as shown in [reference]. % @Dasha
We compare the performance of the eight resulting models on the \textsc{ScienceQA} test data. 

\section{Metrics}
\label{section:metrics}

We evaluate model's performance with QA accuracy. % domain-wise in order to have a fair comparison with leaderboards. 
The reasoning steps were evaluated with semantic similarity metrics, adopted from machine translation, such as BLEU, METEOR, ROUGE, and cosine similarity.
\subsection{Multiple choice Evaluation}
Owing to the simplicity of the test format, only accuracy score is computed, following original evaluation strategy by \cite{lu2022learn}. To extract the answers from the output we prompt the models to output in JSON format with "answer" and "solution" as keys.
\subsection{Answer Reasoning Evaluation}
Due to the peculiarity of scientific texts and approaches to the evaluation of automatically generated texts, the following evaluation approaches were chosen.

\paragraph{BLEU}
BLEU-score \cite{papineni2002bleu} can measure how closely the model's generated solutions aligns with the human-authored explanation, calculating modified \textit{n}-gram precisions adjusted by brevity penalty. BLEU-1 measures if the model uses the right scientific terms or key words (e.g., \textit{"photosynthesis"},  \textit{"temperature"}). However, it ignores word order, 
%and logical progression, 
so it can’t evaluate reasoning or explanation quality. That is why BLEU-4 score is also computed to capture both vocabulary usage and phrase structures.
By computing both BLEU-1 and BLEU-4, we balance term accuracy with linguistic structure, ensuring a more reliable evaluation of the model’s explanatory capabilities in scientific reasoning.
%Why not BLEU-4 alone?

\paragraph{ROUGE}
ROUGE is a set of metrics introduced in \cite{lin2004rouge}, again, to estimate the quality of a generated hypothesis compared to one or more golden references. 
ROUGE-1 and ROUGE-2, like BLEU-\textit{n}, count overlaps of \textit{n}-grams between the hypothesis and the reference, calculating not only the precision, but also the recall and F1-score.
Following evaluation strategy in \cite{lu2022learn}, we adopt ROUGE-L score \cite{lin2004rouge}, that calculates the longest common subsequence between the hypothesis and the reference. 
As opposed to ROUGE-1 and ROUGE-2, this score would assign a higher value to e.g. \textit{"the theory which Einstein proposed"} than to \textit{"Einstein proposed the theory" } for a reference like \textit{"the theory that Einstein proposed"}.
This score, however, would not capture synonyms by default, which is why we do not rely on this score alone. 
\footnote{We performed our evaluation following the scripts proposed by the authors of \textsc{ScienceQA} for consistent comparison. Upon more careful investigation done at the stage of writing this report, we found out that the package used for ROUGE was https://pypi.org/project/rouge/, which does not calculate the score consistently to the algorithm proposed by \cite{lin2004rouge}. A better implementation, that is more accurate to it, would be this one: https://pypi.org/project/rouge-score/.}
%ROUGE-L score, following evaluation strategy in \cite{lu2022learn}, helps analyze the content recall and sequence preservation of model-generated explanations. They highlight whether a model captures key scientific concepts and produces explanations that align with reference reasoning. However, ROUGE should be used in conjunction with other metrics to fully evaluate the logical and factual quality of generated reasoning. ROUGE-L will highlight the mismatch in logical flow, even though some content overlaps in reference and candidate sentences (\ref{example:1}).

% \label{example:1}
% \begin{itemize}
%     \item \textbf{Reference:} Plants absorb sunlight to produce energy.
%     \item \textbf{Candidate:} Sunlight is absorbed by plants to create energy.
% \end{itemize}

\paragraph{METEOR}
METEOR \cite{banerjee2005meteor} provides a more fine-grained and linguistically informed approach to evaluating text similarity, balancing precision and recall, and encouraging correct alignment of semantically meaningful components through fragmentation penalty.  
Later versions of METEOR introduce stemming, synonym matching, as well as fine-grained weights for individual languages for an even better agreement with human judgments.

\paragraph{Cosine similarity with Sentence Transformers}
% Sentence Transformer models are optimized for sentence-level embeddings, which capture the overall contextual similarity of answers more effectively, making them suitable for general-purpose evaluation tasks like scientific reasoning. 
\texttt{all-MiniLM-L6-v2} was utilized in Sentence Transformers library in our evaluation strategy.
Unlike \texttt{SciBERT}, which is specialized for scientific text and could serve as a sentence embedder, \texttt{all-MiniLM-L6-v2} was pre-trained on diverse datasets, allowing it to better handle variations in phrasing and style found in the Science QA Dataset.
Its broader linguistic adaptability ensures that evaluation considers both semantic accuracy and logical flow, which is crucial for assessing reasoning across different scientific domains.

\paragraph{Overall score}
To make the performance of the models to be easier for comparison, we computed an overall score of the textual similarity metrics. In order to obtain a numeric description of the model's performance for these text similarity metrics, we computed the average of them (having normalized cosine similarity to the range [0;1]). 

\section{Experiments}
\label{experiments}
In the following we present the experiments to answer the three RQs.

\subsection{Benchmarking large LLMs}
As described in \ref{section:methodology} we benchmarked six large LLMs on the validation part of the dataset. Our experiments resulted in the following results for accuracy:
\begin{figure} [h]
    \centering
    \includegraphics[width=1\linewidth]{acc_average.pdf}
    \label{fig:benchmark_accuracy}
    \caption{Accuracy scores in answer generation by LLMs. Benchmarking.}
\end{figure}
% Gemini family of models showed best robustness across 4 experimental settings, while GPT models were best in extracting relevant information for answer generation. 

For the overall score as introduced in the \ref{section:metrics} the experiments resulted in the following scores:
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{overall_metrics.pdf}
    \label{fig:benchmark_overall}
    \caption{Overall scores in reasoning by LLMs. Benchmarking.}
\end{figure}
For the individual textual similarity metrics we got the following results:
\begin{table} [H]
\vskip 0.15in
\begin{center}
\begin{small}
%model & setting & BLEU-1 & BLEU-4 & ROUGE & similarity & overall_metric \\

\begin{tabular}{llcccccc}
\toprule
model & s. & bl-1 & bl-4 & r. & m. & cos. & overall \\
\midrule
\textsc{Gemini} & 1 & 0.04 & 0.01 & 0.28 & 0.14 & 0.80 & 21.59 \\
 & 2 & 0.04 & 0.01 & 0.28 & 0.14 & 0.80 & 21.58 \\
 & 3 & 0.12 & \textbf{0.12} & \textbf{0.52} & \textbf{0.30} & \textbf{0.84} & \textbf{35.03} \\
 & 4 & 0.14 & \textbf{0.14} & \textbf{0.53} & \textbf{0.32} & \textbf{0.85} & \textbf{36.55} \\
\midrule
\textsc{LLaVA} & 1 & 0.03 & 0.01 & 0.24 & 0.09 & 0.74 & 17.16 \\
 & 2 & 0.08 & \textbf{0.05} & \textbf{0.33} & 0.06 & 0.80 & 22.43 \\
 & 3 & 0.06 & 0.06 & 0.41 & 0.20 & 0.82 & 27.43 \\
 & 4 & 0.05 & 0.05 & 0.41 & 0.13 & 0.81 & 25.32 \\
\midrule
\textsc{Pixtral} & 1 & 0.05 & 0.01 & 0.23 & 0.11 & 0.78 & 19.11 \\
 & 2 & 0.06 & 0.02 & 0.27 & 0.14 & 0.79 & 21.64 \\
 & 3 & 0.12 & 0.11 & 0.46 & 0.27 & 0.83 & 32.17 \\
 & 4 & 0.11 & 0.11 & 0.47 & 0.27 & 0.83 & 32.29 \\
\midrule
\textsc{Gemini}  & 1 & 0.05 & 0.01 & 0.28 & 0.14 & \textbf{0.81} & 21.68 \\
\textsc{8B} & 2 & 0.05 & 0.01 & 0.27 & 0.14 & 0.81 & 21.59 \\
 & 3 & 0.06 & 0.04 & 0.37 & 0.19 & 0.82 & 26.26 \\
 & 4 & 0.08 & 0.08 & 0.44 & 0.24 & 0.84 & 30.37 \\
\midrule
\textsc{GPT-4o} & 1 & \textbf{0.14} & \textbf{0.03} & 0.28 & \textbf{0.17} & 0.78 & \textbf{23.51} \\
\textsc{-mini} & 2 & \textbf{0.17} & 0.04 & 0.31 & \textbf{0.21} & \textbf{0.82} & \textbf{27.43} \\
 & 3 & \textbf{0.18} & 0.05 & 0.33 & 0.22 & 0.82 & 28.61 \\
 & 4 & \textbf{0.17} & 0.04 & 0.31 & 0.21 & 0.82 & 27.24 \\
\midrule
\textsc{GPT-4} & 1 & 0.11 & 0.02 & \textbf{0.30} & 0.16 & 0.76 & 22.09 \\
 & 2 & 0.13 & 0.04 & 0.32 & 0.18 & 0.78 & 24.41 \\
 & 3 & 0.16 & 0.06 & 0.35 & 0.23 & 0.82 & 28.88 \\
 & 4 & 0.14 & 0.04 & 0.33 & 0.21 & 0.82 & 27.29 \\
\bottomrule
\end{tabular}


% \begin{tabular}{llccccc}
% \toprule
% model & s. & bl-1 & bl-4 & rouge & cos. & overall \\
% \midrule
% \textsc{Gemini} & 1 & 0.04 & 0.01 & 0.28 & 0.80 & 21.59 \\
%  & 2 & 0.04 & 0.01 & 0.28 & 0.80 & 21.58 \\
%  & 3 & 0.12 & 0.12 & 0.52 & 0.84 & 35.03 \\
%  & 4 & 0.14 & \textbf{0.14} & 0.53 & 0.85 & 36.55 \\
% \midrule
% \textsc{LLaVA} & 1 & 0.03 & 0.01 & 0.24 & 0.74 & 17.16 \\
%  & 2 & 0.08 & \textbf{0.05} & 0.33 & 0.80 & 22.43 \\
%  & 3 & 0.06 & 0.06 & 0.41 & 0.82 & 27.43 \\
%  & 4 & 0.05 & 0.05 & 0.41 & 0.81 & 25.32 \\
% \midrule
% \textsc{Pixtral} & 1 & 0.05 & 0.01 & 0.23 & 0.78 & 19.11 \\
%  & 2 & 0.06 & 0.02 & 0.27 & 0.79 & 21.64 \\
%  & 3 & 0.12 & 0.11 & 0.46 & 0.83 & 32.17 \\
%  & 4 & 0.11 & 0.11& 0.47 & 0.83 & 32.29 \\
% \midrule
% \textsc{Gemini\_8B} & 1 & 0.05 & 0.01 & 0.81 & 0.61 & 21.68 \\
%  & 2 & 0.05 & 0.01 & 0.27 & 0.81 & 21.59 \\
%  & 3 & 0.06 & 0.04 & 0.37 & 0.82 & 26.26 \\
%  & 4 & 0.08 & 0.08 & 0.44 & 0.84 & 30.37 \\
% \midrule
% \textsc{GPT-4o-mini} & 1 & \textbf{0.14} & \textbf{0.03} & 0.28 & 0.78 & 23.51 \\
%  & 2 & \textbf{0.17} & 0.04 & 0.31 & 0.82 & 27.43 \\
%  & 3 & \textbf{0.18} & 0.05 & 0.33& 0.82 & 28.61 \\
%  & 4 & \textbf{0.17} & 0.04 & 0.31 & 0.82 & 27.24 \\
% \midrule
% \textsc{GPT-4} & 1 & 0.11 & 0.02 & 0.3 & 0.76 & 22.09 \\
%  & 2 & 0.13 & 0.04 & 0.32 & 0.78 & 24.41 \\
%  & 3 & 0.16 & 0.06 & 0.35 & 0.82 & 28.88 \\
%  & 4 & 0.14 & 0.04 & 0.33 & 0.82 & 27.29 \\
% \bottomrule
% \end{tabular}

\end{small}
\end{center}
\caption{Metrics on scientific reasoning by LLMs. \textit{s.} - setting, \textit{bl-1} - BLEU-1, \textit{bl-4} - BLEU-4, \textit{r.} - ROUGE-L, \textit{m.} - METEOR, \textit{cos.} - cosine similarity, \textit{overall} - overall metric in \%.}
\end{table}

\paragraph{Accuracy}

%1 QTCH: (81.4+50.6+66.3+81.6+73.4+73.7)/6 = 71.167
%2 QTCHL: (81.7+50.7+70.3+81.5+80.5+76)/6 = 73.45
%3 QTCHLS: (88.8+72.8+79.9+96.4+95.7+98.3)/6 = 88.65
%4 QTCHS: (89.7+75.2+81.3+95.8+93.4+97.3)/6 = 88.783

The highest performance was observed in settings that included solutions, indicating that the models were generally capable of extracting relevant information effectively. %Among these, settings combining both lecture and solution information typically yielded slightly better results than those with solutions alone.
We noticed that setting 3 including the lecture did yield worse results for most models compared to the lecture-free setting 4.
\texttt{GPT-4} and \texttt{Gemini-1.5-flash-8B} consistently received highest ratings in scenarios in which lecture and/or solution information was included. The \texttt{Gemini} family of models and  \texttt{Pixtral-12b-2409} model showed better robustness in many scenarios and came in second overall. Notably, \texttt{Gemini} models achieved an average accuracy advantage of 8\% over \texttt{GPT-family} models in the "pure" task context (without lecture or solution information). The \texttt{LLaVA-1.5-7b-hf}, on the other hand, demonstrated the lowest average accuracy scores, particularly in environments without lecture and solution data. For setting 1, which was considered as the most important for further experiments, \texttt{Gemini} models performed with highest accuracy scores.

\paragraph{BLEU-1}
Across settings, the highest BLEU-1 scores were mostly observed in settings 3 and 4 (those incorporating solution or lecture+solution information), indicating that models performed better with richer contextual data. The \texttt{GPT-4o-mini} model demonstrated superior performance, surpassing \texttt{GPT-4}, in achieving the highest results. Lower scores in settings 1 and 2 suggest that models, particularly \texttt{LLaVA-1.5-7b-hf}, may have had trouble in correctly aligning their outputs with reference words when given less auxiliary information. 

\paragraph{BLEU-4}
\texttt{GPT-4} models had higher scores in comparison to other models, which represents a better capability of information extraction. \texttt{Gemini}, at the same time, gained the highest score in the setting without solutions, which indicates the ability of extracting knowledge from massive textual data.

\paragraph{METEOR}
METEOR scores reveal how well the models capture fluency, grammar, and word-level alignment with the references. Similar to cosine similarity, \texttt{GPT-4o-mini} leads with the highest METEOR scores. Notably, \texttt{Pixtral-12b-2409} achieves relatively high METEOR scores in settings 3 and 4, highlighting its ability to produce fluent outputs. When concrete tasks without any additional helpful material are given to the models as an input, \texttt{GPT-4o-mini} model shows a capability of generating concise, fluent answers with correct relevant terminology and explanations.

\paragraph{ROUGE}
The highest ROUGE scores were observed in setting 3, where both lectures and solutions were available, highlighting the importance of comprehensive input for producing informative responses. Setting 1 showed lower ROUGE scores across almost all models, reflecting limited informativeness when models were provided with minimal context.  \texttt{GPT} family of models had highest ROUGE-scores on settings without solutions. Outputs of \texttt{GPT-4} without knowledge of the correct answer had higher ROUGE score than \texttt{GPT-4o-mini} given solutions in the input.


\paragraph{Cosine similarity}
Cosine similarity highlights the semantic alignment between the model outputs and the correct answers. {Gemini-1.5-flash} consistently achieved the highest similarity scores, particularly in settings 3 and 4, indicating strong alignment with the reference answers. \texttt{GPT-4} variants maintain steady scores across all settings, demonstrating robustness, while \texttt{Pixtral-12b-2409} and \texttt{LLaVA-1.5-7b-hf} show more variability, suggesting sensitivity to specific settings. Overall, cosine similarity shows that {Gemini-1.5-flash} excels in semantic understanding across the dataset.

\paragraph{Accuracy and Overall score}
The overall results indicate that the \texttt{GPT} and \texttt{Gemini} model families demonstrate exceptional ability in extracting accurate answers. Notably, the \texttt{Gemini} models exhibited superior performance on datasets lacking relevant lecture information in most of the metrics. The second-best performance on "pure" (without solutions and/or lectures) datasets was achieved by \texttt{GPT-4o-mini}, with only a slight difference in overall score.

\paragraph{Summary}
Overall, the findings demonstrate how important enriched contextual data, such as lectures and solutions, in enhancing model performance across all metrics. The \texttt{GPT-4o-mini} model outperformed \texttt{GPT-4} in multiple instances, indicating the efficacy of smaller, more targeted designs, even if \texttt{GPT-4} continuously received excellent accuracy and scores for reasoning. While \texttt{LLaVA-1.5-7b-hf} model failed without extra input, highlighting its dependence on extensive contextual knowledge to function well, the \texttt{Gemini} family of models demonstrated remarkable robustness and semantic alignment, particularly in enriched situations.
Lecture information doesn’t really affect models’ performance, it can even decrease the ratio of correct answers in some cases.

\subsection{Prefix Tuning and LoRA}
For training the Prefix Tuning adapters we used 20 epochs for Prefix Tuning as derived from \cite{li2021prefix} and limited by computing resources. 
For training the LoRA adapter we used 10 epochs as the loss flattened after more epochs in first experiments.
After training \texttt{Qwen} and texttt{PaliGemma} using both Prefix Tuning and LoRA we benchmarked the resulting models on the \textsc{ScienceQA} test data. We were not able to measure the accuracy because all four models were not able to output valid answers. For the overall score of the text similarity the experiments resulted in the following scores:
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{prefix_lora_test.png}
    \label{fig:prefix_lora_test}
    \caption{Overall score in reasoning by LLMs in base, after Prefix-tuning, and LoRA Adapter-tuning.}
\end{figure}
% \paragraph{Summary} 
Overall we cannot see a consistent trend, for \texttt{Qwen}, the base model outperformed both adapters while for \texttt{PaliGemma} Prefix Tuning resulted in the best overall score, while the base model performed the worst. It was noticed that the more epochs of efficient parameter tuning was done, the shorter the text became that was generated by both of the models. 

\subsection{Knowledge Distillation}
\label{section:experiments:KD}
For training we use the same number of epochs as for the previous experiments.
For evaluating the perfomance of Knowledge Distillation compared to normal fine-tuning using the adapter methods we trained both \texttt{Qwen} and \texttt{Paligemma} using Prefix Tuning and LoRA separately. All eight trained models were not able to output valid answers so we will not compare the accuracy. For the overall score of the text similarity the experiments resulted in the following scores:
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{knowledge_distillation_test.png}
    \label{fig:KD_overall}
    \caption{Overall score in reasoning by LLMs.}
\end{figure}
%\paragraph{Summary}
Overall we can see a clear outperformance of Knowledge Distillation by the fine-tuned models. We can also notice that Knowledge Distillation performs quite similar for all four models while the fine-tuning approach has higher variance in performance. We noticed that the LoRA approach outperforms Prefix Tuning for Knowledge Distillation for both models. This is not the case for the fine-tuned models.

\section{Error Analysis}
\label{error-analysis}
\paragraph{Output format}
We noticed that even front-tier SOTA models could sometimes deviate from the expected output format; this error type was much more common in smaller models, despite attempts of controlled decoding. Furthermore, a fine-tuned version of \texttt{Paligemma} would consistently produce outputs with the \textit{answer} only, not giving any \textit{solution}.
% copy paligemma FT output table
%	input 
% Question: Which of the following organisms is the primary consumer in this food web? Task: closed choice Choices: ['copepod', 'black crappie', 'bacteria'] Hint: Below is a food web from Little Rock Lake, a freshwater lake ecosystem in Wisconsin. A food web models how the matter eaten by organisms moves through an ecosystem. The arrows in a food web represent how matter moves between organisms in an ecosystem. Instruction:  Please output the answer in JSON style with an answer and a solution field
% output 
% The answer is A.
For example, for an input asking to output JSON with solution: "\textit{Question: Which of the following organisms is the primary consumer in this food web? $\langle...\rangle$ Choices: ['copepod', 'black crappie', 'bacteria'] $\langle...\rangle$  Instruction:  Please output the answer in JSON style with an answer and a solution field}" the finetuned \texttt{Paligemma} would only output "\textit{The answer is A.}".
Qwen models would sometimes produce Chinese utterances instead of expected English generations. 
%化学变化 - 经过发酵或分解，可以将有机物（如水果）转化为二氧化碳、水和无机盐。
\newline
\newline 
Models would sometimes generate long sequences going beyond a stop token, degenerating into completely irrelevant text or even code. The \texttt{Pixtral} model could finish answering the given task, and then generate a new $\langle s\rangle[INST]$ token, after which a new hallucinated task would follow, which the model would later answer.
Finally, standardizing the answer, which, given 2-5 choices could be a number, a letter, or a string, proved rather difficult, with some 500 answers left unmapped in each experiment. 
\paragraph{Sparse outputs}
Adapter training surprisingly made the outputs completely degenerate into quasi-empty strings, containing only spaces, functional words, numbers, or rare irrelevant or foreign tokens.
We speculate that this results from an overly strict feedback that the models got every time they generated plausible, yet not exactly the same correct explanations. 
%Here's an example \texttt{Qwen} before prefix tuning:

%'\n)of11111)aofofof)11112222aaaa)::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n\n\n\n  of:  plates\n\n   \n\n \n\n\n蓝\n \n\n两 columns\n\n图\n\n\n\n1)\n)\n\n\n\n\n S S\n pairs force\n\n不用 squares215² ) images magnets)00²)) The- 5 cm\n\n\n experiments) 5 cm\n\n of3: Which about the difference field between the two in the pair. pair the magnets statements is true aboutA  The\n:A: AA) The magnetic of the magnetic force between the in Pair 1 than (1) The magnitude of the magnetic force is larger same in Pair pairs. (2) The magnitude of the magnetic force is larger in Pair 1.Answer: The force show show the magnets of magnets with The magnets in Pair pairs are not have each other. the magnets are in the of the same material.\n', ')))111111::1))1111)))::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n                         file file file file file file    file file   file file  file     file file file file file file file file file file   file file file  file  (: What of is speech is used in the sentence toA the is\'t been for the, he name still to call that he is " the.\n\n now.\n\nAnswer: Identify,\nAnswer: AA) Anymoron (1) hyperhemism ( (: Aouns\n', ')))aa111:::111111111)))::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n                         file file file file file file  file  file    file file file file      file file file file file file file  file   file file file file file  (: What of is you use in a map that that the word clues words:Aanging\n to\nA: Find\n ofA: \nA) harness,1) harness ( (: Theouns\n', "ichernwicklung  '  ai�eedikie ofus,afeced - the that that serves before in afius is ofativeec1.; a that has is before someone something elseelligifer::i or argument argumentative;나ep; place, in is be entered into and anyoneupalf a andechcedstay small room that can be room from. qua mainus bell. cause of war,)i: to look or the in to it:: to who who refuses or or�;;reb expect or predict somethingce before.. government,.7 who fights a government someone1:  the a with rois antecEus aA A: the摄:l AND a a ofon) a the a time of) ( the ( (3) after th a)), at): in the States,. ))731 the1971 after  the the G andell a. bell the the bellellum period, the the mid American in than lo overre  states, the of more about the world in happened in this theellum Per..e sid the words that of the same language of frome onum thee a see word a  the s ea  use"


%and after 19 epochs of prefix tuning lol it's still very bad:
%[' import.,', '::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n                         file file file file file file  file  file file    file file file     file file file file file file file file  file file file file file file file file  (: What is the difference of a cube machine that: Calculate\n questionA: AA) 100,US5 550 ounces (2) 40 liters (Answer: The the correct answer for\n', '::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n                       file  file file file file file file   file     file file file file file    file file file file file file file file file file   file file file    (: What is the difference of a cube tube withAnswer: What-ended questionA: AA) 100,15 200iliters ((: The the correct option.\n\n', '::::::::::::::::::::::::::::::new\n1 are a bot assistant, Thank\n\n:好的\n                 file        file file file file file file  file file file file   file file file file file    file file file file file file file file file file file file  file file  file  (: What of is you use in a map that that the word clues words:A\n noun -A: Find\n\nAnswer: aA) promise,1) pillar ( (: Theouns\n']

\paragraph{Output quality}
Finally, the large LLMs from which the teacher model was selected could not generate perfectly adequate outputs either, being overly vague in explanations or, on the opposite, bringing up irrelevant or hallucinated details. For example, for a task question: "\textit{What's the difference between weather and climate?}" one of the generations was: "\textit{Climate is the pattern of weather in a certain place. It got down to 3°C in Athens, Greece, last night!}", which included irrelevant hallucinated information about the weather in Athens on a specific date. 

% Question: Which property do these three objects have in common?	Choices: ['stretchy', 'transparent', 'rough']
% Solution: An object has different properties. A property of an object can tell you how it looks, feels, tastes, or smells. Properties can also tell you how an object will behave when something happens to it. Different objects can have properties in common. You can use these properties to put objects into groups. Grouping objects by their properties is called classification.

\section{Conclusion}
\label{Conclusion}
In conclusion, while big LLMs demonstrate strong capabilities in multimodal scientific question answering and benefit from extracting information from available solutions, their performance in reasoning from lectures often falls short or even declines. On the other hand, small foundation models like \texttt{Qwen2-VL-2B-Instruct} and \texttt{paligemma-3b-pt-224} show limited effectiveness in scientific reasoning tasks, both in zero-shot settings and when fine-tuned with adapter-based methods. Furthermore, inconsistencies in evaluation metrics (zero-shot, Prefix Tuning, LoRA) across these models highlight challenges in establishing reliable performance benchmarks. The poor performance of adapter tuning may be attributed to the current loss function design, suggesting the need for refinement in optimization strategies. Lastly, knowledge distillation underperforms when compared to directly training on a curated, high-quality dataset, emphasizing the importance of data quality in achieving robust model performance. \newline
It was noticed that all four models had more similar performance with knowledge distillation than with fine-tuning. Another oberservation is that LoRA outperforms Prefix Tuning on both models for knowledge distillation.
\footnote{All our work is available on our GitHub repository: https://github.com/katja-kolos/foundation\_models}

\section{Future work}
\label{Future work}
While we observed that learning from teachers' outputs leads to lower performance compared to learning from human-curated solutions, the precise impact remains to be measured, since we were not able to derive accuracy for the student models. If the teacher's performance is estimated at 80\% of human performance, would models trained on the teacher's outputs achieve 80\% of the performance of those trained on golden data, or could the student's ability to learn from noisy data mitigate this effect?
One could also run experiments with different Knowledge Distillation techniques like the ones introduced in Background (\ref{related-work}) or \cite{gou2021knowledge}.

% copy from Background:
%Alternatively, with \textit{feature-based KD} the knowledge from specific layers of the teacher model is distilled into the student model \cite{sepahvand2022teacher}, while the student model's structure may be a quantized, simplified, or condensed version of the teacher's architecture \cite{gou2021knowledge}. 

Another direction for future work could be trying out other adapter architectures, starting from larger student models (which would require more computational resources) but probably provide better results, and learning with a less strict loss function.
\newline
We could additionally define a multi-head setup for fine-tuning, with part of the model being responsible for explanation generation and another one for answer prediction.



%\printbibliography
\newpage
\newpage
\bibliography{references}
\bibliographystyle{icml2021}

\end{document}