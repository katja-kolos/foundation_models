% dataset
@inproceedings{lu2022learn,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}

% approaches
%% knowledge distillation
@article{shridhar2022distilling,
  title={Distilling reasoning capabilities into smaller language models},
  author={Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2212.00193},
  year={2022}
}
@article{magister2022teaching,
  title={Teaching small language models to reason},
  author={Magister, Lucie Charlotte and Mallinson, Jonathan and Adamek, Jakub and Malmi, Eric and Severyn, Aliaksei},
  journal={arXiv preprint arXiv:2212.08410},
  year={2022}
}
@inproceedings{li2024turning,
  title={Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data},
  author={Li, Yiwei and Yuan, Peiwen and Feng, Shaoxiong and Pan, Boyuan and Sun, Bin and Wang, Xinglin and Wang, Heda and Li, Kan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18591--18599},
  year={2024}
}
@article{gou2021knowledge,
  title={Knowledge distillation: A survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  number={6},
  pages={1789--1819},
  year={2021},
  publisher={Springer}
}
@article{yu2023dataset,
  title={Dataset distillation: A comprehensive review},
  author={Yu, Ruonan and Liu, Songhua and Wang, Xinchao},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}
%%% feature-based KD
@article{sepahvand2022teacher,
  title={Teacher--student knowledge distillation based on decomposed deep feature representation for intelligent mobile applications},
  author={Sepahvand, Majid and Abdali-Mohammadi, Fardin and Taherkordi, Amir},
  journal={Expert Systems with Applications},
  volume={202},
  pages={117474},
  year={2022},
  publisher={Elsevier}
}
%% prompt design
@article{schulhoff2024prompt,
  title={The Prompt Report: A Systematic Survey of Prompting Techniques},
  author={Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and others},
  journal={arXiv preprint arXiv:2406.06608},
  year={2024}
}
@article{chang2024efficient,
  title={Efficient Prompting Methods for Large Language Models: A Survey},
  author={Chang, Kaiyan and Xu, Songcheng and Wang, Chenglong and Luo, Yingfeng and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2404.01077},
  year={2024}
}
@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:2211.01910},
  year={2022}
}
@article{ge2023context,
  title={In-context autoencoder for context compression in a large language model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  journal={arXiv preprint arXiv:2307.06945},
  year={2023}
}
@article{weston2023system,
  title={System 2 Attention (is something you might need too)},
  author={Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2311.11829},
  year={2023}
}
%% agentic architectures
@article{lin2024swiftsage,
  title={Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks},
  author={Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Ammanabrolu, Prithviraj and Choi, Yejin and Ren, Xiang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{ghafarollahi2024sciagents,
  title={SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning},
  author={Ghafarollahi, Alireza and Buehler, Markus J},
  journal={arXiv preprint arXiv:2409.05556},
  year={2024}
}
@article{karpas2022mrkl,
  title={MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
  author={Karpas, Ehud and Abend, Omri and Belinkov, Yonatan and Lenz, Barak and Lieber, Opher and Ratner, Nir and Shoham, Yoav and Bata, Hofit and Levine, Yoav and Leyton-Brown, Kevin and others},
  journal={arXiv preprint arXiv:2205.00445},
  year={2022}
}
@article{lester2021prompt,
    title={The Power of Scale for Parameter-Efficient Prompt Tuning},
    author={Brian Lester, Rami Al-Rfou, Noah Constant},
    journal={arXiv preprint arXiv:2104.08691},
    year={2021}
}
