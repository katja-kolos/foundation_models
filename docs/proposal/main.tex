%%%%%%%%%
% Generic article template using additional packages for table formatting
% New commands are defined for project participants: \katja, \flo, \dasha - 
% to draw attention to important remarks directly in the text
%%%%%%%%%

\documentclass{article}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{adjustbox}

\newcommand{\katja}[1]{\textbf{\textcolor{teal}{(Katja) #1}}}
\newcommand{\dasha}[1]{\textbf{\textcolor{brown}{(Dasha) #1}}}
\newcommand{\flo}[1]{\textbf{\textcolor{red}{(Flo) #1}}}


\begin{document}

\title{Scientific Reasoning with LLMs}

\author{Group ??: Florian \and Ekaterina Kolos \and Daria }

\maketitle             


\section{Introduction}

Scientific machine reasoning is ... 

For our project, we intend to ...

Expected implementation details ...

\section{Background and Related Work}
% Large Language Models

\paragraph{Knowledge distillation (KD)} allows to obtain smaller models capable of successfully following the behavior of larger teacher models. This is particularly useful for privacy reasons (running AI applications on mobile devices) and for cases where access to very large models in the cloud is not guaranteed (e.g. in cars). The teacher model can be used to intelligently select examples on which the student model is trained (dataset distillation) \cite{yu2023dataset}, while \cite{li2024turning} rely on negative samples to show the student what incorrect answers or reasoning paths it should avoid to improve task accuracy. Training small models on a CoT reasoning path of a larger model was also shown to be a way to obtain a small student model replicating reasoning capabilities of teacher on downstream tasks \cite{magister2022teaching}, which is close to _response-based KD_ where the student model mimics the output of the teacher. More effective can be _feature-based KD_ where student also partially replicates the teacher on a feature-based level, when knowledge from specific layers of the teacher model is distilled into the student model \cite{sepahvand2022teacher}, while the student model's structure may be a quantized, simplified, or condensed version of the teacher's architecture \cite{gou2021knowledge}. 

\katja{Crazy idea: can we distill a reasoning path of a whole agent into a smaller model? I.e. instead of intelligent decision making that a large model performs for a complex task (search for X first, then look up Y, then formulate hypotheses Z) the student model learns the "protocols" of "customer support" and acts accordingly, without actually "understanding" why a particular action is good in a particular situation}

Agent-based architectures \cite{lin2024swiftsage} \cite{ghafarollahi2024sciagents}

Prompting techniques \cite{schulhoff2024prompt}

Scientific reasoning and the dataset \cite{lu2022learn}

\section{Methodology}
We postulate the following research questions: 
RQ1: Can smaller student LLMs be still effective at few-shot prompting for scientific reasoning?
RQ2: How does their performance compare with that of a teacher model?
RQ3: 
\paragraph{RQ1} 
We plan to 

\paragraph{RQ2}
We plan to

\paragraph{RQ3}
We plan to

\paragraph{Metrics}

\section{Approximate Timeline}
Maybe just a paragraph here


\katja{Have reverted the doc to simple "article"}

\bibliography{mybibliography}
\bibliographystyle{plain}

\end{document}
