%%%%%%%%%
% Generic article template using additional packages for table formatting
% New commands are defined for project participants: \katja, \flo, \dasha - 
% to draw attention to important remarks directly in the text
%%%%%%%%%

\documentclass[10pt]{article}
\usepackage[a4paper, margin=1.25in]{geometry}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{adjustbox}

\usepackage{titlesec}
\titlespacing*{\section}{0pt}{*0.8}{*0.8}

\newcommand{\katja}[1]{\textbf{\textcolor{teal}{(Katja) #1}}}
\newcommand{\dasha}[1]{\textbf{\textcolor{brown}{(Dasha) #1}}}
\newcommand{\flo}[1]{\textbf{\textcolor{red}{(Flo) #1}}}


\begin{document}

\title{Scientific Reasoning with LLMs}

%we'll probably have a group number later
%names are given in alphabetical order
\author{Stuttgart Team: Florian Dreyer (3667877) \and Ekaterina Kolos (3688474) \and Daria Matiash (3668740)}

\maketitle             


\section{Introduction}

Scientific machine reasoning is the application of AI techniques to simulate scientific reasoning processes, such as  data interpretation, hypothesis generation, and causal inference. It facilitates scientific inquiry and discovery by enabling machines to independently analyze scientific data, generate hypotheses, and make predictions.

For our project, we intend to ...

Expected implementation details ...

\section{Background and Related Work}
% Large Language Models

\paragraph{Prompting techniques}. A prompt is a command a user gives to a generative LLM used to guide its output for a downstream task, which contains several of the following: a role (persona) the LLM has to follow (e.g. "You are a helpful assistant"), a precise task description, examples (exemplars) for analogous learning (few-shot prompting), requirements on the process of reasoning, style instructions, output format requirements, additional information for in-context learning, emotion prompting components (highlighting why the task or a particular requirement is important). Prompts can be combined into sequences or graphs (with complex branching and parallelism). Of particular interest are small requirements on the reasoning process that can significantly improve performance when added to the prompt, such as "Rephrase and expand the question, and respond" \cite{}, or "Let’s think step by step" \cite{kojima2022}. Chain-of-thought prompts improve reasoning capabilities by asking the model to speak its process of thinking out loud, with variations including self-ask, step-back prompting, thread-of-thought, least-to-most prompting (decomposing then solving), plan-and-solve prompting, tree-of-thought, recursion-of-thought and many more \cite{schulhoff2024prompt}. 
%with the resulting systems possibly using multiple CoT reasoning trajectories to select the most likely one (e.g. Uncertainty-Routed CoT Prompting, or making an LLM generate various prompting instructions that are then tested by another LLM on 24 NLP tasks to chose the best one \cite{zhou2022large}). %katja: I omit this to save space as not directly relevant
Furthermore, prompts themselves can be compressed and automatically optimized to improve efficiency and reduce costs \cite{chang2024efficient}. \cite{ge2023context} compress a long context 4x into memory slots, while \cite{weston2023system} ask the LLM to first summarize the prompt and then execute it. Self-criticism and ensembling techniques can further be used to improve reasoning capabilities \cite{schulhoff2024prompt}. 

Soft prompting is an alternative to fine-tuning where the model is frozen while only a small number of "soft" prompt parameters are trained \cite{lester2021prompt}. These parameters are used to guide the model in the right direction.

\paragraph{Knowledge distillation (KD)} allows to obtain smaller models capable of successfully following the behavior of larger teacher models. This is particularly useful for privacy reasons (running AI applications on mobile devices) and for cases where access to very large models in the cloud is not guaranteed (e.g. in cars). The teacher model can be used to intelligently select examples on which the student model is trained (dataset distillation) \cite{yu2023dataset}, or provide on negative samples to show the student what incorrect answers or reasoning paths it should avoid to improve task accuracy  \cite{li2024turning} (c.g. contrastive CoT). Training small models on a CoT reasoning path of a larger model was also shown to be a way to obtain a small student model replicating reasoning capabilities of teacher on downstream tasks \cite{magister2022teaching}, which is close to \textit{response-based KD} where the student model mimics the output of the teacher. More effective can be \textit{feature-based KD} where student also partially replicates the teacher on a feature-based level, when knowledge from specific layers of the teacher model is distilled into the student model \cite{sepahvand2022teacher}, while the student model's structure may be a quantized, simplified, or condensed version of the teacher's architecture \cite{gou2021knowledge}. 

\katja{Crazy idea: can we distill a reasoning path of a whole agent into a smaller model? I.e. instead of intelligent decision making that a large model performs for a complex task (search for X first, then look up Y, then formulate hypotheses Z) the student model learns the "protocols" of "customer support" and acts accordingly, without actually "understanding" why a particular action is good in a particular situation}

Agent-based architectures \cite{lin2024swiftsage} \cite{ghafarollahi2024sciagents}
Reasoning and Acting (ReAct) 

Scientific reasoning and the dataset \cite{lu2022learn}

\section{Methodology}
We postulate the following research questions: 
RQ1: Can we build a LLM Agent to improve the LLMs performance on science questions?
RQ2: Does Soft Prompting improve the performance of the LLM (Agent)?
RQ3: Can an LLM-agent's behavior be distilled into a single model?
\paragraph{RQ1} 
We plan to 

\paragraph{RQ2}
We plan to

\paragraph{RQ3}
In order to distill the knowledge from the model fine-tuned in previous steps, we plan to do the model distillation with Chain-of-Thought Prompting for Reasoning approach. Following techniques described in \cite{magister2022teaching} \cite{wei2022chain}, the student model will be fine-tuned using these CoT responses to produce intermediate reasoning steps. At the same time, the teacher model generates multiple CoT responses, and the student learns from the aggregate (self-consistent) reasoning paths. The described approach can help the student model avoid common mistakes and output more consistent answers.

\paragraph{Dataset}
We plan to use the ScienceQA dataset \cite{lu2022learn}. ScienceQA is a multimodal dataset containing science questions and their answers.

\paragraph{Metrics}
We'll evaluate model's performance with question answering accuracy domain-wise in order to have a fair comparison with leaderboards. A few metrics used in machine translation such as BLEU-1, BLEU-4 and BERTScore can also be used, but the main metric we'll rely on is accuracy for question answering.

\section{Approximate Timeline}
-Now - mid November: 
  Preliminary baseline tests: small LLM relying only on commonsense reasoning / only on pretraining 
  zero-shot, few-shot
  optional: stronger baseline – same LLM with better prompting techniques like CoT/Self-Ask
-mid November - mid December:
  Construct agentic / RAG pipeline with “big smart” model + additional resources.
  optional: soft-prompting for better workflow^ to distill from
-mid December - mid January:
  Knowledge distillation experiments.


\katja{Have reverted the doc to simple "article"}

\bibliography{mybibliography}
\bibliographystyle{plain}

\end{document}
