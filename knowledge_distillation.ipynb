{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ktdrhWnZ87XJ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6TP2Hzr87XM",
    "outputId": "2c09cfa8-ab7b-402a-de07-099e0163bc85",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:23:05.103035Z",
     "start_time": "2024-12-19T23:23:05.079729Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import helpers\n",
    "import prefix_tuning\n",
    "import importlib\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "importlib.reload(helpers)\n",
    "importlib.reload(prefix_tuning)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prefix_tuning' from '/Users/floriandreyer/Library/Mobile Documents/com~apple~CloudDocs/Python Projekte/foundation_models/prefix_tuning.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uvM8QDGl87XP",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:12:32.196078Z",
     "start_time": "2024-12-19T23:12:32.193145Z"
    }
   },
   "source": [
    "#device = torch.device('mps' if (torch.backends.mps.is_available() and torch.backends.mps.is_built()) else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_EPOCHS_FT = 100\n",
    "NUM_EPOCHS_KD = 100\n",
    "BATCH_SIZE = 32"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:12:32.266632Z",
     "start_time": "2024-12-19T23:12:32.263333Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pNEtAiGe87XP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Get Data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gjuLrTbq87XQ",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:23:30.826204Z",
     "start_time": "2024-12-19T23:23:08.214815Z"
    }
   },
   "source": [
    "### train data\n",
    "# data with label and image data\n",
    "df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))\n",
    "\n",
    "df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index()\n",
    "df_train_label['input'] = df_train_label.apply(lambda row: helpers.build_prompt(row)[0], axis=1)\n",
    "df_train_label['message'] = df_train_label.apply(lambda row: helpers.build_message(row), axis=1)\n",
    "\"\"\"transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "print(\"test\")\n",
    "df_train_label['image'] = df_train_label.apply(lambda row: transform(row['image']) if row['image'] else transform(Image.new(\"RGB\", (224, 224), (0, 0, 0))), axis=1)\n",
    "print(\"test\")\"\"\"\n",
    "df_train_label[['index', 'message', 'answer', 'explanation', 'image']] = df_train_label[['index', 'message', 'answer', 'solution', 'image']]\n",
    "\n",
    "# # data from Gemini for KD\n",
    "df_train_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "df_train_gemini = pd.merge(df_train_gemini, df_train_label[['index', 'image']], on='index')\n",
    "df_train_gemini['message'] = df_train_gemini.apply(lambda row: helpers.build_message(row), axis=1)"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "izTEvl6i87XR",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:23:39.763343Z",
     "start_time": "2024-12-19T23:23:30.853505Z"
    }
   },
   "source": [
    "### val data\n",
    "df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))\n",
    "df_val['input'] = df_val.apply(lambda row: helpers.build_prompt(row)[0], axis=1)\n",
    "df_val['message'] = df_val.apply(lambda row: helpers.build_message(row), axis=1)\n",
    "\"\"\"transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "df_val['image'] = df_val.apply(lambda row: transform(row['image']) if row['image'] else transform(Image.new(\"RGB\", (224, 224), (0, 0, 0))), axis=1)\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'transform = transforms.Compose([\\n        transforms.Resize((224, 224)),\\n        transforms.ToTensor()\\n    ])\\ndf_val[\\'image\\'] = df_val.apply(lambda row: transform(row[\\'image\\']) if row[\\'image\\'] else transform(Image.new(\"RGB\", (224, 224), (0, 0, 0))), axis=1)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:04.825223Z",
     "start_time": "2024-12-19T23:13:40.073924Z"
    }
   },
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"float32\", device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4608394183243db96eb0ca14de845b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:06.538227Z",
     "start_time": "2024-12-19T23:14:04.858613Z"
    }
   },
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-19T21:17:21.093260Z",
     "start_time": "2024-12-19T21:17:21.087707Z"
    }
   },
   "source": [
    "def tokenize_dataset(df, tokenizer, input_column=\"input\"):\n",
    "\n",
    "    tokenized_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sample = tokenizer.encode(row[input_column], return_tensors=\"pt\").squeeze(0)\n",
    "        tokenized_data.append(sample)\n",
    "    df[\"input_ids\"] = tokenized_data\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:14.841344Z",
     "start_time": "2024-12-19T23:14:14.791490Z"
    }
   },
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": df_train_label.iloc[10]['image'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": df_train_label.iloc[10]['question'] + \" \" + ' '.join(df_train_label.iloc[10]['choices'])},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "\"\"\"# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\"\"\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Inference: Generation of the output\\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\\ngenerated_ids_trimmed = [\\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\\n]\\noutput_text = processor.batch_decode(\\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\\n)\\nprint(output_text)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T21:18:36.596402Z",
     "start_time": "2024-12-19T21:18:36.576811Z"
    }
   },
   "source": "inputs",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151653,   3838,   1558,  78684,   1492,\n",
       "            264,   6008,    653,     30,   1281,  19056,   3063,  11243,   3063,\n",
       "            501,  10901, 151645,    198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'pixel_values': tensor([[-0.2156, -0.0842, -0.0842,  ...,  2.1459,  2.1459,  2.1459],\n",
       "        [-0.0842, -0.0842, -0.0842,  ...,  2.1459,  2.1459,  1.7477],\n",
       "        [-0.0842,  1.9303,  1.9303,  ...,  2.1459,  2.1459,  2.1459],\n",
       "        ...,\n",
       "        [ 0.8209,  0.9230,  1.9011,  ...,  2.1459,  2.1459,  0.1835],\n",
       "        [ 1.9303,  1.9303,  1.9303,  ...,  0.1835,  0.1835,  0.1835],\n",
       "        [ 1.9303,  1.9303,  1.9303,  ...,  0.1835,  0.1835,  0.0555]]), 'image_grid_thw': tensor([[ 1, 10, 10]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_label[11]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_gemini.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T13:29:12.664536Z",
     "start_time": "2024-12-19T13:29:12.639277Z"
    }
   },
   "source": [
    "\"\"\"model_name = \"Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Datensatz tokenisieren\n",
    "tokenized_data = tokenize_dataset(df_train_gemini, tokenizer, input_column=\"input\", label_column=\"answer\")\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = SoftPromptingDataset(tokenized_data)\n",
    "\n",
    "# Zugriff auf ein Beispiel\n",
    "print(dataset[0])\"\"\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_name = \"Qwen2-VL-2B-Instruct\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\n\\n# Datensatz tokenisieren\\ntokenized_data = tokenize_dataset(df_train_gemini, tokenizer, input_column=\"input\", label_column=\"answer\")\\n\\n# Dataset erstellen\\ndataset = SoftPromptingDataset(tokenized_data)\\n\\n# Zugriff auf ein Beispiel\\nprint(dataset[0])'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NStdSFIf87XS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mBOk6eSR87XT",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:15:33.974798Z",
     "start_time": "2024-12-19T23:15:33.958765Z"
    }
   },
   "source": [
    "def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val):\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    for epoch in tqdm(range(NUM_EPOCHS_FT)):\n",
    "        error = 0\n",
    "        num_samples = 0\n",
    "        for X, y in dataloader_train:\n",
    "            labels = tokenizer(y, return_tensors=\"pt\")\n",
    "            text = processor.apply_chat_template(\n",
    "                X, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs = X[1]\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                labels=labels[\"input_ids\"],\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error += loss.item() * X.size(0)\n",
    "            num_samples += X.size(0)\n",
    "        error /= num_samples\n",
    "        print(f'Error after epoch {epoch}: {error}')\n",
    "        train_errors.append((epoch, error))\n",
    "        if epoch % 10:\n",
    "            val_error = 0\n",
    "            num_samples = 0\n",
    "            for X, y in dataloader_val:\n",
    "                labels = tokenizer(y, return_tensors=\"pt\")\n",
    "                text = processor.apply_chat_template(\n",
    "                    X['message'], tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                image_inputs = X[1]\n",
    "                inputs = processor(\n",
    "                    text=[text],\n",
    "                    images=image_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                outputs = model(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    labels=labels[\"input_ids\"],\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                val_error += loss.item() * X.size(0)\n",
    "                num_samples += X.size(0)\n",
    "            val_error /= num_samples\n",
    "            print(f'Validation error after epoch {epoch}: {val_error}')\n",
    "            val_errors.append((epoch, val_error))\n",
    "    return train_errors_ft, val_error"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yKhEnvHx87XU",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:21.670477Z",
     "start_time": "2024-12-19T23:14:21.665744Z"
    }
   },
   "source": [
    "def visualize_error(train_errors, val_errors):\n",
    "    plt.plot(zip(*train_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(zip(*val_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(\"Train and Validation Error over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:22.240717Z",
     "start_time": "2024-12-19T23:14:22.237100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \"\"\"image = row['image']\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image_tensor = transform(image) if image else transform(Image.new(\"RGB\", (224, 224), (0, 0, 0)))\"\"\"\n",
    "        return {'message': row['message'], 'image': row['image']}, row['answer'].astype(str)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T23:14:22.892853Z",
     "start_time": "2024-12-19T23:14:22.888634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prefix_collate(batch):\n",
    "    X, y = zip(*batch)\n",
    "    message = X[0]\n",
    "    image = X[1]\n",
    "    return [message, image], y"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eoPYPMBq87XU",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "## PrefixTuning using labels"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tcw_Fj0R87XV",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-19T23:23:55.193207Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"float32\",\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "model_prefix = prefix_tuning.PrefixTuningModel(model, tokenizer)\n",
    "optimizer = torch.optim.Adam(model_prefix.prefix_tuning.parameters(), lr=5e-5)\n",
    "# DataLoader for train data\n",
    "dataset_label_train = PrefixDataset(df_train_label)\n",
    "dataloader_label_train=DataLoader(dataset_label_train, collate_fn=prefix_collate, batch_size=32, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_label_val = PrefixDataset(df_val)\n",
    "dataloader_label_val=DataLoader(dataset_label_val, collate_fn=prefix_collate, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "045e0fad73324c288a9109cb48ce11d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bBY-hEz687XV",
    "ExecuteTime": {
     "end_time": "2024-12-19T23:20:20.179062Z",
     "start_time": "2024-12-19T23:20:20.047599Z"
    }
   },
   "source": "train_errors_ft, val_errors_ft = train(model_prefix, tokenizer, processor, optimizer, dataloader_label_train, dataloader_label_val)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not make batched images from {'message': [{'role': 'user', 'content': [{'type': 'image', 'image': tensor([[[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]],\n\n        [[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]],\n\n        [[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]]])}, {'type': 'text', 'text': 'Question: Which property matches this object?\\n Task: closed choice\\n Choices: (0) sour (1) salty\\n Hint: Select the better answer.'}]}], 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=172x208 at 0x3246CB0A0>}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_errors_ft, val_errors_ft \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_label_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_label_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[16], line 13\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val)\u001B[0m\n\u001B[1;32m      9\u001B[0m text \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[1;32m     10\u001B[0m     X, tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     12\u001B[0m image_inputs \u001B[38;5;241m=\u001B[39m X[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m---> 13\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpadding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\n\u001B[1;32m     20\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     21\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39minputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     22\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     23\u001B[0m )\n\u001B[1;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py:115\u001B[0m, in \u001B[0;36mQwen2VLProcessor.__call__\u001B[0;34m(self, images, text, videos, **kwargs)\u001B[0m\n\u001B[1;32m    109\u001B[0m output_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_kwargs(\n\u001B[1;32m    110\u001B[0m     Qwen2VLProcessorKwargs,\n\u001B[1;32m    111\u001B[0m     tokenizer_init_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39minit_kwargs,\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    113\u001B[0m )\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 115\u001B[0m     image_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvideos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moutput_kwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimages_kwargs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    116\u001B[0m     image_grid_thw \u001B[38;5;241m=\u001B[39m image_inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_grid_thw\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[0;34m(self, images, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:394\u001B[0m, in \u001B[0;36mQwen2VLImageProcessor.preprocess\u001B[0;34m(self, images, videos, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format)\u001B[0m\n\u001B[1;32m    391\u001B[0m do_convert_rgb \u001B[38;5;241m=\u001B[39m do_convert_rgb \u001B[38;5;28;01mif\u001B[39;00m do_convert_rgb \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_convert_rgb\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m images \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 394\u001B[0m     images \u001B[38;5;241m=\u001B[39m \u001B[43mmake_batched_images\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m videos \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    396\u001B[0m     videos \u001B[38;5;241m=\u001B[39m make_batched_videos(videos)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl.py:79\u001B[0m, in \u001B[0;36mmake_batched_images\u001B[0;34m(images)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_valid_image(images):\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [images]\n\u001B[0;32m---> 79\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not make batched images from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimages\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Could not make batched images from {'message': [{'role': 'user', 'content': [{'type': 'image', 'image': tensor([[[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]],\n\n        [[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]],\n\n        [[1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000],\n         [1.0000, 0.9843, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [0.9725, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         ...,\n         [0.9765, 0.8941, 0.8980,  ..., 0.8980, 0.8941, 0.9765],\n         [1.0000, 0.9804, 0.9137,  ..., 0.9137, 0.9843, 1.0000],\n         [1.0000, 1.0000, 0.9882,  ..., 0.9882, 1.0000, 1.0000]]])}, {'type': 'text', 'text': 'Question: Which property matches this object?\\n Task: closed choice\\n Choices: (0) sour (1) salty\\n Hint: Select the better answer.'}]}], 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=172x208 at 0x3246CB0A0>}"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weNRYFe787XX"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_ft, val_errors_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qw5SW6bP87XY",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV4PV0m087XY"
   },
   "outputs": [],
   "source": [
    "model_knowledge_distillation = soft_prompting.MultimodalSoftPrompting.from_pretrained(model)\n",
    "# DataLoader for train data\n",
    "dataset_gemini_train = SoftPromptingDataset(df_train_gemini, model_fine_tuned)\n",
    "dataloader_gemini_train=DataLoader(dataset_gemini_train, batch_size=32, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_gemini_val = SoftPromptingDataset(df_val, model_fine_tuned)\n",
    "dataloader_gemini_val=DataLoader(dataset_gemini_val, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP04R7VX87XZ"
   },
   "outputs": [],
   "source": [
    "train_errors_kd, val_errors_kd = train(dataset_gemini_train, dataloader_gemini_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJn7pB2b87XZ"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_kd, val_errors_kd)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
