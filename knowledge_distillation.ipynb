{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports & Setup"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from helpers import build_prompt, build_message\n",
    "from prefix_tuning import PrefixTuningModelPastKeyValues, PrefixDataset, prefix_collate\n",
    "import prefix_tuning\n",
    "import helpers\n",
    "import importlib\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor\n",
    "import gc\n",
    "importlib.reload(helpers)\n",
    "importlib.reload(prefix_tuning)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.cuda.empty_cache()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"mps\")\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "\n",
    "#device = torch.device('cuda:0,1' if torch.cuda.is_available() else 'cpu')\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_EPOCHS_FT = 100\n",
    "NUM_EPOCHS_KD = 100\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!nvidia-smi"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get Data and preprocess it"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### train data\n",
    "# data with label and image data\n",
    "df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))\n",
    "\n",
    "df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index()\n",
    "df_train_label['image'] = df_train_label.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_train_label['input'] = df_train_label.apply(lambda row: build_prompt(row)[0], axis=1)\n",
    "df_train_label['message'] = df_train_label.apply(lambda row: build_message(row), axis=1)\n",
    "\n",
    "# # data from Gemini for KD\n",
    "df_train_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "df_train_gemini = pd.merge(df_train_gemini, df_train_label[['index', 'image']], on='index')\n",
    "df_train_gemini['message'] = df_train_gemini.apply(lambda row: build_message(row), axis=1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### val data\n",
    "df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))\n",
    "\n",
    "df_val = df_val[df_val['solution'] != ''].reset_index()\n",
    "df_val['image'] = df_val.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_val['input'] = df_val.apply(lambda row: build_prompt(row)[0], axis=1)\n",
    "df_val['message'] = df_val.apply(lambda row: build_message(row), axis=1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions for model training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_input_qwen(tokenizer, processor, prompts, texts, images, y, device):\n",
    "    messages = [processor.apply_chat_template(\n",
    "                text, tokenize=False, add_generation_prompt=False\n",
    "    ) for text in texts]\n",
    "    image_inputs, video_inputs = process_vision_info(texts)\n",
    "    inputs = processor(\n",
    "        text=messages,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    max_length = inputs[\"input_ids\"].size(1)\n",
    "    labels = tokenizer(y, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return inputs.to(device, dtype=torch.bfloat16), labels.to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_input_paligemma(tokenizer, processor, prompts, texts, images, y, device):\n",
    "    images = [np.array(image.resize((224, 224))) / 127.5 -1 for image in images]\n",
    "    inputs = processor(\n",
    "        text=prompts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\"\n",
    "    )\n",
    "    labels = tokenizer(y, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    return inputs.to(device), labels.to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val, preprocess_func):\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(NUM_EPOCHS_FT)):\n",
    "        error = 0\n",
    "        num_samples = 0\n",
    "        for prompts, texts, images, y in dataloader_train:\n",
    "            inputs, labels = preprocess_func(tokenizer, processor, prompts, texts, images, y, device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(\n",
    "                    inputs=inputs,\n",
    "                    labels=labels,\n",
    "                )\n",
    "            #output_ids = outputs.logits.argmax(-1)\n",
    "            #output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error += loss.item() * len(texts)\n",
    "            num_samples += len(texts)\n",
    "            del labels, inputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        error /= num_samples\n",
    "        print(f'Error after epoch {epoch}: {error}')\n",
    "        train_errors.append((epoch, error))\n",
    "        if epoch % 10:\n",
    "            val_error = 0\n",
    "            num_samples = 0\n",
    "            for prompts, texts, images, y in dataloader_val:\n",
    "                inputs, labels = preprocess_func(tokenizer, processor, prompts, texts, images, y, device)\n",
    "                outputs = model(\n",
    "                    inputs=inputs,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                val_error += loss.item() * len(texts)\n",
    "                num_samples += len(texts)\n",
    "                del labels, inputs\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            val_error /= num_samples\n",
    "            print(f'Validation error after epoch {epoch}: {val_error}')\n",
    "            val_errors.append((epoch, val_error))\n",
    "    return train_errors, val_error"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualize_error(train_errors, val_errors):\n",
    "    plt.plot(zip(*train_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(zip(*val_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(\"Train and Validation Error over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PrefixTuning using labels"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DataLoader for train data\n",
    "dataset_label_train = PrefixDataset(df_train_label)\n",
    "dataloader_label_train=DataLoader(dataset_label_train, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_label_val = PrefixDataset(df_val)\n",
    "dataloader_label_val=DataLoader(dataset_label_val, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Qwen"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "match_n_layer = model.config.num_hidden_layers\n",
    "match_n_head = model.config.num_key_value_heads\n",
    "n_embd = model.config.hidden_size // 6\n",
    "model_prefix = PrefixTuningModelPastKeyValues(model, match_n_layer, match_n_head, n_embd, device).to(device)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_errors_ft_qwen, val_errors_ft_qwen = train(model_prefix, tokenizer, processor, optimizer, dataloader_label_train, dataloader_label_val, preprocess_input_qwen)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_error(train_errors_ft_qwen, val_errors_ft_qwen)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Paligemma"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_prefix = PrefixTuningModelPastKeyValues(model, tokenizer).to(device)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_errors_ft_paligemma, val_errors_ft_paligemma = train(model_prefix, tokenizer, processor, optimizer, dataloader_label_train, dataloader_label_val, preprocess_input_paligemma)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_error(train_errors_ft_paligemma, val_errors_ft_paligemma)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Leaving the prompt blank for pre-trained models\n",
    "prompt = \"\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Knowledge Distillation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DataLoader for train data\n",
    "dataset_gemini_train = PrefixDataset(df_train_gemini)\n",
    "dataloader_gemini_train=DataLoader(dataset_gemini_train, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_label_val = PrefixDataset(df_val)\n",
    "dataloader_label_val=DataLoader(dataset_label_val, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Qwen"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "match_n_layer = model.config.num_hidden_layers\n",
    "match_n_head = model.config.num_key_value_heads\n",
    "n_embd = model.config.hidden_size // 6\n",
    "model_prefix = PrefixTuningModelPastKeyValues(model, match_n_layer, match_n_head, n_embd, device).to(device)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_errors_kd, val_errors_kd = train(dataset_gemini_train, dataloader_gemini_train)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_error(train_errors_kd, val_errors_kd)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Paligemma"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg\"\n",
    "image = load_image(url)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"auto\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_prefix = PrefixTuningModelPastKeyValues(model, tokenizer).to(device)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_errors_kd_paligemma, val_errors_kd_paligemma = train(model_prefix, tokenizer, processor, optimizer, dataloader_gemini_train, dataloader_label_val, preprocess_input_paligemma)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "visualize_error(train_errors_kd_paligemma, val_errors_kd_paligemma)"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
