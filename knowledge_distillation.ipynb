{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktdrhWnZ87XJ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T17:02:35.159927Z",
     "start_time": "2024-12-28T17:02:14.272513Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6TP2Hzr87XM",
    "outputId": "2c09cfa8-ab7b-402a-de07-099e0163bc85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 21:23:20.272269: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-28 21:23:20.285874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735417400.302630 2150482 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735417400.307851 2150482 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-28 21:23:20.325674: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/lhome/matiada/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lhome/matiada/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensorflow.python.keras.backend import dtype\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "#import helpers\n",
    "#import prefix_tuning\n",
    "import importlib\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "#importlib.reload(helpers)\n",
    "#importlib.reload(prefix_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:35:03.331713Z",
     "start_time": "2024-12-28T14:35:03.319640Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "def get_question_text(problem):\n",
    "    question = problem['question']\n",
    "    return question\n",
    "\n",
    "\n",
    "def get_choice_text(probelm, options):\n",
    "    choices = probelm['choices']\n",
    "    choice_list = []\n",
    "    for i, c in enumerate(choices):\n",
    "        choice_list.append(\"({}) {}\".format(options[i], c))\n",
    "    choice_txt = \" \".join(choice_list)\n",
    "    return choice_txt\n",
    "\n",
    "\n",
    "def get_context_text(problem, use_caption):\n",
    "    txt_context = problem['hint']\n",
    "    img_context = problem['caption'] if use_caption else \"\"\n",
    "    context = \" \".join([txt_context, img_context]).strip()\n",
    "    if context == \"\":\n",
    "        context = \"N/A\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def build_prompt(question_data, use_lecture=False, use_solution=False):\n",
    "    question = get_question_text(question_data)\n",
    "    choices = get_choice_text(question_data, [choice_num for choice_num in range(5)])\n",
    "    hint = get_context_text(question_data, False)\n",
    "    #image = question_data['image']\n",
    "    task = question_data['task']\n",
    "    input_prompt = f'Question: {question}\\n Task: {task}\\n Choices: {choices}\\n Hint: {hint}'\n",
    "    if use_lecture:\n",
    "        lecture = f'\\n Lecture: {question_data[\"lecture\"]}'\n",
    "        input_prompt += lecture\n",
    "    if use_solution and question_data[\"solution\"]:\n",
    "        solution = f'\\n Solution: {question_data[\"solution\"]}'\n",
    "        input_prompt += solution\n",
    "    prompt = [input_prompt]\n",
    "    #if image:\n",
    "    #    prompt.append(image)\n",
    "    return prompt\n",
    "\n",
    "def build_message(row):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": row[\"image\"],\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": row['input']},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prefix_tuning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:35:03.348809Z",
     "start_time": "2024-12-28T14:35:03.344061Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PrefixTuning(nn.Module):\n",
    "    def __init__(self, config, prefix_length=10):\n",
    "        super().__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.prefix_embeddings = nn.Parameter(torch.randn(prefix_length, config.hidden_size))\n",
    "\n",
    "    def forward(self, inputs_embeds):\n",
    "        batch_size = inputs_embeds.size(0)\n",
    "        prefix = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        return torch.cat([prefix.to(device, dtype=torch.bfloat16), inputs_embeds.to(device, dtype=torch.bfloat16)], dim=1)\n",
    "\n",
    "\n",
    "class PrefixTuningModel(nn.Module):\n",
    "    def __init__(self, model, tokenizer, prefix_length=10):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.freeze_main_model()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prefix_tuning = PrefixTuning(self.model.config, prefix_length)\n",
    "\n",
    "    def freeze_main_model(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        inputs_embeds = self.model.get_input_embeddings()(inputs[\"input_ids\"])\n",
    "        # Add Prefix\n",
    "        inputs_embeds = self.prefix_tuning(inputs_embeds)\n",
    "\n",
    "        # Modify attention mask for prefix\n",
    "        prefix_mask = torch.ones((inputs[\"input_ids\"].size(0), self.prefix_tuning.prefix_length), device=device)\n",
    "        attention_mask = torch.cat([prefix_mask, inputs[\"attention_mask\"]], dim=1)\n",
    "\n",
    "        return self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, pixel_values=inputs[\"pixel_values\"], labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:52:36.737590Z",
     "start_time": "2024-12-28T14:52:36.703341Z"
    },
    "id": "uvM8QDGl87XP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A6000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"mps\")\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "\n",
    "#device = torch.device('cuda:0,1' if torch.cuda.is_available() else 'cpu')\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_EPOCHS_FT = 100\n",
    "NUM_EPOCHS_KD = 100\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:45:17.047135Z",
     "start_time": "2024-12-28T14:45:17.037173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 28 21:24:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off |   00000000:15:00.0 Off |                  Off |\n",
      "| 31%   51C    P8             19W /  300W |      18MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off |   00000000:2D:00.0 Off |                  Off |\n",
      "| 30%   31C    P8              8W /  300W |     121MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      6589      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      6589      G   /usr/lib/xorg/Xorg                            107MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNEtAiGe87XP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Get Data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:45:45.719119Z",
     "start_time": "2024-12-28T14:45:19.267655Z"
    },
    "id": "gjuLrTbq87XQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since derek-thomas/ScienceQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /lhome/matiada/.cache/huggingface/datasets/derek-thomas___science_qa/default/0.0.0/f18b0a70359ebfb41f658fd564208d0355b013f4 (last modified on Sat Dec 28 18:59:54 2024).\n"
     ]
    }
   ],
   "source": [
    "### train data\n",
    "# data with label and image data\n",
    "df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))\n",
    "\n",
    "df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index()\n",
    "df_train_label['image'] = df_train_label.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_train_label['input'] = df_train_label.apply(lambda row: build_prompt(row)[0], axis=1)\n",
    "df_train_label['message'] = df_train_label.apply(lambda row: build_message(row), axis=1)\n",
    "\n",
    "# # data from Gemini for KD\n",
    "df_train_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "df_train_gemini = pd.merge(df_train_gemini, df_train_label[['index', 'image']], on='index')\n",
    "df_train_gemini['message'] = df_train_gemini.apply(lambda row: build_message(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:47:30.198764Z",
     "start_time": "2024-12-28T14:47:20.149122Z"
    },
    "id": "izTEvl6i87XR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since derek-thomas/ScienceQA couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /lhome/matiada/.cache/huggingface/datasets/derek-thomas___science_qa/default/0.0.0/f18b0a70359ebfb41f658fd564208d0355b013f4 (last modified on Sat Dec 28 18:59:54 2024).\n"
     ]
    }
   ],
   "source": [
    "### val data\n",
    "df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))\n",
    "df_val['image'] = df_val.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_val['input'] = df_val.apply(lambda row: build_prompt(row)[0], axis=1)\n",
    "df_val['message'] = df_val.apply(lambda row: build_message(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NStdSFIf87XS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:52:41.005894Z",
     "start_time": "2024-12-28T14:52:40.980903Z"
    },
    "id": "mBOk6eSR87XT"
   },
   "outputs": [],
   "source": [
    "# def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val):\n",
    "#     train_errors = []\n",
    "#     val_errors = []\n",
    "#     model.train()\n",
    "#     for epoch in tqdm(range(NUM_EPOCHS_FT)):\n",
    "#         error = 0\n",
    "#         num_samples = 0\n",
    "#         for texts, images, y in dataloader_train:\n",
    "#             print(num_samples)\n",
    "#             messages = [processor.apply_chat_template(\n",
    "#                     text, tokenize=False, add_generation_prompt=False\n",
    "#             ) for text in texts]\n",
    "#             image_inputs, video_inputs = process_vision_info(texts)\n",
    "#             inputs = processor(\n",
    "#                 text=messages,\n",
    "#                 images=image_inputs,\n",
    "#                 videos=video_inputs,\n",
    "#                 padding=True,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             )\n",
    "#             labels = tokenizer(y, padding=True, return_tensors=\"pt\")[\"input_ids\"].tolist()\n",
    "#             max_length = inputs[\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "#             for i in range(len(labels)):\n",
    "#                 labels[i] += [tokenizer.pad_token_id] * (max_length - len(labels[i]))\n",
    "\n",
    "#             inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "#             print(inputs)\n",
    "# #             inputs = inputs.to(device)\n",
    "#             labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "            \n",
    "# #             labels = torch.tensor(labels).to(device, dtype=torch.bfloat16)\n",
    "# #             inputs = inputs.to(device)\n",
    "# #             labels = labels.to(device)\n",
    "            \n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "# #             inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "            \n",
    "#             outputs = model(inputs, labels=labels)\n",
    "#             #output_ids = outputs.logits.argmax(-1)\n",
    "#             #output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             error += loss.item() * len(texts)\n",
    "#             num_samples += len(texts)\n",
    "#             del labels, inputs\n",
    "#         error /= num_samples\n",
    "#         print(f'Error after epoch {epoch}: {error}')\n",
    "#         train_errors.append((epoch, error))\n",
    "#         if epoch % 10:\n",
    "#             val_error = 0\n",
    "#             num_samples = 0\n",
    "#             for texts, images, y in dataloader_val:\n",
    "#                 labels = tokenizer(y, padding=True, return_tensors=\"pt\")\n",
    "#                 messages = [processor.apply_chat_template(\n",
    "#                     text, tokenize=False, add_generation_prompt=False\n",
    "#                 ) for text in texts]\n",
    "#                 image_inputs, video_inputs = process_vision_info(texts)\n",
    "#                 inputs = processor(\n",
    "#                     text=messages,\n",
    "#                     images=image_inputs,\n",
    "#                     videos=video_inputs,\n",
    "#                     padding=True,\n",
    "#                     return_tensors=\"pt\",\n",
    "#                 )\n",
    "#                 max_length = inputs[\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "#                 for i in range(len(labels)):\n",
    "#                     labels[i] += [tokenizer.pad_token_id] * (max_length - len(labels[i]))\n",
    "# #                 labels = torch.tensor(labels, dtype=torch.long)\n",
    "# #                 inputs = inputs.to(device)\n",
    "# #                 labels = labels.to(device)\n",
    "# #                 inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "            \n",
    "# #                 labels = torch.tensor(labels).to(device, dtype=torch.bfloat16)\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "                \n",
    "#                 outputs = model(\n",
    "#                     inputs=inputs,\n",
    "#                     labels=labels,\n",
    "#                 )\n",
    "#                 loss = outputs.loss\n",
    "#                 val_error += loss.item() * len(texts)\n",
    "#                 num_samples += len(texts)\n",
    "#             val_error /= num_samples\n",
    "#             print(f'Validation error after epoch {epoch}: {val_error}')\n",
    "#             val_errors.append((epoch, val_error))\n",
    "#     return train_errors_ft, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val):\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(NUM_EPOCHS_FT)):\n",
    "        error = 0\n",
    "        num_samples = 0\n",
    "        for texts, images, y in dataloader_train:\n",
    "            messages = [processor.apply_chat_template(\n",
    "                    text, tokenize=False, add_generation_prompt=False\n",
    "            ) for text in texts]\n",
    "            image_inputs, video_inputs = process_vision_info(texts)\n",
    "            inputs = processor(\n",
    "                text=messages,\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "#             labels = tokenizer(y, padding=True, return_tensors=\"pt\")[\"input_ids\"].tolist()\n",
    "            max_length = inputs[\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "            labels = tokenizer(y, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "#             for i in range(len(labels)):\n",
    "#                 labels[i] += [tokenizer.pad_token_id] * (max_length - len(labels[i]))\n",
    "#             labels = torch.tensor(labels, dtype=torch.long)\n",
    "            inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "            labels = labels.to(device)\n",
    "#             labels = labels.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            #output_ids = outputs.logits.argmax(-1)\n",
    "            #output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error += loss.item() * len(texts)\n",
    "            num_samples += len(texts)\n",
    "            del labels, inputs\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        error /= num_samples\n",
    "        print(f'Error after epoch {epoch}: {error}')\n",
    "        train_errors.append((epoch, error))\n",
    "        if epoch % 10:\n",
    "            val_error = 0\n",
    "            num_samples = 0\n",
    "            for texts, images, y in dataloader_val:\n",
    "                messages = [processor.apply_chat_template(\n",
    "                    text, tokenize=False, add_generation_prompt=False\n",
    "                ) for text in texts]\n",
    "                image_inputs, video_inputs = process_vision_info(texts)\n",
    "                inputs = processor(\n",
    "                    text=messages,\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                \n",
    "                \n",
    "                max_length = inputs[\"input_ids\"].size(1) + 10 # +10 for later prefix\n",
    "                labels = tokenizer(y, padding=\"max_length\",truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"]\n",
    "#                 for i in range(len(labels)):\n",
    "#                     labels[i] += [tokenizer.pad_token_id] * (max_length - len(labels[i]))\n",
    "#                 labels = torch.tensor(labels, dtype=torch.long)\n",
    "                inputs = inputs.to(device, dtype=torch.bfloat16)\n",
    "                labels = labels.to(device, dtype=torch.bfloat16)\n",
    "                outputs = model(\n",
    "                    inputs=inputs,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                val_error += loss.item() * len(texts)\n",
    "                num_samples += len(texts)\n",
    "                del labels, inputs\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            val_error /= num_samples\n",
    "            print(f'Validation error after epoch {epoch}: {val_error}')\n",
    "            val_errors.append((epoch, val_error))\n",
    "    return train_errors_ft, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:48:25.984132Z",
     "start_time": "2024-12-28T14:48:25.979851Z"
    },
    "id": "yKhEnvHx87XU"
   },
   "outputs": [],
   "source": [
    "def visualize_error(train_errors, val_errors):\n",
    "    plt.plot(zip(*train_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(zip(*val_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(\"Train and Validation Error over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:48:26.568626Z",
     "start_time": "2024-12-28T14:48:26.565131Z"
    }
   },
   "outputs": [],
   "source": [
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        return row['message'], row['image'], row['solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:48:27.141222Z",
     "start_time": "2024-12-28T14:48:27.138105Z"
    }
   },
   "outputs": [],
   "source": [
    "def prefix_collate(batch):\n",
    "    message, image, y = zip(*batch)\n",
    "    return message, image, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoPYPMBq87XU",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PrefixTuning using labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T14:54:40.385867Z",
     "start_time": "2024-12-28T14:54:10.953874Z"
    },
    "id": "Tcw_Fj0R87XV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "import gc\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "model_prefix = PrefixTuningModel(model, tokenizer).to(device)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)\n",
    "# DataLoader for train data\n",
    "dataset_label_train = PrefixDataset(df_train_label)\n",
    "dataloader_label_train=DataLoader(dataset_label_train, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_label_val = PrefixDataset(df_val)\n",
    "dataloader_label_val=DataLoader(dataset_label_val, collate_fn=prefix_collate, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-28T14:54:46.371845Z"
    },
    "id": "bBY-hEz687XV",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "train_errors_ft, val_errors_ft = train(model_prefix, tokenizer, processor, optimizer, dataloader_label_train, dataloader_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weNRYFe787XX"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_ft, val_errors_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qw5SW6bP87XY",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV4PV0m087XY"
   },
   "outputs": [],
   "source": [
    "model_knowledge_distillation = soft_prompting.MultimodalSoftPrompting.from_pretrained(model)\n",
    "# DataLoader for train data\n",
    "dataset_gemini_train = SoftPromptingDataset(df_train_gemini, model_fine_tuned)\n",
    "dataloader_gemini_train=DataLoader(dataset_gemini_train, batch_size=32, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_gemini_val = SoftPromptingDataset(df_val, model_fine_tuned)\n",
    "dataloader_gemini_val=DataLoader(dataset_gemini_val, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP04R7VX87XZ"
   },
   "outputs": [],
   "source": [
    "train_errors_kd, val_errors_kd = train(dataset_gemini_train, dataloader_gemini_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJn7pB2b87XZ"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_kd, val_errors_kd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"float32\", device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(df, tokenizer, input_column=\"input\"):\n",
    "\n",
    "    tokenized_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sample = tokenizer.encode(row[input_column], return_tensors=\"pt\").squeeze(0)\n",
    "        tokenized_data.append(sample)\n",
    "    df[\"input_ids\"] = tokenized_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": df_train_label.iloc[10]['image'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": df_train_label.iloc[10]['question'] + \" \" + ' '.join(df_train_label.iloc[10]['choices'])},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_name = \"Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Datensatz tokenisieren\n",
    "tokenized_data = tokenize_dataset(df_train_gemini, tokenizer, input_column=\"input\", label_column=\"answer\")\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = SoftPromptingDataset(tokenized_data)\n",
    "\n",
    "# Zugriff auf ein Beispiel\n",
    "print(dataset[0])\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
