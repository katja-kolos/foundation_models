{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ktdrhWnZ87XJ",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6TP2Hzr87XM",
    "outputId": "2c09cfa8-ab7b-402a-de07-099e0163bc85",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:11.329583Z",
     "start_time": "2024-12-25T13:08:08.823315Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tensorflow.python.keras.backend import dtype\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "import ast\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import helpers\n",
    "import prefix_tuning\n",
    "import importlib\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "importlib.reload(helpers)\n",
    "importlib.reload(prefix_tuning)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'prefix_tuning' from '/Users/floriandreyer/Library/Mobile Documents/com~apple~CloudDocs/Python Projekte/foundation_models/prefix_tuning.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uvM8QDGl87XP",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:11.338295Z",
     "start_time": "2024-12-25T13:08:11.336377Z"
    }
   },
   "source": [
    "#device = torch.device('mps' if (torch.backends.mps.is_available() and torch.backends.mps.is_built()) else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# CONSTANTS\n",
    "NUM_EPOCHS_FT = 100\n",
    "NUM_EPOCHS_KD = 100\n",
    "BATCH_SIZE = 32"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:11.387685Z",
     "start_time": "2024-12-25T13:08:11.385151Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pNEtAiGe87XP",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Get Data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gjuLrTbq87XQ",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:39.199359Z",
     "start_time": "2024-12-25T13:08:11.392702Z"
    }
   },
   "source": [
    "### train data\n",
    "# data with label and image data\n",
    "df_train_label = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='train'))\n",
    "\n",
    "df_train_label = df_train_label[df_train_label['solution'] != ''].reset_index()\n",
    "df_train_label['image'] = df_train_label.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_train_label['input'] = df_train_label.apply(lambda row: helpers.build_prompt(row)[0], axis=1)\n",
    "df_train_label['message'] = df_train_label.apply(lambda row: helpers.build_message(row), axis=1)\n",
    "\n",
    "# # data from Gemini for KD\n",
    "df_train_gemini = pd.read_csv('gemini_1_5_flash_output_train.csv', sep=\"\\t\")[['index', 'input', 'answer', 'explanation']]\n",
    "df_train_gemini = pd.merge(df_train_gemini, df_train_label[['index', 'image']], on='index')\n",
    "df_train_gemini['message'] = df_train_gemini.apply(lambda row: helpers.build_message(row), axis=1)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "izTEvl6i87XR",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:49.474139Z",
     "start_time": "2024-12-25T13:08:39.218068Z"
    }
   },
   "source": [
    "### val data\n",
    "df_val = pd.DataFrame(load_dataset('derek-thomas/ScienceQA', split='validation'))\n",
    "df_val['image'] = df_val.apply(lambda row: row['image'] if row['image'] else Image.new(\"RGB\", (224, 224), (0, 0, 0)), axis=1)\n",
    "df_val['input'] = df_val.apply(lambda row: helpers.build_prompt(row)[0], axis=1)\n",
    "df_val['message'] = df_val.apply(lambda row: helpers.build_message(row), axis=1)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NStdSFIf87XS",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mBOk6eSR87XT",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:42:04.325189Z",
     "start_time": "2024-12-25T13:42:04.302779Z"
    }
   },
   "source": [
    "def train(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val):\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(NUM_EPOCHS_FT)):\n",
    "        error = 0\n",
    "        num_samples = 0\n",
    "        for texts, images, y in dataloader_train:\n",
    "            print(texts)\n",
    "            print(y)\n",
    "            messages = [processor.apply_chat_template(\n",
    "                    text, tokenize=False, add_generation_prompt=False\n",
    "            ) for text in texts]\n",
    "            image_inputs, video_inputs = process_vision_info(texts)\n",
    "            inputs = processor(\n",
    "                text=messages,\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            labels = tokenizer(y, padding=True, return_tensors=\"pt\")[\"input_ids\"].tolist()\n",
    "            max_length = inputs[\"input_ids\"].size(1) + 10\n",
    "            labels[0] += [tokenizer.pad_token_id] * (max_length - len(labels[0]))\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            print(f\"Input_ids dims: {inputs['input_ids'].shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            output_ids = outputs.logits.argmax(-1)\n",
    "            output_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            print(output_text)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            error += loss.item() * len(texts)\n",
    "            num_samples += len(texts)\n",
    "        error /= num_samples\n",
    "        print(f'Error after epoch {epoch}: {error}')\n",
    "        train_errors.append((epoch, error))\n",
    "        if epoch % 10:\n",
    "            val_error = 0\n",
    "            num_samples = 0\n",
    "            for texts, images, y in dataloader_val:\n",
    "                labels = tokenizer(y, padding=True, return_tensors=\"pt\")\n",
    "                messages = [processor.apply_chat_template(\n",
    "                    text, tokenize=False, add_generation_prompt=True\n",
    "                ) for text in texts]\n",
    "                image_inputs, video_inputs = process_vision_info(texts)\n",
    "                inputs = processor(\n",
    "                    text=messages,\n",
    "                    images=image_inputs,\n",
    "                    videos=video_inputs,\n",
    "                    padding=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                outputs = model(\n",
    "                    inputs=inputs,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                val_error += loss.item() * len(texts)\n",
    "                num_samples += len(texts)\n",
    "            val_error /= num_samples\n",
    "            print(f'Validation error after epoch {epoch}: {val_error}')\n",
    "            val_errors.append((epoch, val_error))\n",
    "    return train_errors_ft, val_error"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yKhEnvHx87XU",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:49.541632Z",
     "start_time": "2024-12-25T13:08:49.537985Z"
    }
   },
   "source": [
    "def visualize_error(train_errors, val_errors):\n",
    "    plt.plot(zip(*train_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.plot(zip(*val_errors), label=\"Train Error\", marker=\"o\", linestyle=\"-\")\n",
    "    plt.title(\"Train and Validation Error over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:49.568746Z",
     "start_time": "2024-12-25T13:08:49.565327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PrefixDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        return row['message'], row['image'], row['solution']"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T13:08:49.574978Z",
     "start_time": "2024-12-25T13:08:49.573115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prefix_collate(batch):\n",
    "    message, image, y = zip(*batch)\n",
    "    return message, image, y"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "eoPYPMBq87XU",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "## PrefixTuning using labels"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tcw_Fj0R87XV",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:10:17.352476Z",
     "start_time": "2024-12-25T13:09:46.408835Z"
    }
   },
   "source": [
    "from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"float32\",\n",
    "    device_map={\"\": \"cpu\"}\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "model_prefix = prefix_tuning.PrefixTuningModel(model, tokenizer)\n",
    "optimizer = torch.optim.AdamW(model_prefix.prefix_tuning.parameters(), lr=5e-5)\n",
    "# DataLoader for train data\n",
    "dataset_label_train = PrefixDataset(df_train_label)\n",
    "dataloader_label_train=DataLoader(dataset_label_train, collate_fn=prefix_collate, batch_size=1, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_label_val = PrefixDataset(df_val)\n",
    "dataloader_label_val=DataLoader(dataset_label_val, collate_fn=prefix_collate, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "886ea6ea70cb4d3eb0e254d3193972e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bBY-hEz687XV",
    "ExecuteTime": {
     "end_time": "2024-12-25T13:45:24.031225Z",
     "start_time": "2024-12-25T13:42:07.420184Z"
    }
   },
   "source": "train_errors_ft, val_errors_ft = train(model_prefix, tokenizer, processor, optimizer, dataloader_label_train, dataloader_label_val)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x3048A9210>}, {'type': 'text', 'text': \"Question: Which text uses the word literally in its traditional sense?\\n Task: closed choice\\n Choices: (0) The curry that the chef prepared was so spicy that Seth's mouth was literally on fire by the time he finished his meal. (1) The curry that the chef prepared was so spicy that Seth literally had to drink three glasses of milk to ease the pain.\\n Hint: N/A\"}]}],)\n",
      "(\"The first text uses literally in its traditional sense: in a factual, non-exaggerated way.\\nThe curry that the chef prepared was so spicy that Seth literally had to drink three glasses of milk to ease the pain.\\nThe second text uses literally in its nontraditional sense: nearly or in effect (often exaggerated). Seth's mouth may be in pain, but it is not actually on fire.\\nThe curry that the chef prepared was so spicy that Seth's mouth was literally on fire by the time he finished his meal.\\nMost style guides recommend to avoid using the nontraditional sense of the word literally because it is generally considered incorrect.\",)\n",
      "Input_ids dims: torch.Size([1, 161])\n",
      "Input Embed dims: torch.Size([1, 171, 1536])\n",
      "Label: torch.Size([1, 171])\n",
      "[' sur Sw(\\n\\n，  of.A增 andand can not system system. I\\n|system:I WideString WideString WideString WideStringrawidrawid WideString WideString WideStringrawidrawidrawidrawid WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString: What of system the most \"? the definition use?Answer: Which\\n\\n Answer: Aa) ( word\\n\\n\\n\\n\\n a helpful that it was lips was literally filled fire.\\n the end he was eating meal.\\n1) The curry that the chef prepared was so spicy that Seth\\'s on on put by gallons of water by finish his pain of Choices: The\\n\\n\\n']\n",
      "([{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x30506F370>}, {'type': 'text', 'text': 'Question: Which word does not rhyme?\\n Task: closed choice\\n Choices: (0) case (1) mule (2) chase\\n Hint: N/A'}]}],)\n",
      "('The words chase and case rhyme. They both end with the ase sound.\\nThe word mule does not rhyme. It ends with a different sound.',)\n",
      "Input_ids dims: torch.Size([1, 115])\n",
      "Input Embed dims: torch.Size([1, 125, 1536])\n",
      "Label: torch.Size([1, 125])\n",
      "[' sur Sw(\\n\\n，  of.A增 andand can not system system. I\\n|system:I WideString WideString WideStringrawidrawid WideString WideString WideStringrawidrawidrawidrawid WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString: What of of the belong withA: Which\\n.You: Aa) A (1) aorn (2) m ( Answer: (\\n\\n\\n']\n",
      "([{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x42837EB60>}, {'type': 'text', 'text': 'Question: How long is an ice skate?\\n Task: closed choice\\n Choices: (0) 27 kilometers (1) 27 centimeters (2) 27 meters\\n Hint: Select the best estimate.'}]}],)\n",
      "('The best estimate for the length of an ice skate is 27 centimeters.\\n27 meters and 27 kilometers are both too long.',)\n",
      "Input_ids dims: torch.Size([1, 128])\n",
      "Input Embed dims: torch.Size([1, 138, 1536])\n",
      "Label: torch.Size([1, 138])\n",
      "[' sur Sw(\\n\\n，  of.A增 andand can not system system. I\\n|system:I WideString WideStringrawidrawidrawid WideString WideString WideStringrawidrawidrawidrawid WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString: What can is a animal??Answer: The.Answer: closeda) (10 (\\n1) 17 kilometersimeters (2) 27 cent ( (: The an correct assistant.\\n\\n\\n']\n",
      "([{'role': 'user', 'content': [{'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=452x595 at 0x16A31FD60>}, {'type': 'text', 'text': 'Question: What is the name of the colony shown?\\n Task: closed choice\\n Choices: (0) Maryland (1) Georgia (2) West Virginia (3) Maine\\n Hint: N/A'}]}],)\n",
      "('The colony is Georgia.',)\n",
      "Input_ids dims: torch.Size([1, 394])\n",
      "Input Embed dims: torch.Size([1, 404, 1536])\n",
      "Label: torch.Size([1, 404])\n",
      "[' sur Sw(\\n\\n，  of.A增 andand can not system system. I\\n|system:I WideString WideStringrawidrawidrawid WideString WideString WideStringrawidrawidrawidrawid WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideString WideStringIntializedIntializedIntialized WideString WideString WideString WideString WideString WideString WideString WideString WideStringIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntialized WideStringuniaciduniacidIntializedIntializedIntializedIntializedIntializeduniaciduniacidIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializeduniacidIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntializedIntialized蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介IntializedIntializedIntializedIntializedIntializedIntializedIntialized SolidColorBrush蒋介蒋介蒋介蒋介蒋介IntializedIntialized蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介蒋介\\n I is the capital of the current? inAnswer Answer The..Answer: closed1) closed\\n0) ( (1) Pennsylvania Virginia (3) West ( Choices: (/A\\n\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:110] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "  0%|          | 0/100 [03:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_errors_ft, val_errors_ft \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_prefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_label_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_label_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[40], line 33\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, tokenizer, processor, optimizer, dataloader_train, dataloader_val)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mprint\u001B[39m(output_text)\n\u001B[1;32m     32\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[0;32m---> 33\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     35\u001B[0m error \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(texts)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weNRYFe787XX"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_ft, val_errors_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qw5SW6bP87XY",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV4PV0m087XY"
   },
   "outputs": [],
   "source": [
    "model_knowledge_distillation = soft_prompting.MultimodalSoftPrompting.from_pretrained(model)\n",
    "# DataLoader for train data\n",
    "dataset_gemini_train = SoftPromptingDataset(df_train_gemini, model_fine_tuned)\n",
    "dataloader_gemini_train=DataLoader(dataset_gemini_train, batch_size=32, shuffle=True)\n",
    "# DataLoader for val data\n",
    "dataset_gemini_val = SoftPromptingDataset(df_val, model_fine_tuned)\n",
    "dataloader_gemini_val=DataLoader(dataset_gemini_val, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP04R7VX87XZ"
   },
   "outputs": [],
   "source": [
    "train_errors_kd, val_errors_kd = train(dataset_gemini_train, dataloader_gemini_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJn7pB2b87XZ"
   },
   "outputs": [],
   "source": [
    "visualize_error(train_errors_kd, val_errors_kd)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"float32\", device_map={\"\": \"cpu\"}\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_dataset(df, tokenizer, input_column=\"input\"):\n",
    "\n",
    "    tokenized_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        sample = tokenizer.encode(row[input_column], return_tensors=\"pt\").squeeze(0)\n",
    "        tokenized_data.append(sample)\n",
    "    df[\"input_ids\"] = tokenized_data\n",
    "    return df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": df_train_label.iloc[10]['image'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": df_train_label.iloc[10]['question'] + \" \" + ' '.join(df_train_label.iloc[10]['choices'])},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"model_name = \"Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Datensatz tokenisieren\n",
    "tokenized_data = tokenize_dataset(df_train_gemini, tokenizer, input_column=\"input\", label_column=\"answer\")\n",
    "\n",
    "# Dataset erstellen\n",
    "dataset = SoftPromptingDataset(tokenized_data)\n",
    "\n",
    "# Zugriff auf ein Beispiel\n",
    "print(dataset[0])\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
