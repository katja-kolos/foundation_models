{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 57580,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1736714136853074,
      "grad_norm": 0.9572440981864929,
      "learning_rate": 1.9826328586314695e-05,
      "loss": 23.7907,
      "step": 500
    },
    {
      "epoch": 0.3473428273706148,
      "grad_norm": 1.0376472473144531,
      "learning_rate": 1.9652657172629385e-05,
      "loss": 20.4088,
      "step": 1000
    },
    {
      "epoch": 0.5210142410559222,
      "grad_norm": 1.375080943107605,
      "learning_rate": 1.947898575894408e-05,
      "loss": 16.4579,
      "step": 1500
    },
    {
      "epoch": 0.6946856547412296,
      "grad_norm": 0.9952253699302673,
      "learning_rate": 1.9305314345258772e-05,
      "loss": 11.5039,
      "step": 2000
    },
    {
      "epoch": 0.868357068426537,
      "grad_norm": 0.5855786800384521,
      "learning_rate": 1.9131642931573466e-05,
      "loss": 7.6758,
      "step": 2500
    },
    {
      "epoch": 1.0420284821118444,
      "grad_norm": 0.2904445230960846,
      "learning_rate": 1.8957971517888156e-05,
      "loss": 6.3843,
      "step": 3000
    },
    {
      "epoch": 1.2156998957971519,
      "grad_norm": 0.2562377452850342,
      "learning_rate": 1.878430010420285e-05,
      "loss": 5.8618,
      "step": 3500
    },
    {
      "epoch": 1.3893713094824591,
      "grad_norm": 0.3660363256931305,
      "learning_rate": 1.8610628690517543e-05,
      "loss": 5.5444,
      "step": 4000
    },
    {
      "epoch": 1.5630427231677666,
      "grad_norm": 0.16241873800754547,
      "learning_rate": 1.8436957276832233e-05,
      "loss": 5.2847,
      "step": 4500
    },
    {
      "epoch": 1.7367141368530739,
      "grad_norm": 0.2305196225643158,
      "learning_rate": 1.8263285863146927e-05,
      "loss": 5.0578,
      "step": 5000
    },
    {
      "epoch": 1.9103855505383813,
      "grad_norm": 0.11875094473361969,
      "learning_rate": 1.808961444946162e-05,
      "loss": 4.8744,
      "step": 5500
    },
    {
      "epoch": 2.084056964223689,
      "grad_norm": 0.17743560671806335,
      "learning_rate": 1.7915943035776314e-05,
      "loss": 4.7137,
      "step": 6000
    },
    {
      "epoch": 2.2577283779089963,
      "grad_norm": 0.13531075417995453,
      "learning_rate": 1.7742271622091004e-05,
      "loss": 4.5856,
      "step": 6500
    },
    {
      "epoch": 2.4313997915943037,
      "grad_norm": 0.1709917038679123,
      "learning_rate": 1.7568600208405697e-05,
      "loss": 4.4614,
      "step": 7000
    },
    {
      "epoch": 2.605071205279611,
      "grad_norm": 0.17960090935230255,
      "learning_rate": 1.739492879472039e-05,
      "loss": 4.3225,
      "step": 7500
    },
    {
      "epoch": 2.7787426189649183,
      "grad_norm": 0.1362837553024292,
      "learning_rate": 1.7221257381035084e-05,
      "loss": 4.1964,
      "step": 8000
    },
    {
      "epoch": 2.9524140326502257,
      "grad_norm": 0.12229001522064209,
      "learning_rate": 1.7047585967349774e-05,
      "loss": 4.0861,
      "step": 8500
    },
    {
      "epoch": 3.126085446335533,
      "grad_norm": 0.19687685370445251,
      "learning_rate": 1.6873914553664468e-05,
      "loss": 3.9472,
      "step": 9000
    },
    {
      "epoch": 3.2997568600208407,
      "grad_norm": 0.2589303255081177,
      "learning_rate": 1.670024313997916e-05,
      "loss": 3.8478,
      "step": 9500
    },
    {
      "epoch": 3.473428273706148,
      "grad_norm": 0.12065392732620239,
      "learning_rate": 1.6526571726293855e-05,
      "loss": 3.7473,
      "step": 10000
    },
    {
      "epoch": 3.6470996873914556,
      "grad_norm": 0.1094803586602211,
      "learning_rate": 1.6352900312608545e-05,
      "loss": 3.653,
      "step": 10500
    },
    {
      "epoch": 3.8207711010767627,
      "grad_norm": 0.12477348744869232,
      "learning_rate": 1.617922889892324e-05,
      "loss": 3.5616,
      "step": 11000
    },
    {
      "epoch": 3.99444251476207,
      "grad_norm": 0.09294167160987854,
      "learning_rate": 1.6005557485237932e-05,
      "loss": 3.4761,
      "step": 11500
    },
    {
      "epoch": 4.168113928447378,
      "grad_norm": 0.11232904344797134,
      "learning_rate": 1.5831886071552625e-05,
      "loss": 3.4075,
      "step": 12000
    },
    {
      "epoch": 4.341785342132685,
      "grad_norm": 0.16911675035953522,
      "learning_rate": 1.5658214657867316e-05,
      "loss": 3.3254,
      "step": 12500
    },
    {
      "epoch": 4.5154567558179926,
      "grad_norm": 0.7784968018531799,
      "learning_rate": 1.548454324418201e-05,
      "loss": 3.237,
      "step": 13000
    },
    {
      "epoch": 4.6891281695033,
      "grad_norm": 0.10504090785980225,
      "learning_rate": 1.53108718304967e-05,
      "loss": 3.1901,
      "step": 13500
    },
    {
      "epoch": 4.8627995831886075,
      "grad_norm": 0.22590692341327667,
      "learning_rate": 1.5137200416811394e-05,
      "loss": 3.1299,
      "step": 14000
    },
    {
      "epoch": 5.0364709968739145,
      "grad_norm": 0.11215690523386002,
      "learning_rate": 1.4963529003126088e-05,
      "loss": 3.0582,
      "step": 14500
    },
    {
      "epoch": 5.210142410559222,
      "grad_norm": 0.10538720339536667,
      "learning_rate": 1.478985758944078e-05,
      "loss": 2.9969,
      "step": 15000
    },
    {
      "epoch": 5.3838138242445295,
      "grad_norm": 0.14087532460689545,
      "learning_rate": 1.4616186175755472e-05,
      "loss": 2.9525,
      "step": 15500
    },
    {
      "epoch": 5.5574852379298365,
      "grad_norm": 0.11297066509723663,
      "learning_rate": 1.4442514762070163e-05,
      "loss": 2.908,
      "step": 16000
    },
    {
      "epoch": 5.731156651615144,
      "grad_norm": 0.11919628083705902,
      "learning_rate": 1.4268843348384857e-05,
      "loss": 2.8458,
      "step": 16500
    },
    {
      "epoch": 5.9048280653004515,
      "grad_norm": 0.14737537503242493,
      "learning_rate": 1.409517193469955e-05,
      "loss": 2.8122,
      "step": 17000
    },
    {
      "epoch": 6.078499478985759,
      "grad_norm": 0.1411878615617752,
      "learning_rate": 1.3921500521014242e-05,
      "loss": 2.765,
      "step": 17500
    },
    {
      "epoch": 6.252170892671066,
      "grad_norm": 0.1459674835205078,
      "learning_rate": 1.3747829107328934e-05,
      "loss": 2.7184,
      "step": 18000
    },
    {
      "epoch": 6.425842306356373,
      "grad_norm": 0.2049950659275055,
      "learning_rate": 1.3574157693643627e-05,
      "loss": 2.678,
      "step": 18500
    },
    {
      "epoch": 6.599513720041681,
      "grad_norm": 0.11295530200004578,
      "learning_rate": 1.3400486279958321e-05,
      "loss": 2.6377,
      "step": 19000
    },
    {
      "epoch": 6.773185133726988,
      "grad_norm": 0.09791700541973114,
      "learning_rate": 1.3226814866273013e-05,
      "loss": 2.6013,
      "step": 19500
    },
    {
      "epoch": 6.946856547412296,
      "grad_norm": 0.126442551612854,
      "learning_rate": 1.3053143452587705e-05,
      "loss": 2.5845,
      "step": 20000
    },
    {
      "epoch": 7.120527961097603,
      "grad_norm": 0.07795196771621704,
      "learning_rate": 1.2879472038902398e-05,
      "loss": 2.5198,
      "step": 20500
    },
    {
      "epoch": 7.29419937478291,
      "grad_norm": 0.17052045464515686,
      "learning_rate": 1.270580062521709e-05,
      "loss": 2.5459,
      "step": 21000
    },
    {
      "epoch": 7.467870788468218,
      "grad_norm": 0.10286588966846466,
      "learning_rate": 1.2532129211531783e-05,
      "loss": 2.4837,
      "step": 21500
    },
    {
      "epoch": 7.641542202153525,
      "grad_norm": 0.12090875208377838,
      "learning_rate": 1.2358457797846475e-05,
      "loss": 2.4459,
      "step": 22000
    },
    {
      "epoch": 7.815213615838833,
      "grad_norm": 0.07983209192752838,
      "learning_rate": 1.2184786384161169e-05,
      "loss": 2.4245,
      "step": 22500
    },
    {
      "epoch": 7.98888502952414,
      "grad_norm": 0.05568615719676018,
      "learning_rate": 1.201111497047586e-05,
      "loss": 2.3877,
      "step": 23000
    },
    {
      "epoch": 8.162556443209448,
      "grad_norm": 0.18117864429950714,
      "learning_rate": 1.1837443556790554e-05,
      "loss": 2.3853,
      "step": 23500
    },
    {
      "epoch": 8.336227856894755,
      "grad_norm": 0.10937915742397308,
      "learning_rate": 1.1663772143105246e-05,
      "loss": 2.35,
      "step": 24000
    },
    {
      "epoch": 8.509899270580062,
      "grad_norm": 0.1275090128183365,
      "learning_rate": 1.1490100729419938e-05,
      "loss": 2.329,
      "step": 24500
    },
    {
      "epoch": 8.68357068426537,
      "grad_norm": 0.16216446459293365,
      "learning_rate": 1.1316429315734631e-05,
      "loss": 2.3119,
      "step": 25000
    },
    {
      "epoch": 8.857242097950678,
      "grad_norm": 0.1445135474205017,
      "learning_rate": 1.1142757902049323e-05,
      "loss": 2.2976,
      "step": 25500
    },
    {
      "epoch": 9.030913511635985,
      "grad_norm": 0.2587380111217499,
      "learning_rate": 1.0969086488364016e-05,
      "loss": 2.278,
      "step": 26000
    },
    {
      "epoch": 9.204584925321292,
      "grad_norm": 0.24325674772262573,
      "learning_rate": 1.0795415074678708e-05,
      "loss": 2.2438,
      "step": 26500
    },
    {
      "epoch": 9.3782563390066,
      "grad_norm": 0.12098748236894608,
      "learning_rate": 1.0621743660993402e-05,
      "loss": 2.237,
      "step": 27000
    },
    {
      "epoch": 9.551927752691906,
      "grad_norm": 0.16238221526145935,
      "learning_rate": 1.0448072247308094e-05,
      "loss": 2.2343,
      "step": 27500
    },
    {
      "epoch": 9.725599166377215,
      "grad_norm": 0.0690966546535492,
      "learning_rate": 1.0274400833622787e-05,
      "loss": 2.2166,
      "step": 28000
    },
    {
      "epoch": 9.899270580062522,
      "grad_norm": 0.1052691787481308,
      "learning_rate": 1.0100729419937479e-05,
      "loss": 2.1729,
      "step": 28500
    },
    {
      "epoch": 10.072941993747829,
      "grad_norm": 0.16130337119102478,
      "learning_rate": 9.92705800625217e-06,
      "loss": 2.2,
      "step": 29000
    },
    {
      "epoch": 10.246613407433136,
      "grad_norm": 0.2948465049266815,
      "learning_rate": 9.753386592566864e-06,
      "loss": 2.1592,
      "step": 29500
    },
    {
      "epoch": 10.420284821118443,
      "grad_norm": 0.22807277739048004,
      "learning_rate": 9.579715178881556e-06,
      "loss": 2.1436,
      "step": 30000
    },
    {
      "epoch": 10.593956234803752,
      "grad_norm": 0.08048545569181442,
      "learning_rate": 9.40604376519625e-06,
      "loss": 2.1311,
      "step": 30500
    },
    {
      "epoch": 10.767627648489059,
      "grad_norm": 0.06721319258213043,
      "learning_rate": 9.232372351510941e-06,
      "loss": 2.1306,
      "step": 31000
    },
    {
      "epoch": 10.941299062174366,
      "grad_norm": 0.19569216668605804,
      "learning_rate": 9.058700937825635e-06,
      "loss": 2.1263,
      "step": 31500
    },
    {
      "epoch": 11.114970475859673,
      "grad_norm": 0.23495177924633026,
      "learning_rate": 8.885029524140327e-06,
      "loss": 2.1024,
      "step": 32000
    },
    {
      "epoch": 11.28864188954498,
      "grad_norm": 0.13971330225467682,
      "learning_rate": 8.711358110455018e-06,
      "loss": 2.081,
      "step": 32500
    },
    {
      "epoch": 11.462313303230289,
      "grad_norm": 0.09707928448915482,
      "learning_rate": 8.537686696769712e-06,
      "loss": 2.0974,
      "step": 33000
    },
    {
      "epoch": 11.635984716915596,
      "grad_norm": 0.05382204428315163,
      "learning_rate": 8.364015283084404e-06,
      "loss": 2.0746,
      "step": 33500
    },
    {
      "epoch": 11.809656130600903,
      "grad_norm": 0.10519110411405563,
      "learning_rate": 8.190343869399097e-06,
      "loss": 2.0604,
      "step": 34000
    },
    {
      "epoch": 11.98332754428621,
      "grad_norm": 0.07183253765106201,
      "learning_rate": 8.016672455713789e-06,
      "loss": 2.0407,
      "step": 34500
    },
    {
      "epoch": 12.156998957971519,
      "grad_norm": 0.11257331818342209,
      "learning_rate": 7.843001042028483e-06,
      "loss": 2.0497,
      "step": 35000
    },
    {
      "epoch": 12.330670371656826,
      "grad_norm": 0.12530386447906494,
      "learning_rate": 7.669329628343174e-06,
      "loss": 2.037,
      "step": 35500
    },
    {
      "epoch": 12.504341785342133,
      "grad_norm": 0.19320809841156006,
      "learning_rate": 7.495658214657868e-06,
      "loss": 2.0237,
      "step": 36000
    },
    {
      "epoch": 12.67801319902744,
      "grad_norm": 0.08324132114648819,
      "learning_rate": 7.321986800972561e-06,
      "loss": 2.0142,
      "step": 36500
    },
    {
      "epoch": 12.851684612712747,
      "grad_norm": 0.08291331678628922,
      "learning_rate": 7.148315387287253e-06,
      "loss": 2.0089,
      "step": 37000
    },
    {
      "epoch": 13.025356026398056,
      "grad_norm": 0.10664788633584976,
      "learning_rate": 6.974643973601946e-06,
      "loss": 2.0064,
      "step": 37500
    },
    {
      "epoch": 13.199027440083363,
      "grad_norm": 0.29259759187698364,
      "learning_rate": 6.800972559916639e-06,
      "loss": 2.006,
      "step": 38000
    },
    {
      "epoch": 13.37269885376867,
      "grad_norm": 0.35291287302970886,
      "learning_rate": 6.62730114623133e-06,
      "loss": 1.9912,
      "step": 38500
    },
    {
      "epoch": 13.546370267453977,
      "grad_norm": 0.19429759681224823,
      "learning_rate": 6.453629732546024e-06,
      "loss": 1.9654,
      "step": 39000
    },
    {
      "epoch": 13.720041681139284,
      "grad_norm": 0.08603592962026596,
      "learning_rate": 6.279958318860716e-06,
      "loss": 1.974,
      "step": 39500
    },
    {
      "epoch": 13.893713094824593,
      "grad_norm": 0.20727775990962982,
      "learning_rate": 6.106286905175408e-06,
      "loss": 1.987,
      "step": 40000
    },
    {
      "epoch": 14.0673845085099,
      "grad_norm": 0.19398048520088196,
      "learning_rate": 5.932615491490101e-06,
      "loss": 1.9883,
      "step": 40500
    },
    {
      "epoch": 14.241055922195207,
      "grad_norm": 0.07342811673879623,
      "learning_rate": 5.758944077804794e-06,
      "loss": 1.9499,
      "step": 41000
    },
    {
      "epoch": 14.414727335880514,
      "grad_norm": 0.07988019287586212,
      "learning_rate": 5.585272664119486e-06,
      "loss": 1.9694,
      "step": 41500
    },
    {
      "epoch": 14.58839874956582,
      "grad_norm": 0.15737022459506989,
      "learning_rate": 5.411601250434179e-06,
      "loss": 1.9384,
      "step": 42000
    },
    {
      "epoch": 14.76207016325113,
      "grad_norm": 0.16023215651512146,
      "learning_rate": 5.237929836748872e-06,
      "loss": 1.9434,
      "step": 42500
    },
    {
      "epoch": 14.935741576936437,
      "grad_norm": 0.12610815465450287,
      "learning_rate": 5.0642584230635635e-06,
      "loss": 1.9518,
      "step": 43000
    },
    {
      "epoch": 15.109412990621744,
      "grad_norm": 0.13978387415409088,
      "learning_rate": 4.890587009378256e-06,
      "loss": 1.9277,
      "step": 43500
    },
    {
      "epoch": 15.28308440430705,
      "grad_norm": 0.2718261778354645,
      "learning_rate": 4.716915595692949e-06,
      "loss": 1.921,
      "step": 44000
    },
    {
      "epoch": 15.456755817992358,
      "grad_norm": 0.0708150565624237,
      "learning_rate": 4.5432441820076414e-06,
      "loss": 1.9236,
      "step": 44500
    },
    {
      "epoch": 15.630427231677666,
      "grad_norm": 0.08952101320028305,
      "learning_rate": 4.369572768322334e-06,
      "loss": 1.9395,
      "step": 45000
    },
    {
      "epoch": 15.804098645362973,
      "grad_norm": 0.3826293647289276,
      "learning_rate": 4.195901354637027e-06,
      "loss": 1.9057,
      "step": 45500
    },
    {
      "epoch": 15.97777005904828,
      "grad_norm": 0.12375425547361374,
      "learning_rate": 4.022229940951719e-06,
      "loss": 1.9309,
      "step": 46000
    },
    {
      "epoch": 16.151441472733588,
      "grad_norm": 0.06258042901754379,
      "learning_rate": 3.848558527266412e-06,
      "loss": 1.9238,
      "step": 46500
    },
    {
      "epoch": 16.325112886418896,
      "grad_norm": 0.1487891674041748,
      "learning_rate": 3.6748871135811047e-06,
      "loss": 1.9055,
      "step": 47000
    },
    {
      "epoch": 16.4987843001042,
      "grad_norm": 0.12262062728404999,
      "learning_rate": 3.5012156998957974e-06,
      "loss": 1.9106,
      "step": 47500
    },
    {
      "epoch": 16.67245571378951,
      "grad_norm": 0.1284181773662567,
      "learning_rate": 3.32754428621049e-06,
      "loss": 1.9009,
      "step": 48000
    },
    {
      "epoch": 16.84612712747482,
      "grad_norm": 0.09020502865314484,
      "learning_rate": 3.1538728725251827e-06,
      "loss": 1.8919,
      "step": 48500
    },
    {
      "epoch": 17.019798541160124,
      "grad_norm": 0.05399171635508537,
      "learning_rate": 2.9802014588398754e-06,
      "loss": 1.9014,
      "step": 49000
    },
    {
      "epoch": 17.193469954845433,
      "grad_norm": 0.08324481546878815,
      "learning_rate": 2.806530045154568e-06,
      "loss": 1.9153,
      "step": 49500
    },
    {
      "epoch": 17.36714136853074,
      "grad_norm": 0.23711879551410675,
      "learning_rate": 2.6328586314692607e-06,
      "loss": 1.8922,
      "step": 50000
    },
    {
      "epoch": 17.540812782216047,
      "grad_norm": 0.1894703358411789,
      "learning_rate": 2.459187217783953e-06,
      "loss": 1.8832,
      "step": 50500
    },
    {
      "epoch": 17.714484195901356,
      "grad_norm": 0.18669116497039795,
      "learning_rate": 2.2855158040986456e-06,
      "loss": 1.8818,
      "step": 51000
    },
    {
      "epoch": 17.88815560958666,
      "grad_norm": 0.1022626981139183,
      "learning_rate": 2.1118443904133382e-06,
      "loss": 1.8784,
      "step": 51500
    },
    {
      "epoch": 18.06182702327197,
      "grad_norm": 0.069497250020504,
      "learning_rate": 1.938172976728031e-06,
      "loss": 1.903,
      "step": 52000
    },
    {
      "epoch": 18.235498436957275,
      "grad_norm": 0.17592181265354156,
      "learning_rate": 1.7645015630427233e-06,
      "loss": 1.8766,
      "step": 52500
    },
    {
      "epoch": 18.409169850642584,
      "grad_norm": 0.15617911517620087,
      "learning_rate": 1.590830149357416e-06,
      "loss": 1.8763,
      "step": 53000
    },
    {
      "epoch": 18.582841264327893,
      "grad_norm": 0.17137368023395538,
      "learning_rate": 1.4171587356721085e-06,
      "loss": 1.8854,
      "step": 53500
    },
    {
      "epoch": 18.7565126780132,
      "grad_norm": 0.1432119458913803,
      "learning_rate": 1.2434873219868011e-06,
      "loss": 1.885,
      "step": 54000
    },
    {
      "epoch": 18.930184091698507,
      "grad_norm": 0.08842911571264267,
      "learning_rate": 1.0698159083014936e-06,
      "loss": 1.8891,
      "step": 54500
    },
    {
      "epoch": 19.103855505383812,
      "grad_norm": 0.08801652491092682,
      "learning_rate": 8.961444946161862e-07,
      "loss": 1.8775,
      "step": 55000
    },
    {
      "epoch": 19.27752691906912,
      "grad_norm": 0.19669102132320404,
      "learning_rate": 7.224730809308789e-07,
      "loss": 1.8823,
      "step": 55500
    },
    {
      "epoch": 19.45119833275443,
      "grad_norm": 0.12773890793323517,
      "learning_rate": 5.488016672455714e-07,
      "loss": 1.8897,
      "step": 56000
    },
    {
      "epoch": 19.624869746439735,
      "grad_norm": 0.08131954073905945,
      "learning_rate": 3.7513025356026403e-07,
      "loss": 1.8933,
      "step": 56500
    },
    {
      "epoch": 19.798541160125044,
      "grad_norm": 0.05750471353530884,
      "learning_rate": 2.014588398749566e-07,
      "loss": 1.8623,
      "step": 57000
    },
    {
      "epoch": 19.97221257381035,
      "grad_norm": 0.06290140002965927,
      "learning_rate": 2.7787426189649184e-08,
      "loss": 1.8644,
      "step": 57500
    }
  ],
  "logging_steps": 500,
  "max_steps": 57580,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9795873460460288e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
