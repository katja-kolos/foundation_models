# -*- coding: utf-8 -*-
"""Foundation Models Small Multi (2)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ynqpZK_YPcbB_H_4kiXEEXaLWR9VY5FB

# Load Data
"""

!pip install datasets

from datasets import load_dataset
dataset = load_dataset("derek-thomas/ScienceQA")
validation_data = dataset["validation"]
validation_data

"""### This is a normal datapoint"""

validation_data[3500]

validation_data[3500]["image"]

"""### This is an abnormal datapoint with no image
But the text is enough to figure out the solution
"""

validation_data[0]

"""### This is a bad abnormal datapoint
Image is part of the task input and is missing. The solution is also missing.
"""

validation_data[4240]

filtered_validation_data = validation_data.filter(lambda example: example["lecture"] != "" and example["solution"] != "")
filtered_validation_data

"""# Experiment with Models"""

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

from abc import ABC, abstractmethod

class Model(ABC):
    """Abstract base class for models with custom processing."""

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.model = self.load_model()
        self.processor = self.load_processor()

    @abstractmethod
    def load_model(self):
        """Load the model architecture and weights."""
        pass

    @abstractmethod
    def load_processor(self):
        """Load the processor associated with the model."""
        pass

    @abstractmethod
    def process(self, *args, **kwargs):
        """Define the processing logic for the model."""
        pass

"""## Preliminary Experiments"""

class LLaVAModel(Model):
    def load_model(self):
        from transformers import LlavaForConditionalGeneration
        return LlavaForConditionalGeneration.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True)

    def load_processor(self):
        from transformers import AutoProcessor
        return AutoProcessor.from_pretrained(self.model_name)

    def process(self, text_input, image, **kwargs):
        # Workaround for "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing."
        # from https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf/discussions/34
        self.processor.patch_size = self.model.config.vision_config.patch_size
        self.processor.vision_feature_select_strategy = self.model.config.vision_feature_select_strategy

        # Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt
        # Each value in "content" has to be a list of dicts with types ("text", "image")
        conversation = [
            {
              "role": "user",
              "content": [
                  {"type": "text", "text": text_input},
                  {"type": "image"},
                ],
            },
        ]
        prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
        inputs = self.processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)
        self.model = self.model.to(device)
        with torch.no_grad():
          output = self.model.generate(**inputs, max_new_tokens=200, do_sample=False)
          response = self.processor.decode(output[0][2:], skip_special_tokens=True)

        print(f"\nPrompt:\n {prompt}")
        print(f"\nModel response:\n {response}")
        return response

models = {
    "LLaVAModel1.5-7b": LLaVAModel("llava-hf/llava-1.5-7b-hf")
}

"""### Prompt LLaVa with a chat setting
- additional user-defined instruction: `You are a helpful assistant. Answer the user's request to the best of your knowledge.`
- output format instruction: `Solve task. Return JSON: {"answer": "choice number", "solution": "your step-by-step thinking"}`

**Notes**:
- (?) possibility of empty responses of ASSISTANT (e.g. first example, `QTCHL` setting) -- why?
- (!) output format instruction `Return JSON: {"answer": "choice number", "solution": "your step-by-step thinking"}` actually resulted in `ASSISTANT: {"answer": "choice number", "solution": "your step-by-step thinking"}` in some cases.
- in other cases the format was correct, although the answer is different from the golden answer (and the model can actually justify its reasoning with some plausible) -- could this be the result of "the best of your knowledge"?
"""

n = 3500
#QCMIT
def process_datapoint(model, n):
  system_prompt = "You are a helpful assistant. Answer the user's request to the best of your knowledge."
  question = validation_data[n]["question"]
  context = validation_data[n]["hint"]
  choice = validation_data[n]["choices"]
  image = validation_data[n]["image"]
  task = validation_data[n]["task"]
  lecture = validation_data[n]["lecture"]
  solution = validation_data[n]["solution"]
  prompt_answer_and_solution = '\nSolve task. Return JSON: {"answer": "choice number", "solution": "your step-by-step thinking"}'
  prompt_answer_only = '\nSolve task. Return JSON: {"answer": "choice number"}'

  # text_input = f"Question: {question}\nContext: {context}\nOptions: {choice}\nTask: {task}"
  # QTCH
  text_input = f"{system_prompt} Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  # QTCHL
  text_input = f"{system_prompt} Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  # QTCHLS
  text_input = f"{system_prompt} Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\nSolution: {solution}\n Instruction: {prompt_answer_only}"
  model.process(text_input=text_input, image=image)

model = models["LLaVAModel1.5-7b"]
process_datapoint(model, n)

n = 2400
model = models["LLaVAModel1.5-7b"]
process_datapoint(model, n)

# won't work since the image is none
# n = 1500
# model = models["LLaVAModel1.5-7b"]
# process_datapoint(model, n)

"""### Prompt LLaVa with a chat setting (2)
- no additional user instruction (as there is already one handled implicitly by `self.processor.apply_chat_template(conversation, add_generation_prompt=True)` above at model definition
- output format instruction: `\nPlease output the answer in JSON style with an answer and a solution field`

**Notes**:
- some answers are still empty, but for the rest -
- **now, the answers are correct**! Diorite is now a rock, not a mineral! Was it really because of the "to the best of your knowledge" part, or is the output format instruction also infuencing the generation?
"""

def process_datapoint(model, n):
  question = validation_data[n]["question"]
  context = validation_data[n]["hint"]
  choice = validation_data[n]["choices"]
  image = validation_data[n]["image"]
  task = validation_data[n]["task"]
  lecture = validation_data[n]["lecture"]
  solution = validation_data[n]["solution"]
  prompt_answer_and_solution = '\nPlease output the answer in JSON style with an answer and a solution field'

  # text_input = f"Question: {question}\nContext: {context}\nOptions: {choice}\nTask: {task}"
  # QTCH
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  # QTCHL
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  # QTCHLS
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\nSolution: {solution}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  # QTCHS
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nSolution: {solution}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)

n = 3500
model = models["LLaVAModel1.5-7b"]
process_datapoint(model, n)

n = 2400
model = models["LLaVAModel1.5-7b"]
process_datapoint(model, n)

# won't work since no image
#n = 1500
#model = models["LLaVAModel1.5-7b"]
#process_datapoint(model, n)

"""# Process whole Dataset"""

class LLaVAModel(Model):
    def load_model(self):
        from transformers import LlavaForConditionalGeneration
        return LlavaForConditionalGeneration.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True)

    def load_processor(self):
        from transformers import AutoProcessor
        return AutoProcessor.from_pretrained(self.model_name)

    def process(self, text_input, image, **kwargs):
        # Workaround for "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing."
        # from https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf/discussions/34
        self.processor.patch_size = self.model.config.vision_config.patch_size
        self.processor.vision_feature_select_strategy = self.model.config.vision_feature_select_strategy

        # Define a chat histiry and use `apply_chat_template` to get correctly formatted prompt
        # Each value in "content" has to be a list of dicts with types ("text", "image")
        if image:
          conversation = [
              {
                "role": "user",
                "content": [
                    {"type": "text", "text": text_input},
                    {"type": "image"},
                  ],
              },
          ]
          prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
          inputs = self.processor(images=image, text=prompt, return_tensors='pt').to(device, torch.float16)
        else:
          conversation = [
              {
                "role": "user",
                "content": [
                    {"type": "text", "text": text_input},
                  ],
              },
          ]
          prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)
          inputs = self.processor(text=prompt, return_tensors='pt').to(device, torch.float16)
        self.model = self.model.to(device)
        with torch.no_grad():
          output = self.model.generate(**inputs, max_new_tokens=200, do_sample=False)
          response = self.processor.decode(output[0][2:], skip_special_tokens=True)

        response = response.split("ASSISTANT:")[1] #only take what follows after keyword ASSISTANT (the actual chat response)
        print(f"\nPrompt:\n {prompt}")
        print(f"\nModel response:\n {response}")
        return response

models = {
    "LLaVAModel1.5-7b": LLaVAModel("llava-hf/llava-1.5-7b-hf")
}

results = {"LLaVAModel1.5-7b": {"QTCH": [], "QTCHL": [], "QTCHLS": [], "QTCHS": []}}

def process_datapoint(model_name, n):
  model = models["LLaVAModel1.5-7b"]

  question = validation_data[n]["question"]
  context = validation_data[n]["hint"]
  choice = validation_data[n]["choices"]
  image = validation_data[n]["image"]
  task = validation_data[n]["task"]
  lecture = validation_data[n]["lecture"]
  solution = validation_data[n]["solution"]
  subject = validation_data[n]["subject"]
  prompt_answer_and_solution = '\nPlease output the answer in JSON style with an answer and a solution field'

  # QTCH
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context}\n Instruction: {prompt_answer_and_solution}"
  answer = model.process(text_input=text_input, image=image)
  results[model_name]["QTCH"].append((n, text_input, answer, subject))
  # QTCHL
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  results[model_name]["QTCHL"].append((n, text_input, answer, subject))
  # QTCHLS
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nLecture: {lecture}\nSolution: {solution}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  results[model_name]["QTCHLS"].append((n, text_input, answer, subject))
  # QTCHS
  text_input = f"Question: {question}\n Task: {task}\n Choices: {choice}\n Hint: {context} \nSolution: {solution}\n Instruction: {prompt_answer_and_solution}"
  model.process(text_input=text_input, image=image)
  results[model_name]["QTCHS"].append((n, text_input, answer, subject))

n = 2400
# model = models["LLaVAModel1.5-7b"]
model_name = "LLaVAModel1.5-7b"
process_datapoint(model_name, n)

results

"""# Save results"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

prompting_settings = ["QTCH", "QTCHL", "QTCHLS", "QTCHS"]

def save_model_results(model_name, results):
  results_save_path = f"/content/drive/MyDrive/foundation_models/benchmarking/{model_name}"
  for setting in prompting_settings:
    df = pd.DataFrame(results[model_name][setting], colums=["idx", "input", "output", "subject"])
    df.to_csv(f"{results_save_path}/{setting}.csv", index=False, sep="\t")

save_model_results("LLaVAModel1.5-7b")

